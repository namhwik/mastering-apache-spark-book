== YarnSparkHadoopUtil

`YarnSparkHadoopUtil` is...FIXME

It can only be created when link:spark-yarn-client.adoc#SPARK_YARN_MODE[SPARK_YARN_MODE flag is enabled].

NOTE: It belongs to `org.apache.spark.deploy.yarn` package.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.yarn.YarnSparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.yarn.YarnSparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[MEMORY_OVERHEAD_FACTOR]] MEMORY_OVERHEAD_FACTOR

`MEMORY_OVERHEAD_FACTOR` is a constant that equals to `10%` for memory overhead.

=== [[MEMORY_OVERHEAD_MIN]] MEMORY_OVERHEAD_MIN

`MEMORY_OVERHEAD_MIN` is a constant that equals to `384L` for memory overhead.

=== [[getApplicationAclsForYarn]] getApplicationAclsForYarn

CAUTION: FIXME

=== [[expandEnvironment]] Resolving Environment Variable (expandEnvironment method)

[source, scala]
----
expandEnvironment(environment: Environment): String
----

`expandEnvironment` resolves `environment` variable using YARN's `Environment.$` or `Environment.$$` methods (depending on the version of Hadoop used).

=== [[get]] Getting YarnSparkHadoopUtil Instance (get method)

CAUTION: FIXME

=== [[getContainerId]] Computing YARN's ContainerId (getContainerId method)

[source, scala]
----
getContainerId: ContainerId
----

`getContainerId` is a `private[spark]` method that gets YARN's `ContainerId` from the YARN environment variable `ApplicationConstants.Environment.CONTAINER_ID` and converts it to the return object using YARN's `ConverterUtils.toContainerId`.

=== [[startExecutorDelegationTokenRenewer]] startExecutorDelegationTokenRenewer

CAUTION: FIXME

=== [[stopExecutorDelegationTokenRenewer]] stopExecutorDelegationTokenRenewer

CAUTION: FIXME

=== [[getInitialTargetExecutorNumber]] Calculating Initial Number of Executors (getInitialTargetExecutorNumber method)

[source, scala]
----
getInitialTargetExecutorNumber(
  conf: SparkConf,
  numExecutors: Int = DEFAULT_NUMBER_EXECUTORS): Int
----

`getInitialTargetExecutorNumber` calculates the initial number of executors for Spark on YARN. It varies by whether link:spark-dynamic-allocation.adoc#isDynamicAllocationEnabled[dynamic allocation is enabled or not].

NOTE: The default number of executors (aka `DEFAULT_NUMBER_EXECUTORS`) is `2`.

If link:spark-dynamic-allocation.adoc#isDynamicAllocationEnabled[dynamic allocation is enabled], `getInitialTargetExecutorNumber` returns the value of link:spark-dynamic-allocation.adoc#spark.dynamicAllocation.initialExecutors[spark.dynamicAllocation.initialExecutors] or link:spark-dynamic-allocation.adoc#spark.dynamicAllocation.minExecutors[spark.dynamicAllocation.minExecutors] or `0`.

If however link:spark-dynamic-allocation.adoc#isDynamicAllocationEnabled[dynamic allocation is disabled], `getInitialTargetExecutorNumber` returns the value of link:spark-executor.adoc#spark.executor.instances[spark.executor.instances] setting or `SPARK_EXECUTOR_INSTANCES` environment variable, or the default value (of the input parameter `numExecutors`) `2`.

NOTE: It is used to calculate link:spark-yarn-yarnschedulerbackend.adoc#totalExpectedExecutors[totalExpectedExecutors] to link:spark-yarn-client-yarnclientschedulerbackend.adoc#totalExpectedExecutors[start Spark on YARN in client mode] or link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc#totalExpectedExecutors[cluster mode].

=== [[addPathToEnvironment]] addPathToEnvironment

[source, scala]
----
addPathToEnvironment(env: HashMap[String, String], key: String, value: String): Unit
----

CAUTION: FIXME
