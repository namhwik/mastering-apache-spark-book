== [[YarnClientSchedulerBackend]] YarnClientSchedulerBackend -- SchedulerBackend for YARN in Client Deploy Mode

`YarnClientSchedulerBackend` is the link:spark-scheduler-backends.adoc[SchedulerBackend] for link:README.adoc[Spark on YARN] for link:spark-submit.adoc#deploy-mode[client deploy mode].

NOTE: `client` deploy mode is the default deploy mode of Spark on YARN.

`YarnClientSchedulerBackend` is a link:spark-yarn-yarnschedulerbackend.adoc[YarnSchedulerBackend] that comes with just two custom implementations of the methods from the link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract]:

* <<start, start>>
* <<stop, stop>>

`YarnClientSchedulerBackend` uses <<client, client>> internal attribute to submit a Spark application when it <<start, starts up>> and <<waitForApplication, waits for the Spark application>> until it has exited, either successfully or due to some failure.

In order to initialize a `YarnClientSchedulerBackend` Spark passes a link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl] and link:spark-sparkcontext.adoc[SparkContext] (but only `SparkContext` is used in this object with `TaskSchedulerImpl` being passed on to the supertype -- link:spark-yarn-yarnschedulerbackend.adoc[YarnSchedulerBackend]).

`YarnClientSchedulerBackend` belongs to `org.apache.spark.scheduler.cluster` package.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend` logger to see what happens inside `YarnClientSchedulerBackend`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[client]] client Internal Attribute

`client` private attribute is an instance of link:spark-yarn-client.adoc[Client] that `YarnClientSchedulerBackend` creates an instance of when it <<start, starts>> and uses to link:spark-yarn-client.adoc#submitApplication[submit the Spark application].

`client` is also used to link:spark-yarn-client.adoc#monitorApplication[monitor the Spark application] when <<waitForApplication, `YarnClientSchedulerBackend` waits for the application>>.

link:spark-yarn-client.adoc#stop[`client` is stopped] when <<stop, `YarnClientSchedulerBackend` stops>>.

=== [[start]] Starting YarnClientSchedulerBackend (start method)

`start` is part of the link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract]. It is executed when link:spark-taskschedulerimpl.adoc#start[`TaskSchedulerImpl` starts].

[source, scala]
----
start(): Unit
----

It creates the internal <<client, client>> object and link:spark-yarn-client.adoc#submitApplication[submits the Spark application] to link:spark-yarn-introduction.adoc#ResourceManager[YARN ResourceManager]. After the application is deployed to YARN and running, it starts the internal `monitorThread` state monitor thread. In the meantime it also calls the supertype's `start`.

.Starting YarnClientSchedulerBackend
image::../images/spark-yarn-YarnClientSchedulerBackend-start.png[align="center"]

`start` sets link:spark-driver.adoc#spark_driver_appUIAddress[spark.driver.appUIAddress] to be link:spark-webui.adoc#SparkUI-appUIAddress[SparkUI.appUIAddress] (only if link:spark-sparkcontext.adoc#creating-instance[Spark's web UI is enabled]).

With DEBUG log level enabled you should see the following DEBUG message in the logs:

```
DEBUG YarnClientSchedulerBackend: ClientArguments called with: --arg [hostport]
```

NOTE: `hostport` is link:spark-sparkenv.adoc#spark.driver.host[spark.driver.host] and link:spark-sparkenv.adoc#spark.driver.port[spark.driver.port] separated by `:`, e.g. `192.168.99.1:64905`.

It then creates an instance of link:spark-yarn-client.adoc#ClientArguments[ClientArguments] (using `--arg [hostport]` arguments).

[[totalExpectedExecutors]]
It sets the parent's link:spark-yarn-yarnschedulerbackend.adoc#totalExpectedExecutors[totalExpectedExecutors] to link:spark-yarn-YarnSparkHadoopUtil.adoc#getInitialTargetExecutorNumber[the initial number of executors].

CAUTION: FIXME Why is this part of subtypes since they both set it to the same value?

It creates a link:spark-yarn-client.adoc[Client] object using the instance of `ClientArguments` and `SparkConf`.

The parent's link:spark-yarn-yarnschedulerbackend.adoc#bindToYarn[YarnSchedulerBackend.bindToYarn] method is called with the current application id (being the result of calling link:spark-yarn-client.adoc#submitApplication[Client.submitApplication]) and `None` for the optional `attemptId`.

The parent's link:spark-yarn-yarnschedulerbackend.adoc#start[YarnSchedulerBackend.start] is called.

<<waitForApplication, waitForApplication>> is executed that blocks until the application is running or an `SparkException` is thrown.

If link:spark-yarn-settings.adoc#spark.yarn.credentials.file[spark.yarn.credentials.file] is defined, link:spark-yarn-YarnSparkHadoopUtil.adoc#startExecutorDelegationTokenRenewer[YarnSparkHadoopUtil.get.startExecutorDelegationTokenRenewer(conf)] is called.

CAUTION: FIXME Why? What does `startExecutorDelegationTokenRenewer` do?

A <<MonitorThread, MonitorThread>> object is created (using `asyncMonitorApplication`) and started to asynchronously monitor the currently running application.

=== [[stop]] stop

`stop` is part of the link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract].

It stops the internal helper objects, i.e. `monitorThread` and `client` as well as "announces" the stop to other services through `Client.reportLauncherState`. In the meantime it also calls the supertype's `stop`.

`stop` makes sure that the internal `client` has already been created (i.e. it is not `null`), but not necessarily started.

`stop` stops the internal `monitorThread` using `MonitorThread.stopMonitor` method.

It then "announces" the stop using link:spark-yarn-client.adoc#reportLauncherState[Client.reportLauncherState(SparkAppHandle.State.FINISHED)].

Later, it passes the call on to the suppertype's `stop` and, once the supertype's `stop` has finished, it calls link:spark-yarn-YarnSparkHadoopUtil.adoc#stopExecutorDelegationTokenRenewer[YarnSparkHadoopUtil.stopExecutorDelegationTokenRenewer] followed by link:spark-yarn-client.adoc#stop[stopping the internal client].

Eventually, when all went fine, you should see the following INFO message in the logs:

```
INFO YarnClientSchedulerBackend: Stopped
```

=== [[waitForApplication]] Waiting For Spark Application (waitForApplication method)

[source, scala]
----
waitForApplication(): Unit
----

`waitForApplication` is an internal (private) method that waits until the current application is running (using link:spark-yarn-client.adoc#monitorApplication[Client.monitorApplication]).

If the application has `FINISHED`, `FAILED`, or has been `KILLED`, a `SparkException` is thrown with the following message:

```
Yarn application has already ended! It might have been killed or unable to launch application master.
```

You should see the following INFO message in the logs for `RUNNING` state:

```
INFO YarnClientSchedulerBackend: Application [appId] has started running.
```

=== [[asyncMonitorApplication]] asyncMonitorApplication

[source, scala]
----
asyncMonitorApplication(): MonitorThread
----

`asyncMonitorApplication` internal method creates a separate daemon <<MonitorThread, MonitorThread>> thread called "Yarn application state monitor".

NOTE: `asyncMonitorApplication` does not start the daemon thread.

=== [[MonitorThread]] MonitorThread

`MonitorThread` internal class is to monitor a Spark application deployed to YARN in client mode.

When started, it calls the blocking  link:spark-yarn-client.adoc#monitorApplication[Client.monitorApplication] (with no application reports printed out to the console, i.e. `logApplicationReport` is disabled).

NOTE: `Client.monitorApplication` is a blocking operation and hence it is wrapped in `MonitorThread` to be executed in a separate thread.

When the call to `Client.monitorApplication` has finished, it is assumed that the application has exited. You should see the following ERROR message in the logs:

```
ERROR Yarn application has already exited with state [state]!
```

That leads to stopping the current `SparkContext` (using link:spark-sparkcontext.adoc#stop[SparkContext.stop]).
