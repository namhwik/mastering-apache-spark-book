== ExecutorRunnable

`ExecutorRunnable` <<run, starts a YARN container>> with link:spark-executor-backends-coarse-grained.adoc#main[`CoarseGrainedExecutorBackend` application]. If link:spark-ExternalShuffleService.adoc#spark_shuffle_service_enabled[external shuffle service is used], it is <<startContainer, set in the `ContainerLaunchContext` context as a service data using `spark_shuffle`>>.

NOTE: Despite the name `ExecutorRunnable` is not a http://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable] anymore after https://issues.apache.org/jira/browse/SPARK-12447[SPARK-12447].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.deploy.yarn.ExecutorRunnable` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.yarn.ExecutorRunnable=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating ExecutorRunnable Instance

[source, scala]
----
ExecutorRunnable(
  container: Container,
  conf: Configuration,
  sparkConf: SparkConf,
  masterAddress: String,
  slaveId: String,
  hostname: String,
  executorMemory: Int,
  executorCores: Int,
  appId: String,
  securityMgr: SecurityManager,
  localResources: Map[String, LocalResource])
----

`YarnAllocator` creates an instance of `ExecutorRunnable` when link:spark-yarn-YarnAllocator.adoc#runAllocatedContainers[launching Spark executors in allocated YARN containers].

A single `ExecutorRunnable` is created with the YARN container to run a Spark executor in.

The input `conf` (Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration]), `sparkConf`, `masterAddress` directly correspond to the constructor arguments of link:spark-yarn-YarnAllocator.adoc[YarnAllocator].

The input `slaveId` is from the internal counter in `YarnAllocator`.

The input `hostname` is the host of the YARN container.

The input `executorMemory` and `executorCores` are from `YarnAllocator`, but come from link:../spark-executor.adoc#spark_executor_memory[spark.executor.memory] and link:../spark-executor.adoc#spark_executor_cores[spark.executor.cores] configuration settings.

The input `appId`, `securityMgr`, and `localResources` are the same as link:spark-yarn-YarnAllocator.adoc#creating-instance[`YarnAllocator` was created for].

=== [[prepareEnvironment]] prepareEnvironment

CAUTION: FIXME

=== [[run]] Running ExecutorRunnable (run method)

When called, you should see the following INFO message in the logs:

```
INFO ExecutorRunnable: Starting Executor Container
```

It creates a YARN https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/NMClient.html[NMClient], inits it with <<yarnConf, yarnConf>> and starts it.

It ultimately <<startContainer, starts `CoarseGrainedExecutorBackend` in the container>>.

=== [[startContainer]] Starting CoarseGrainedExecutorBackend in Container (startContainer method)

[source, scala]
----
startContainer(): java.util.Map[String, ByteBuffer]
----

`startContainer` uses the https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/NMClient.html[NMClient] API to start a link:spark-executor-backends-coarse-grained.adoc[CoarseGrainedExecutorBackend] in a YARN container.

When `startContainer` is executed, you should see the following INFO message in the logs:

```
INFO ExecutorRunnable: Setting up ContainerLaunchContext
```

It then creates a YARN https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.html[ContainerLaunchContext] (which represents all of the information for the YARN NodeManager to launch a container) with the local resources and environment being the `localResources` and `env`, respectively, passed in to the `ExecutorRunnable` when <<creating-instance, it was created>>. It also sets security tokens.

It <<prepareCommand, prepares the command>> to launch `CoarseGrainedExecutorBackend` with all the details as provided when <<creating-instance, the `ExecutorRunnable` was created>>.

You should see the following INFO message in the logs:

```
INFO ExecutorRunnable:
===============================================================================
YARN executor launch context:
  env:
    [key] -> [value]
    ...

  command:
    [commands]
===============================================================================
```

The command is set to the just-created `ContainerLaunchContext`.

It sets application ACLs using link:spark-yarn-YarnSparkHadoopUtil.adoc#getApplicationAclsForYarn[YarnSparkHadoopUtil.getApplicationAclsForYarn].

If link:spark-ExternalShuffleService.adoc#spark_shuffle_service_enabled[external shuffle service is used], it registers with the YARN shuffle service already started on the NodeManager. The external shuffle service is set in the `ContainerLaunchContext` context as a service data using `spark_shuffle`.

Ultimately, `startContainer` requests the YARN NodeManager to start the YARN container for a Spark executor (as passed in when <<creating-instance, the `ExecutorRunnable` was created>>) with the `ContainerLaunchContext` context.

If any exception happens, a `SparkException` is thrown.

```
Exception while starting container [containerId] on host [hostname]
```

NOTE: `startContainer` is exclusively called as a part of <<run, running `ExecutorRunnable`>>.

==== [[prepareCommand]] Preparing Command to Launch CoarseGrainedExecutorBackend (prepareCommand method)

[source, scala]
----
prepareCommand(
  masterAddress: String,
  slaveId: String,
  hostname: String,
  executorMemory: Int,
  executorCores: Int,
  appId: String): List[String]
----

`prepareCommand` is a private method to prepare the command that is used to <<startContainer, start `org.apache.spark.executor.CoarseGrainedExecutorBackend` application in a YARN container>>. All the input parameters of `prepareCommand` become the link:spark-executor-backends-coarse-grained.adoc#main[command-line arguments of `CoarseGrainedExecutorBackend` application].

The input `executorMemory` is in `m` and becomes `-Xmx` in the JVM options.

It uses the optional link:spark-executor.adoc#spark_executor_extraJavaOptions[spark.executor.extraJavaOptions] for the JVM options.

If the optional `SPARK_JAVA_OPTS` environment variable is defined, it is added to the JVM options.

It uses the optional link:spark-executor.adoc#spark_executor_extraLibraryPath[spark.executor.extraLibraryPath] to set `prefixEnv`. It uses `Client.getClusterPath`.

CAUTION: FIXME `Client.getClusterPath`?

It sets `-Dspark.yarn.app.container.log.dir=<LOG_DIR>`
It sets the user classpath (using `Client.getUserClasspath`).

CAUTION: FIXME `Client.getUserClasspath`?

Finally, it creates the entire command to start link:spark-executor-backends-coarse-grained.adoc[org.apache.spark.executor.CoarseGrainedExecutorBackend] with the following arguments:

* `--driver-url` being the input `masterAddress`
* `--executor-id` being the input `slaveId`
* `--hostname` being the input `hostname`
* `--cores` being the input `executorCores`
* `--app-id` being the input `appId`

=== [[internal-registries]] Internal Registries

==== [[yarnConf]] yarnConf

`yarnConf` is an instance of YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/conf/YarnConfiguration.html[YarnConfiguration]. It is created when <<creating-instance, `ExecutorRunnable` is>>.
