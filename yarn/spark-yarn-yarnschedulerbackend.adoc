== [[YarnSchedulerBackend]] YarnSchedulerBackend -- Coarse-Grained Scheduler Backend for YARN

`YarnSchedulerBackend` is an abstract link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend] for YARN that contains common logic for the `client` and `cluster` YARN scheduler backends, i.e. link:spark-yarn-client-yarnclientschedulerbackend.adoc[YarnClientSchedulerBackend] and link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[YarnClusterSchedulerBackend] respectively.

`YarnSchedulerBackend` is available in the RPC Environment as link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc[`YarnScheduler` RPC Endpoint] (or <<yarnSchedulerEndpointRef, yarnSchedulerEndpointRef>> internally).

`YarnSchedulerBackend` expects link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl] and link:spark-sparkcontext.adoc[SparkContext] to initialize itself.

It works for a single Spark application (as `appId` of type `ApplicationId`)

CAUTION: FIXME It may be a note for scheduler backends in general.

=== [[attemptId]] attemptId Internal Attribute

[source, scala]
----
attemptId: Option[ApplicationAttemptId] = None
----

`attemptId` is the application attempt ID for this run of a Spark application. It is only available for link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

It is explicitly set to `None` when link:spark-yarn-client-yarnclientschedulerbackend.adoc#start[YarnClientSchedulerBackend starts] (and <<bindToYarn, bindToYarn>> is called).

It is set to the current attempt id (using YARN API's `ApplicationMaster.getAttemptId`) when link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc#start[YarnClusterSchedulerBackend starts] (and <<bindToYarn, bindToYarn>> is called).

NOTE: `attemptId` is exposed using <<applicationAttemptId, applicationAttemptId>> which is a part of link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract].

=== [[applicationAttemptId]] applicationAttemptId

NOTE: `applicationAttemptId` is a part of link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract].

[source, scala]
----
applicationAttemptId(): Option[String]
----

`applicationAttemptId` returns the <<applicationAttemptId, application attempt id>> of a Spark application.

=== [[reset]] Resetting YarnSchedulerBackend

NOTE: `reset` is a part of link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#contract[CoarseGrainedSchedulerBackend Contract].

`reset` link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#reset[resets] the parent `CoarseGrainedSchedulerBackend` scheduler backend and link:spark-service-executor-allocation-manager.adoc[ExecutorAllocationManager] (accessible by `SparkContext.executorAllocationManager`).

=== [[doRequestTotalExecutors]] doRequestTotalExecutors

[source, scala]
----
def doRequestTotalExecutors(requestedTotal: Int): Boolean
----

NOTE: `doRequestTotalExecutors` is a part of the link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#doRequestTotalExecutors[CoarseGrainedSchedulerBackend Contract].

.Requesting Total Executors in YarnSchedulerBackend (doRequestTotalExecutors method)
image::../images/spark-YarnSchedulerBackend-doRequestTotalExecutors.png[align="center"]

`doRequestTotalExecutors` simply sends a blocking link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RequestExecutors[RequestExecutors] message to <<yarnSchedulerEndpointRef, YarnScheduler RPC Endpoint>> with the input `requestedTotal` and the internal `localityAwareTasks` and `hostToLocalTaskCount` attributes.

CAUTION: FIXME The internal attributes are already set. When and how?

=== [[yarnSchedulerEndpointRef]] Reference to YarnScheduler RPC Endpoint (yarnSchedulerEndpointRef attribute)

`yarnSchedulerEndpointRef` is the reference to link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc[YarnScheduler RPC Endpoint].

=== [[totalExpectedExecutors]] totalExpectedExecutors

`totalExpectedExecutors` is a value that is `0` <<creating-instance, initially when a `YarnSchedulerBackend` instance is created>> but later changes when Spark on YARN starts (in  link:spark-yarn-client-yarnclientschedulerbackend.adoc#totalExpectedExecutors[client mode] or link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc#totalExpectedExecutors[cluster mode]).

NOTE: After Spark on YARN is started, `totalExpectedExecutors` is initialized to a proper value.

It is used in <<sufficientResourcesRegistered, sufficientResourcesRegistered>>.

CAUTION: FIXME Where is this used?

=== [[initialization]][[creating-instance]] Creating YarnSchedulerBackend Instance

When created, `YarnSchedulerBackend` sets the internal <<minRegisteredRatio, minRegisteredRatio>> which is `0.8` when <<spark.scheduler.minRegisteredResourcesRatio, spark.scheduler.minRegisteredResourcesRatio>> is _not_ set or the link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#minRegisteredRatio[parent's minRegisteredRatio].

<<totalExpectedExecutors, totalExpectedExecutors>> is set to `0`.

It creates a link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc[YarnSchedulerEndpoint] (as `yarnSchedulerEndpoint`) and registers it as *YarnScheduler* with the link:spark-rpc.adoc[RPC Environment].

It sets the internal `askTimeout` link:spark-rpc.adoc#ask-timeout[Spark timeout for RPC ask operations] using the `SparkContext` constructor parameter.

It sets optional `appId` (of type `ApplicationId`), `attemptId` (for cluster mode only and of type `ApplicationAttemptId`).

It also creates `SchedulerExtensionServices` object (as `services`).

CAUTION: FIXME What is `SchedulerExtensionServices`?

The internal <<shouldResetOnAmRegister, shouldResetOnAmRegister>> flag is turned off.

=== [[sufficientResourcesRegistered]] sufficientResourcesRegistered

`sufficientResourcesRegistered` checks whether `totalRegisteredExecutors` is greater than or equals to `totalExpectedExecutors` multiplied by <<minRegisteredRatio, minRegisteredRatio>>.

NOTE: It overrides the parent's link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#sufficientResourcesRegistered[CoarseGrainedSchedulerBackend.sufficientResourcesRegistered].

CAUTION: FIXME Where's this used?

=== [[minRegisteredRatio]] minRegisteredRatio

`minRegisteredRatio` is set when <<creating-instance, `YarnSchedulerBackend` is created>>.

It is used in <<sufficientResourcesRegistered, sufficientResourcesRegistered>>.

=== [[start]] Starting the Backend (start method)

`start` creates a `SchedulerExtensionServiceBinding` object (using `SparkContext`, `appId`, and `attemptId`) and starts it (using `SchedulerExtensionServices.start(binding)`).

NOTE: A `SchedulerExtensionServices` object is created when <<initialization, YarnSchedulerBackend is initialized>> and available as `services`.

Ultimately, it calls the parent's link:../spark-executor-backends-CoarseGrainedExecutorBackend.adoc#start[CoarseGrainedSchedulerBackend.start].

[NOTE]
====
`start` throws `IllegalArgumentException` when the internal `appId` has not been set yet.

```
java.lang.IllegalArgumentException: requirement failed: application ID unset
```
====

=== [[stop]] Stopping the Backend (stop method)

`stop` calls the parent's link:../spark-executor-backends-CoarseGrainedExecutorBackend.adoc#requestTotalExecutors[CoarseGrainedSchedulerBackend.requestTotalExecutors] (using `(0, 0, Map.empty)` parameters).

CAUTION: FIXME Explain what `0, 0, Map.empty` means after the method's described for the parent.

It calls the parent's link:../spark-executor-backends-CoarseGrainedExecutorBackend.adoc#stop[CoarseGrainedSchedulerBackend.stop].

Ultimately, it stops the internal `SchedulerExtensionServiceBinding` object (using `services.stop()`).

CAUTION: FIXME Link the description of `services.stop()` here.

=== [[bindToYarn]] Recording Application and Attempt Ids (bindToYarn method)

[source, scala]
----
bindToYarn(appId: ApplicationId, attemptId: Option[ApplicationAttemptId]): Unit
----

`bindToYarn` sets the internal `appId` and `attemptId` to the value of the input parameters, `appId` and `attemptId`, respectively.

NOTE: <<start, start>> requires `appId`.

=== [[internal-registries]] Internal Registries

==== [[shouldResetOnAmRegister]] shouldResetOnAmRegister flag

When <<creating-instance, YarnSchedulerBackend is created>>, `shouldResetOnAmRegister` is disabled (i.e. `false`).

`shouldResetOnAmRegister` controls link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RegisterClusterManager[whether to reset `YarnSchedulerBackend` when another `RegisterClusterManager` RPC message arrives].

It allows resetting internal state after the initial  ApplicationManager failed and a new one was registered.

NOTE: It can only happen in link:spark-deploy-mode.adoc#client[client deploy mode].

=== [[settings]] Settings

==== [[spark.scheduler.minRegisteredResourcesRatio]] spark.scheduler.minRegisteredResourcesRatio

`spark.scheduler.minRegisteredResourcesRatio` (default: `0.8`)
