== [[ApplicationMaster]][[ExecutorLauncher]] ApplicationMaster (aka ExecutorLauncher)

`ApplicationMaster` class acts as the link:yarn/spark-yarn-introduction.adoc#ApplicationMaster[YARN ApplicationMaster] for a Spark application running on a YARN cluster (which is commonly called link:README.adoc[Spark on YARN]).

It uses <<allocator, YarnAllocator>> to manage YARN containers for executors.

`ApplicationMaster` is a <<main, standalone application>> that link:yarn/spark-yarn-introduction.adoc#NodeManager[YARN NodeManager] runs inside a YARN resource container and is responsible for the execution of a Spark application on YARN.

When <<creating-instance, created>> `ApplicationMaster` class is given a <<client, YarnRMClient>> (which is responsible for registering and unregistering a Spark application).

[NOTE]
====
<<ExecutorLauncher, ExecutorLauncher>> is a custom `ApplicationMaster` for link:../spark-deploy-mode.adoc#client[client deploy mode] only for the purpose of easily distinguishing client and cluster deploy modes when using `ps` or `jps`.

[options="wrap"]
----
$ jps -lm

71253 org.apache.spark.deploy.yarn.ExecutorLauncher --arg 192.168.99.1:50188 --properties-file /tmp/hadoop-jacek/nm-local-dir/usercache/jacek/appcache/application_1468961163409_0001/container_1468961163409_0001_01_000001/__spark_conf__/__spark_conf__.properties

70631 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
70934 org.apache.spark.deploy.SparkSubmit --master yarn --class org.apache.spark.repl.Main --name Spark shell spark-shell
71320 sun.tools.jps.Jps -lm
70731 org.apache.hadoop.yarn.server.nodemanager.NodeManager
----
====

`ApplicationMaster` (and `ExecutorLauncher`) is launched as a result of link:spark-yarn-client.adoc#createContainerLaunchContext[`Client` creating a `ContainerLaunchContext`] to launch Spark on YARN.

.Launching ApplicationMaster
image::../images/spark-yarn-ApplicationMaster-main.png[align="center"]

NOTE: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.html[ContainerLaunchContext] represents all of the information needed by the YARN NodeManager to launch a container.

=== [[client]] client Internal Reference to YarnRMClient

`client` is the internal reference to link:spark-yarn-yarnrmclient.adoc[YarnRMClient] that `ApplicationMaster` is given when <<creating-instance, created>>.

`client` is primarily used to <<registerAM, register the `ApplicationMaster` and request containers for executors from YARN>> and later <<unregister, unregister `ApplicationMaster` from YARN ResourceManager>>.

Besides, it helps obtaining <<getAttemptId, an application attempt id>> and link:spark-yarn-yarnrmclient.adoc#getMaxRegAttempts[the allowed number of attempts to register `ApplicationMaster`]. It also <<addAmIpFilter, gets filter parameters to secure ApplicationMaster's UI>>.

=== [[allocator]] allocator Internal Reference to YarnAllocator

`allocator` is the internal reference to link:spark-yarn-YarnAllocator.adoc[YarnAllocator] that `ApplicationMaster` uses to request new or release outstanding containers for executors.

It is link:spark-yarn-yarnrmclient.adoc#register[created] when <<registerAM, `ApplicationMaster` is registered>> (using the internal <<client, YarnRMClient reference>>).

=== [[main]] main

`ApplicationMaster` is started as a standalone command-line application inside a YARN container on a node.

NOTE: The command-line application is executed as a result of sending a `ContainerLaunchContext` request to launch `ApplicationMaster` to YARN ResourceManager (after link:spark-yarn-client.adoc#createContainerLaunchContext[creating the request for `ApplicationMaster`])

.Submitting ApplicationMaster to YARN NodeManager
image::../images/spark-yarn-ApplicationMaster-client-submitApplication.png[align="center"]

When executed, `main` first parses <<command-line-parameters, command-line parameters>> and then uses `SparkHadoopUtil.runAsSparkUser` to run the main code with a Hadoop `UserGroupInformation` as a thread local variable (distributed to child threads) for authenticating HDFS and YARN calls.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.SparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.SparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

You should see the following message in the logs:

```
DEBUG running as user: [user]
```

`SparkHadoopUtil.runAsSparkUser` function executes a block that <<creating-instance, creates a `ApplicationMaster`>> (passing the <<ApplicationMasterArguments, ApplicationMasterArguments>> instance and a brand new link:spark-yarn-yarnrmclient.adoc[YarnRMClient]) and then <<run, runs>> it.

=== [[command-line-parameters]][[ApplicationMasterArguments]] Command-Line Parameters (ApplicationMasterArguments class)

`ApplicationMaster` uses `ApplicationMasterArguments` class to handle command-line parameters.

`ApplicationMasterArguments` is created right after <<main, main>> method has been executed for `args` command-line parameters.

It accepts the following command-line parameters:

* `--jar JAR_PATH` -- the path to the Spark application's JAR file
* `--class CLASS_NAME` -- the name of the Spark application's main class
* `--arg ARG` -- an argument to be passed to the Spark application's main class. There can be multiple `--arg` arguments that are passed in order.
* `--properties-file FILE` -- the path to a custom Spark properties file.
* `--primary-py-file FILE` -- the main Python file to run.
* `--primary-r-file FILE` -- the main R file to run.

When an unsupported parameter is found the following message is printed out to standard error output and `ApplicationMaster` exits with the exit code `1`.

```
Unknown/unsupported param [unknownParam]

Usage: org.apache.spark.deploy.yarn.ApplicationMaster [options]
Options:
  --jar JAR_PATH       Path to your application's JAR file
  --class CLASS_NAME   Name of your application's main class
  --primary-py-file    A main Python file
  --primary-r-file     A main R file
  --arg ARG            Argument to be passed to your application's main class.
                       Multiple invocations are possible, each will be passed in order.
  --properties-file FILE Path to a custom Spark properties file.
```

=== [[registerAM]] Registering ApplicationMaster with YARN ResourceManager and Requesting Resources (registerAM method)

When <<runDriver, runDriver>> or <<runExecutorLauncher, runExecutorLauncher>> are executed, they use the private helper procedure `registerAM` to link:spark-yarn-yarnrmclient.adoc#register[register the `ApplicationMaster`] (with the link:spark-yarn-introduction.adoc#ResourceManager[YARN ResourceManager]) and link:spark-yarn-YarnAllocator.adoc#allocateResources[request resources] (given hints about where to allocate containers to be as close to the data as possible).

[source, scala]
----
registerAM(
  _rpcEnv: RpcEnv,
  driverRef: RpcEndpointRef,
  uiAddress: String,
  securityMgr: SecurityManager): Unit
----

Internally, it first reads link:spark-yarn-settings.adoc#spark.yarn.historyServer.address[spark.yarn.historyServer.address] setting and substitute Hadoop variables to create a complete address of the History Server, i.e. `[address]/history/[appId]/[attemptId]`.

CAUTION: FIXME substitute Hadoop variables?

Then, `registerAM` creates a link:../spark-rpc.adoc#RpcEndpointAddress[RpcEndpointAddress] for link:../spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#CoarseGrainedScheduler[CoarseGrainedScheduler RPC Endpoint] on the driver available on link:../spark-sparkenv.adoc#spark_driver_host[spark.driver.host] and link:../spark-sparkenv.adoc#spark_driver_port[spark.driver.port].

It link:spark-yarn-yarnrmclient.adoc#register[registers the `ApplicationMaster`] with the link:spark-yarn-introduction.adoc#ResourceManager[YARN ResourceManager] and link:spark-yarn-YarnAllocator.adoc#allocateResources[request resources] (given hints about where to allocate containers to be as close to the data as possible).

Ultimately, `registerAM` <<launchReporterThread, launches reporter thread>>.

.Registering ApplicationMaster with YARN ResourceManager
image::../images/spark-yarn-ApplicationMaster-registerAM.png[align="center"]

=== [[runDriver]] Running Driver in Cluster Mode (runDriver method)

[source, scala]
----
runDriver(securityMgr: SecurityManager): Unit
----

`runDriver` is a private procedure to...???

It starts by registering Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

It then starts the user class (with the driver) in a separate thread. You should see the following INFO message in the logs:

```
INFO Starting the user application in a separate Thread
```

CAUTION: FIXME Review `startUserApplication`.

You should see the following INFO message in the logs:

```
INFO Waiting for spark context initialization
```

CAUTION: FIXME Review `waitForSparkContextInitialized`

CAUTION: FIXME Finish...

=== [[runExecutorLauncher]] Running Executor Launcher (runExecutorLauncher method)

[source, scala]
----
runExecutorLauncher(securityMgr: SecurityManager): Unit
----

`runExecutorLauncher` reads link:spark-yarn-settings.adoc#spark.yarn.am.port[spark.yarn.am.port] (or assume `0`) and starts the `sparkYarnAM` RPC Environment (in client mode).

CAUTION: FIXME What's client mode?

It then waits for the driver to be available.

CAUTION: FIXME Review `waitForSparkDriver`

It registers Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

Ultimately, `runExecutorLauncher` <<registerAM, registers the `ApplicationMaster` and requests resources>> and waits until the <<reporterThread, reporterThread>> dies.

CAUTION: FIXME Describe `registerAM`

=== [[reporterThread]] reporterThread

CAUTION: FIXME

=== [[launchReporterThread]] launchReporterThread

CAUTION: FIXME

=== [[sparkContextInitialized]] Setting Internal SparkContext Reference (sparkContextInitialized methods)

[source, scala]
----
sparkContextInitialized(sc: SparkContext): Unit
----

`sparkContextInitialized` passes the call on to the `ApplicationMaster.sparkContextInitialized` that sets the internal `sparkContextRef` reference (to be `sc`).

=== [[sparkContextStopped]] Clearing Internal SparkContext Reference (sparkContextStopped methods)

[source, scala]
----
sparkContextStopped(sc: SparkContext): Boolean
----

`sparkContextStopped` passes the call on to the `ApplicationMaster.sparkContextStopped` that clears the internal `sparkContextRef` reference (i.e. sets it to `null`).

=== [[creating-instance]] Creating ApplicationMaster Instance

.ApplicationMaster's Dependencies
image::../images/spark-yarn-ApplicationMaster.png[align="center"]

When creating an instance of `ApplicationMaster` it requires <<ApplicationMasterArguments, ApplicationMasterArguments>> and link:spark-yarn-yarnrmclient.adoc[YarnRMClient].

It instantiates link:spark-configuration.adoc[SparkConf] and Hadoop's `YarnConfiguration` (using link:../varia/spark-hadoop.adoc#newConfiguration[SparkHadoopUtil.newConfiguration]).

It assumes link:spark-deploy-mode.adoc#cluster[cluster deploy mode] when <<command-line-parameters, `--class` was specified>>.

It computes the internal `maxNumExecutorFailures` using the optional link:spark-yarn-settings.adoc#spark.yarn.max.executor.failures[spark.yarn.max.executor.failures] if set. Otherwise, it is twice link:spark-executor.adoc#spark_executor_instances[spark.executor.instances] or link:spark-dynamic-allocation.adoc#spark_dynamicAllocation_maxExecutors[spark.dynamicAllocation.maxExecutors] (with dynamic allocation enabled) with the minimum of `3`.

It reads `yarn.am.liveness-monitor.expiry-interval-ms` (default: `120000`) from YARN to set the heartbeat interval. It is set to the minimum of the half of the YARN setting or link:spark-yarn-settings.adoc#spark.yarn.scheduler.heartbeat.interval-ms[spark.yarn.scheduler.heartbeat.interval-ms] with the minimum of `0`.

`initialAllocationInterval` is set to the minimum of the heartbeat interval or link:spark-yarn-settings.adoc#spark.yarn.scheduler.initial-allocation.interval[spark.yarn.scheduler.initial-allocation.interval].

It then <<localResources, loads the localized files>> (as set by the client).

CAUTION: FIXME Who's the client?

=== [[localResources]] localResources attribute

When <<creating-instance, `ApplicationMaster` is instantiated>>, it computes internal `localResources` collection of YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResource.html[LocalResource] by name based on the internal `spark.yarn.cache.*` configuration settings.

[source, scala]
----
localResources: Map[String, LocalResource]
----

You should see the following INFO message in the logs:

```
INFO ApplicationMaster: Preparing Local resources
```

It starts by reading the internal Spark configuration settings (that were earlier set when link:spark-yarn-client.adoc#prepareLocalResources[`Client` prepared local resources to distribute]):

* link:spark-yarn-settings.adoc#spark.yarn.cache.filenames[spark.yarn.cache.filenames]
* link:spark-yarn-settings.adoc#spark.yarn.cache.sizes[spark.yarn.cache.sizes]
* link:spark-yarn-settings.adoc#spark.yarn.cache.timestamps[spark.yarn.cache.timestamps]
* link:spark-yarn-settings.adoc#spark.yarn.cache.visibilities[spark.yarn.cache.visibilities]
* link:spark-yarn-settings.adoc#spark.yarn.cache.types[spark.yarn.cache.types]

For each file name in link:spark-yarn-settings.adoc#spark.yarn.cache.filenames[spark.yarn.cache.filenames] it maps link:spark-yarn-settings.adoc#spark.yarn.cache.types[spark.yarn.cache.types] to an appropriate YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResourceType.html[LocalResourceType] and creates a new YARN https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResource.html[LocalResource].

NOTE: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResource.html[LocalResource] represents a local resource required to run a container.

If link:spark-yarn-settings.adoc#spark.yarn.cache.confArchive[spark.yarn.cache.confArchive] is set, it is added to `localResources` as https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResourceType.html#ARCHIVE[ARCHIVE] resource type and https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/LocalResourceVisibility.html#PRIVATE[PRIVATE] visibility.

NOTE: link:spark-yarn-settings.adoc#spark.yarn.cache.confArchive[spark.yarn.cache.confArchive] is set when link:spark-yarn-client.adoc#prepareLocalResources[`Client` prepares local resources].

NOTE: `ARCHIVE` is an archive file that is automatically unarchived by the NodeManager.

NOTE: `PRIVATE` visibility means to share a resource among all applications of the same user on the node.

Ultimately, it removes the cache-related settings from the link:spark-configuration.adoc[Spark configuration] and system properties.

You should see the following INFO message in the logs:

```
INFO ApplicationMaster: Prepared Local resources [resources]
```

=== [[run]] Running ApplicationMaster (run method)

When `ApplicationMaster` is <<main, started as a standalone command-line application>> (in a YARN container on a node in a YARN cluster), ultimately `run` is executed.

[source, scala]
----
run(): Int
----

The result of calling `run` is the final result of the `ApplicationMaster` command-line application.

`run` sets <<cluster-mode-settings, cluster mode settings>>, registers the <<shutdown-hook, cleanup shutdown hook>>, schedules `AMDelegationTokenRenewer` and finally registers `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> for cluster mode or <<runExecutorLauncher, runExecutorLauncher>> for client mode).

After the <<cluster-mode-settings, cluster mode settings>> are set, `run` prints the following INFO message out to the logs:

```
INFO ApplicationAttemptId: [appAttemptId]
```

The `appAttemptId` is the link:spark-yarn-yarnrmclient.adoc#getAttemptId[current application attempt id] (using the constructor's link:spark-yarn-yarnrmclient.adoc[YarnRMClient] as `client`).

The <<shutdown-hook, cleanup shutdown hook>> is registered with shutdown priority lower than that of link:spark-sparkcontext.adoc[SparkContext] (so it is executed after `SparkContext`).

link:spark-security.adoc[SecurityManager] is instantiated with the internal link:spark-configuration.adoc[Spark configuration]. If the link:spark-yarn-settings.adoc#spark.yarn.credentials.file[credentials file config] (as `spark.yarn.credentials.file`) is present, a `AMDelegationTokenRenewer` is started.

CAUTION: FIXME Describe `AMDelegationTokenRenewer#scheduleLoginFromKeytab`

It finally runs `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> when in cluster mode or <<runExecutorLauncher, runExecutorLauncher>> otherwise).

It exits with `0` exit code.

In case of an exception, `run` prints the following ERROR message out to the logs:

```
ERROR Uncaught exception: [exception]
```

And the application run attempt is <<finish, finished>> with `FAILED` status and `EXIT_UNCAUGHT_EXCEPTION` (10) exit code.

=== [[cluster-mode-settings]] Cluster Mode Settings

When in <<isClusterMode, cluster mode>>, `ApplicationMaster` sets the following system properties (in <<run, run>>):

* link:spark-webui.adoc#spark_ui_port[spark.ui.port] as `0`
* link:spark-configuration.adoc#spark.master[spark.master] as `yarn`
* link:spark-deploy-mode.adoc#spark.submit.deployMode[spark.submit.deployMode] as `cluster`
* link:spark-yarn-settings.adoc#spark.yarn.app.id[spark.yarn.app.id] as application id

CAUTION: FIXME Why are the system properties required? Who's expecting them?

=== [[cluster-mode]][[isClusterMode]] isClusterMode Internal Flag

CAUTION: FIXME link:spark-yarn-client.adoc#isClusterMode[Since `org.apache.spark.deploy.yarn.ExecutorLauncher` is used for client deploy mode], the `isClusterMode` flag could be set there (not depending on `--class` which is correct yet not very obvious).

`isClusterMode` is an internal flag that is enabled (i.e. `true`) for link:../spark-deploy-mode.adoc#cluster[cluster mode].

Specifically, it says whether the main class of the Spark application (through <<command-line-parameters, `--class` command-line argument>>) was specified or not. That is how the developers decided to inform `ApplicationMaster` about being run in link:../spark-deploy-mode.adoc#cluster[cluster mode] when link:spark-yarn-client.adoc#createContainerLaunchContext[`Client` creates YARN's `ContainerLaunchContext` (for launching `ApplicationMaster`)].

It is used to set <<cluster-mode-settings, additional system properties>> in <<run, run>> and <<runDriver, runDriver>> (the flag is enabled) or <<runExecutorLauncher, runExecutorLauncher>> (when disabled).

Besides, it controls the <<getDefaultFinalStatus, default final status of a Spark application>> being `FinalApplicationStatus.FAILED` (when the flag is enabled) or `FinalApplicationStatus.UNDEFINED`.

The flag also controls whether to set system properties in <<addAmIpFilter, addAmIpFilter>> (when the flag is enabled) or <<addAmIpFilter, send a `AddWebUIFilter` instead>>.

=== [[unregister]] Unregistering ApplicationMaster from YARN ResourceManager (unregister method)

`unregister` unregisters the `ApplicationMaster` for the Spark application from the link:spark-yarn-introduction.adoc#ResourceManager[YARN ResourceManager].

[source, scala]
----
unregister(status: FinalApplicationStatus, diagnostics: String = null): Unit
----

NOTE: It is called from the <<shutdown-hook, cleanup shutdown hook>> (that was registered in `ApplicationMaster` when it <<run, started running>>) and only when the application's final result is successful or it was the last attempt to run the application.

It first checks that the `ApplicationMaster` has not already been unregistered (using the internal `unregistered` flag). If so, you should see the following INFO message in the logs:

```
INFO ApplicationMaster: Unregistering ApplicationMaster with [status]
```

There can also be an optional diagnostic message in the logs:

```
(diag message: [msg])
```

The internal `unregistered` flag is set to be enabled, i.e. `true`.

It then requests link:spark-yarn-yarnrmclient.adoc#unregister[`YarnRMClient` to unregister].

=== [[shutdown-hook]] Cleanup Shutdown Hook

When <<run, `ApplicationMaster` starts running>>, it registers a shutdown hook that <<unregister, unregisters the Spark application from the YARN ResourceManager>> and <<cleanupStagingDir, cleans up the staging directory>>.

Internally, it checks the internal `finished` flag, and if it is disabled, it <<finish, marks the Spark application as failed with `EXIT_EARLY`>>.

If the internal `unregistered` flag is disabled, it <<unregister, unregisters the Spark application>> and <<cleanupStagingDir, cleans up the staging directory>> afterwards only when the final status of the ApplicationMaster's registration is `FinalApplicationStatus.SUCCEEDED` or the link:README.adoc#multiple-application-attempts[number of application attempts is more than allowed].

The shutdown hook runs after the SparkContext is shut down, i.e. the shutdown priority is one less than SparkContext's.

The shutdown hook is registered using Spark's own `ShutdownHookManager.addShutdownHook`.

=== [[finish]] finish

CAUTION: FIXME

=== [[ExecutorLauncher]] ExecutorLauncher

`ExecutorLauncher` comes with no extra functionality when compared to `ApplicationMaster`. It serves as a helper class to run `ApplicationMaster` under another class name in link:spark-deploy-mode.adoc#client[client deploy mode].

With the two different class names (pointing at the same class `ApplicationMaster`) you should be more successful to distinguish between `ExecutorLauncher` (which is really a `ApplicationMaster`) in link:spark-deploy-mode.adoc#client[client deploy mode] and the `ApplicationMaster` in link:spark-deploy-mode.adoc#cluster[cluster deploy mode] using tools like `ps` or `jps`.

NOTE: Consider `ExecutorLauncher` a `ApplicationMaster` for client deploy mode.

=== [[getAttemptId]] Obtain Application Attempt Id (getAttemptId method)

[source, scala]
----
getAttemptId(): ApplicationAttemptId
----

`getAttemptId` returns YARN's `ApplicationAttemptId` (of the Spark application to which the container was assigned).

Internally, it queries YARN by means of link:spark-yarn-yarnrmclient.adoc#getAttemptId[YarnRMClient].

=== [[addAmIpFilter]] addAmIpFilter helper method

[source, scala]
----
addAmIpFilter(): Unit
----

`addAmIpFilter` is a helper method that ...???

It starts by reading Hadoop's environmental variable https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/ApplicationConstants.html#APPLICATION_WEB_PROXY_BASE_ENV[ApplicationConstants.APPLICATION_WEB_PROXY_BASE_ENV] that it passes to link:spark-yarn-yarnrmclient.adoc#getAmIpFilterParams[`YarnRMClient` to compute the configuration for the `AmIpFilter` for web UI].

In cluster deploy mode (when `ApplicationMaster` runs with web UI), it sets `spark.ui.filters` system property as `org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter`. It also sets system properties from the key-value configuration of `AmIpFilter` (computed earlier) as `spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.[key]` being `[value]`.

In client deploy mode (when `ApplicationMaster` runs on another JVM or even host than web UI), it simply sends a `AddWebUIFilter` to `ApplicationMaster` (namely to link:spark-yarn-AMEndpoint.adoc[AMEndpoint RPC Endpoint]).
