== [[Optimizer]] Logical Query Plan Optimizer

*Catalyst* is a Spark SQL framework for manipulating trees. It can work with trees of relational operators and expressions in link:spark-sql-catalyst-LogicalPlan.adoc[logical plans] before they end up as link:spark-sql-catalyst-SparkPlan.adoc[physical execution plans].

[source, scala]
----
scala> sql("select 1 + 1 + 1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias(((1 + 1) + 1), None)]
+- OneRowRelation$

== Analyzed Logical Plan ==
((1 + 1) + 1): int
Project [((1 + 1) + 1) AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Optimized Logical Plan ==
Project [3 AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Physical Plan ==
*Project [3 AS ((1 + 1) + 1)#4]
+- Scan OneRowRelation[]
----

Spark 2.0 uses Catalyst's tree manipulation library to build an extensible *query plan optimizer* with a number of query optimizations.

.(A subset of) Catalyst Plan Optimizer's Logical Plan Optimizations
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Name | Description
| link:spark-sql-catalyst-constant-folding.adoc[Constant Folding] |
| link:spark-sql-catalyst-optimizer-PushDownPredicate.adoc[Predicate Pushdown] |
| link:spark-sql-catalyst-nullability-propagation.adoc[Nullability (NULL Value) Propagation] |
| link:spark-sql-catalyst-vectorized-parquet-decoder.adoc[Vectorized Parquet Decoder] |
| link:spark-sql-catalyst-optimizer-CombineTypedFilters.adoc[Combine Typed Filters] |
| link:spark-sql-catalyst-optimizer-PropagateEmptyRelation.adoc[Propagate Empty Relation] |
| link:spark-sql-catalyst-optimizer-SimplifyCasts.adoc[Simplify Casts] |
| link:spark-sql-catalyst-optimizer-ColumnPruning.adoc[Column Pruning] |
| link:spark-sql-catalyst-current-database-time.adoc[GetCurrentDatabase / ComputeCurrentTime] |
| link:spark-sql-catalyst-current-database-time.adoc[Eliminate Serialization] |
|======================

Catalyst supports both rule-based and cost-based optimization.

CAUTION: Review https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala[sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala].

=== [[SparkOptimizer]] SparkOptimizer -- The Default Logical Query Plan Optimizer

`SparkOptimizer` is a custom <<Optimizer, logical query plan optimizer>> for a link:spark-sql-sessionstate.adoc#SessionCatalog[SessionCatalog] and link:spark-sql-SQLConf.adoc[SQLConf] with its own additional optimization batches:

1. *Optimize Metadata Only Query*
2. *Extract Python UDF from Aggregate*
3. *User Provided Optimizers* with the input `ExperimentalMethods`.

CAUTION: FIXME Review the batches. What is `ExperimentalMethods`?

`SparkOptimizer` is the default query optimizer that is available as link:spark-sql-sessionstate.adoc#optimizer[`optimizer` attribute of `SessionState`].

You can see the result of executing `SparkOptimizer` on a query plan using link:spark-sql-query-execution.adoc#optimizedPlan[`optimizedPlan` attribute of `QueryExecution`].

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[i-want-more]] Further reading or watching

1. https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]

2. (video) https://youtu.be/_1byVWTEK1s?t=19m7s[Modern Spark DataFrame and Dataset (Intermediate Tutorial)] by https://twitter.com/adbreind[Adam Breindel] from Databricks.
