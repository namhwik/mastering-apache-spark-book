== [[DataType]] Data Types

`DataType` abstract class is the base type of all <<built-in-data-types, built-in data types>> in Spark SQL, e.g. strings, longs. It is possible to extend the type system by creating <<user-defined-types, user-defined types>>.

The <<contract, DataType Contract>> defines methods to build SQL, JSON and string representations.

`DataType` (and the concrete Spark SQL types) live in `org.apache.spark.sql.types` package.

[source, scala]
----
import org.apache.spark.sql.types.StringType

scala> StringType.json
res0: String = "string"

scala> StringType.sql
res1: String = STRING

scala> StringType.catalogString
res2: String = string
----

You should use <<DataTypes, DataTypes>> object in your code to create complex Spark SQL types, i.e. arrays or maps.

[source, scala]
----
import org.apache.spark.sql.types.DataTypes

scala> val arrayType = DataTypes.createArrayType(BooleanType)
arrayType: org.apache.spark.sql.types.ArrayType = ArrayType(BooleanType,true)

scala> val mapType = DataTypes.createMapType(StringType, LongType)
mapType: org.apache.spark.sql.types.MapType = MapType(StringType,LongType,true)
----

`DataType` has support for Scala's pattern matching using `unapply` method.

[source, scala]
----
???
----

=== [[contract]] DataType Contract

Any type in Spark SQL follows the `DataType` contract which means that the types define the following methods:

* `json` and `prettyJson` to build JSON representations of a data type
* `defaultSize` to know the default size of values of a type
* `simpleString` and `catalogString` to build user-friendly string representations (with the latter for external catalogs)
* `sql` to build SQL representation

=== [[built-in-data-types]] Built-In Data Types

Spark SQL comes with the following built-in <<DataType, data types>>:

* `CalendarIntervalType`
* link:spark-sql-schema.adoc#StructType[StructType]
* `MapType`
* `ArrayType`
* `NullType`

* Atomic Types
** `TimestampType`
** `StringType`
** `BooleanType`
** `DateType`
** `BinaryType`

* Fractional Types
** `DoubleType`
** `FloatType`

* Integral Types
** `ByteType`
** `IntegerType`
** `LongType`
** `ShortType`

You can also create your own <<user-defined-types, user-defined types (UDTs)>>.

=== [[DataTypes]] DataTypes -- Factory Methods for Complex DataTypes

`DataTypes` is an Scala object with methods to create complex Spark SQL types, i.e. arrays and maps. It lives in `org.apache.spark.sql.types` package.

[source, scala]
----
import org.apache.spark.sql.types.DataTypes

scala> val arrayType = DataTypes.createArrayType(BooleanType)
arrayType: org.apache.spark.sql.types.ArrayType = ArrayType(BooleanType,true)

scala> val mapType = DataTypes.createMapType(StringType, LongType)
mapType: org.apache.spark.sql.types.MapType = MapType(StringType,LongType,true)
----

TIP: `DataTypes` offers singleton objects for Spark SQL data types but they seem to be only for Java programmers. Since the data types are also defined as `case object` alongside their definitions, you are lucky to code in Scala and have no reason to use `DataTypes` objects to access the types. Just import the package and you have access to the objects.

=== [[user-defined-types]] UDTs -- User-Defined Types

CAUTION: FIXME
