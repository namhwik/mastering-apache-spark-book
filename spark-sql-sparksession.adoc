== [[SparkSession]] SparkSession -- Entry Point to Datasets

`SparkSession` is the entry point to developing Spark applications using the link:spark-sql-dataset.adoc[Dataset] and (less preferred these days) link:spark-sql-dataframe.adoc[DataFrame] APIs.

You should use the <<builder, builder>> method to create an instance of `SparkSession`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .getOrCreate()
----

The `private` more direct API to create a `SparkSession` requires a link:spark-sparkcontext.adoc[SparkContext] and an optional <<SharedState, SharedState>> (that represents the shared state across `SparkSession` instances).

[NOTE]
====
`SparkSession` has replaced link:spark-sql-sqlcontext.adoc[SQLContext] as of Spark *2.0.0*.
====

=== [[implicits]] Implicits -- SparkSession.implicits

The `implicits` object is a helper class with methods to convert objects to link:spark-sql-dataset.adoc[Datasets] and link:spark-sql-dataframe.adoc[DataFrames], and also comes with many link:spark-sql-Encoder.adoc[Encoders] for "primitive" types as well as the collections thereof.

[NOTE]
====
Import the implicits by `import spark.implicits._` as follows:

[source, scala]
----
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
----
====

It holds link:spark-sql-Encoder.adoc[Encoders] for Scala "primitive" types like `Int`, `Double`, `String`, and their products and collections.

It offers support for creating `Dataset` from `RDD` of any type (for which an link:spark-sql-Encoder.adoc[encoder] exists in scope), or case classes or tuples, and `Seq`.

It also offers conversions from Scala's `Symbol` or `$` to `Column`.

It also offers conversions from `RDD` or `Seq` of `Product` types (e.g. case classes or tuples) to `DataFrame`. It has direct conversions from `RDD` of `Int`, `Long` and `String` to `DataFrame` with a single column name `_1`.

NOTE: It is not possible to call `toDF` methods on `RDD` objects of "primitive" types but `Int`, `Long`, and `String`.

=== [[readStream]] readStream

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[emptyDataset]] emptyDataset

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` creates an empty link:spark-sql-dataset.adoc[Dataset] (assuming that future records being of type `T`).

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

The link:spark-sql-logical-plan.adoc[LogicalPlan] is `LocalRelation`.

=== [[createDataset]] createDataset methods

[source, scala]
----
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
----

`createDataset` creates a link:spark-sql-dataset.adoc[Dataset] from the local collection or the distributed `RDD`.

[source, scala]
----
scala> val nums = spark.createDataset(1 to 5)
nums: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> nums.show
+-----+
|value|
+-----+
|    1|
|    2|
|    3|
|    4|
|    5|
+-----+
----

The link:spark-sql-logical-plan.adoc[LogicalPlan] is `LocalRelation` (for the input `data` collection) or `LogicalRDD` (for the input `RDD[T]`).

=== [[range]] range methods

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` family of methods create a link:spark-sql-dataset.adoc[Dataset] of longs.

[source, scala]
----
scala> spark.range(0, 10, 2, 5).show
+---+
| id|
+---+
|  0|
|  2|
|  4|
|  6|
|  8|
+---+
----

=== [[emptyDataFrame]] emptyDataFrame

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty `DataFrame` (with no rows and columns).

It calls <<createDataFrame, createDataFrame>> with an empty `RDD[Row]` and an empty schema link:spark-sql-dataframe-structtype.adoc[StructType(Nil)].

=== [[createDataFrame]] createDataFrame method

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

=== [[streams]] streams Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-sessionstate.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[udf]] udf Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to `UDFRegistration` that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL queries.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("select myUpper(value) from strs").show
+----------+
|UDF(value)|
+----------+
|         A|
|         B|
|         C|
+----------+
----

Internally, it uses link:spark-sql-sessionstate.adoc#udf[SessionState.udf].

=== [[catalog]] catalog Attribute

`catalog` attribute is an interface to the current link:spark-sql-Catalog.adoc[catalog] (of databases, tables, functions, table columns, and temporary views).

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

=== [[table]] table method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` creates a link:spark-sql-dataframe.adoc[DataFrame] from records in the `tableName` table (if exists).

[source, scala]
----
val df = spark.table("mytable")
----

=== [[streamingQueryManager]] streamingQueryManager Attribute

`streamingQueryManager` is...

=== [[listenerManager]] listenerManager Attribute

`listenerManager` is...

=== [[ExecutionListenerManager]] ExecutionListenerManager

`ExecutionListenerManager` is...

=== [[functionRegistry]] functionRegistry Attribute

`functionRegistry` is...

=== [[experimentalMethods]] experimentalMethods Attribute

`experimentalMethods` is...

=== [[newSession]] newSession method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-sparkcontext.adoc[SparkContext] and <<SharedState, SharedState>>).

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[sharedState]] sharedState Attribute

`sharedState` points at the current <<SharedState, SharedState>>.

=== [[SharedState]] SharedState

`SharedState` represents the shared state across all active SQL sessions (i.e. <<SparkSession, SparkSession>> instances) by sharing link:spark-cachemanager.adoc[CacheManager], link:spark-webui-SQLListener.adoc[SQLListener], and `ExternalCatalog`.

There are two implementations of `SharedState`:

* `org.apache.spark.sql.internal.SharedState` (default)
* `org.apache.spark.sql.hive.HiveSharedState`

You can select `SharedState` for the active `SparkSession` using  link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting.

`SharedState` is created lazily, i.e. when first accessed after <<creating-instance, `SparkSession` is created>>. It can happen when a <<newSession, new session is created>> or when the shared services are accessed. It is created with a link:spark-sparkcontext.adoc[SparkContext].

=== [[creating-instance]] Creating SparkSession Instance

CAUTION: FIXME

=== [[createDataset]] Creating Datasets (createDataset methods)

[source, scala]
----
createDataset[T: Encoder](data: Seq[T]): Dataset[T]
createDataset[T: Encoder](data: RDD[T]): Dataset[T]

// For Java
createDataset[T: Encoder](data: java.util.List[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]` or Java's `List[T]`, or an `RDD[T]`.

[source, scala]
----
val ints = spark.createDataset(0 to 9)
----

NOTE: You'd rather not be using `createDataset` since you have the link:spark-sql-dataset.adoc#implicits[Scala implicits and `toDS` method].

=== [[read]] Accessing DataFrameReader (read method)

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration (conf attribute)

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] sessionState

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

`sessionState` is a lazily-created value based on the internal <<spark.sql.catalogImplementation, spark.sql.catalogImplementation>> setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` when the setting is `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`.

=== [[sql]] Executing SQL (sql method)

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, it creates a link:spark-sql-dataset.adoc[Dataset] using the current `SparkSession` and the plan (based on the input `sqlText` and parsed using link:spark-sql-sql-parsers.adoc#ParserInterface[ParserInterface.parsePlan] available using <<sessionState, sessionState.sqlParser>>).

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[builder]] Creating SessionBuilder (builder method)

[source, scala]
----
builder(): Builder
----

`SessionBuilder.builder` method creates a new `SparkSession.Builder` to build a `SparkSession` off it using a fluent API.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----

=== [[settings]] Settings

==== [[spark.sql.catalogImplementation]] spark.sql.catalogImplementation

`spark.sql.catalogImplementation` (default: `in-memory`) is an internal setting with two possible values: `hive` and `in-memory`.
