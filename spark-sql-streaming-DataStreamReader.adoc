== DataStreamReader

`DataStreamReader` is an interface for <<load, reading>> streaming data in link:spark-sql-dataframe.adoc[DataFrame] from data sources with specified <<format, format>>, <<schema, schema>> and <<options, options>>.

`DataStreamReader` offers support for the built-in formats: <<json, json>>, <<csv, csv>>, <<parquet, parquet>>, <<text, text>>. `parquet` format is the default data source as configured using link:spark-sql-settings.adoc#spark.sql.sources.default[spark.sql.sources.default] setting.

`DataStreamReader` is available using link:spark-sql-sparksession.adoc#readStream[SparkSession.readStream] method.

[source, scala]
----
val spark: SparkSession = ...

val schema = spark.read
  .format("csv")
  .option("header", true)
  .option("inferSchema", true)
  .load("csv-logs/*.csv")
  .schema

val df = spark.readStream
  .format("csv")
  .schema(schema)
  .load("csv-logs/*.csv")
----

=== [[format]] format

[source, scala]
----
format(source: String): DataStreamReader
----

`format` specifies the `source` format of the streaming data source.

=== [[schema]] schema

[source, scala]
----
schema(schema: StructType): DataStreamReader
----

`schema` specifies the `schema` of the streaming data source.

=== [[option]] option Methods

[source, scala]
----
option(key: String, value: String): DataStreamReader
option(key: String, value: Boolean): DataStreamReader
option(key: String, value: Long): DataStreamReader
option(key: String, value: Double): DataStreamReader
----

`option` family of methods specifies additional options to a streaming data source.

There is support for values of `String`, `Boolean`, `Long`, and `Double` types for user convenience, and internally are converted to `String` type.

NOTE: You can also set options in bulk using <<options, options>> method. You have to do the type conversion yourself, though.

=== [[options]] options

[source, scala]
----
options(options: scala.collection.Map[String, String]): DataStreamReader
----

`options` method allows specifying one or many options of the streaming input data source.

NOTE: You can also set options one by one using <<option, option>> method.

=== [[load]] load Methods

[source, scala]
----
load(): DataFrame
load(path: String): DataFrame // <1>
----
<1> Specifies `path` option before passing calls to `load()`

`load` loads streaming input data as link:spark-sql-dataframe.adoc[DataFrame].

Internally, `load` creates a `DataFrame` from the current link:spark-sql-sparksession.adoc[SparkSession] and a link:spark-sql-streaming-streamingrelation.adoc[StreamingRelation] (of a link:spark-sql-datasource.adoc[DataSource] based on <<schema, schema>>, <<format, format>>, and <<options, options>>).

=== [[builtin-formats]][[json]][[csv]][[parquet]][[text]] Built-in Formats

[source, scala]
----
json(path: String): DataFrame
csv(path: String): DataFrame
parquet(path: String): DataFrame
text(path: String): DataFrame
----

`DataStreamReader` can load streaming data from data sources of the following <<format, formats>>:

* `json`
* `csv`
* `parquet`
* `text`

The methods simply pass calls to <<format, `format`>> followed by <<load, `load(path)`>>.
