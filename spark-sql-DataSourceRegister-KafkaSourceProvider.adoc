== [[KafkaSourceProvider]] KafkaSourceProvider

`KafkaSourceProvider` is an link:spark-sql-DataSourceRegister.adoc[interface to register] Apache Kafka as a data source.

`KafkaSourceProvider` is a link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider] and link:spark-sql-RelationProvider.adoc[RelationProvider].

`KafkaSourceProvider` is registered under `kafka` alias.

[source, scala]
----
// start Spark application like spark-shell with the following package
// --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT
scala> val fromKafkaTopic1 = spark.
  read.
  format("kafka").
  option("subscribe", "topic1").  // subscribe, subscribepattern, or assign
  option("kafka.bootstrap.servers", "localhost:9092").
  load("gauge_one")
----

`KafkaSourceProvider` <<sourceSchema, uses a fixed schema>> (and makes sure that a user did not set a custom one).

[source, scala]
----
import org.apache.spark.sql.types.StructType
val schema = new StructType().add($"id".int)
scala> spark
  .read
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .schema(schema) // <-- defining a custom schema is not supported
  .load
org.apache.spark.sql.AnalysisException: kafka does not allow user-specified schemas.;
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:307)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
  ... 48 elided
----


=== [[createRelation-RelationProvider]] `createRelation` Method (from RelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  parameters: Map[String, String]): BaseRelation
----

CAUTION: FIXME

NOTE: `createRelation` is a part of link:spark-sql-RelationProvider.adoc[RelationProvider Contract].

=== [[createRelation-CreatableRelationProvider]] `createRelation` Method (from CreatableRelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  mode: SaveMode,
  parameters: Map[String, String],
  df: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `createRelation` is a part of link:spark-sql-CreatableRelationProvider.adoc#contract[CreatableRelationProvider Contract].

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(
  sqlContext: SQLContext,
  metadataPath: String,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): Source
----

CAUTION: FIXME

NOTE: `createSource` is a part of Structured Streaming's `StreamSourceProvider` Contract.

=== [[sourceSchema]] `sourceSchema` Method

[source, scala]
----
sourceSchema(
  sqlContext: SQLContext,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): (String, StructType)
----

CAUTION: FIXME

[source, scala]
----
val fromKafka = spark.read.format("kafka")...
scala> fromKafka.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

NOTE: `sourceSchema` is a part of Structured Streaming's `StreamSourceProvider` Contract.
