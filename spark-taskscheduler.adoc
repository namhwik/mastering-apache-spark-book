== [[TaskScheduler]] TaskScheduler

A `TaskScheduler` schedules link:spark-taskscheduler-tasks.adoc[tasks] for a link:spark-anatomy-spark-application.adoc[single Spark application] according to link:spark-taskscheduler-schedulingmode.adoc[scheduling mode].

.TaskScheduler works for a single SparkContext
image::images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png[align="center"]

A `TaskScheduler` gets sets of tasks (as link:spark-taskscheduler-tasksets.adoc[TaskSets]) submitted to it from the link:spark-dagscheduler.adoc[DAGScheduler] for each stage, and is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.

NOTE: `TaskScheduler` is a `private[spark]` Scala trait. You can find the sources in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala[org.apache.spark.scheduler.TaskScheduler].

=== [[contract]] TaskScheduler Contract

Every `TaskScheduler` follows the following contract:

* It can be <<start, started>>.
* It can be <<stop, stopped>>.
* It can <<postStartHook, do post-start initialization>> if needed for additional post-start initialization.
* It <<submitTasks, submits TaskSets for execution>>.
* It can <<cancelTasks, cancel tasks for a stage>>.
* It can <<setDAGScheduler, set a custom DAGScheduler>>.
* It can calculate <<defaultParallelism, the default level of parallelism>>.
* It <<applicationId, returns a custom application id>>.
* It <<applicationAttemptId, returns an application attempt id>>.
* It can handle <<executorHeartbeatReceived, executor's heartbeats>> and <<executorLost, executor lost events>>.

[[rootPool]]
* It has a `rootPool` link:spark-taskscheduler-pool.adoc[Pool] (of link:spark-taskscheduler-schedulable.adoc[Schedulables]).

[[schedulingMode]]
* It can put tasks in order according to a link:spark-taskscheduler-schedulingmode.adoc[scheduling policy] (as `schedulingMode`). It is used in link:spark-sparkcontext.adoc#getSchedulingMode[SparkContext.getSchedulingMode].

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

=== [[lifecycle]] TaskScheduler's Lifecycle

A `TaskScheduler` is created while link:spark-sparkcontext.adoc#creating-instance[SparkContext is being created] (by calling link:spark-sparkcontext-creating-instance-internals.adoc#createTaskScheduler[SparkContext.createTaskScheduler] for a given link:spark-deployment-environments.adoc[master URL] and link:spark-submit.adoc#deploy-mode[deploy mode]).

.TaskScheduler uses SchedulerBackend to support different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

At this point in SparkContext's lifecycle, the internal `_taskScheduler` points at the `TaskScheduler` (and it is "announced" by sending a blocking link:spark-sparkcontext-HeartbeatReceiver.adoc#TaskSchedulerIsSet[`TaskSchedulerIsSet` message to HeartbeatReceiver RPC endpoint]).

The <<start, TaskScheduler is started>> right after the blocking `TaskSchedulerIsSet` message receives a response.

The <<applicationId, application ID>> and the <<applicationAttemptId, application's attempt ID>> are set at this point (and `SparkContext` uses the application id to set up `spark.app.id`, link:spark-webui.adoc#SparkUI[SparkUI], and link:spark-blockmanager.adoc[BlockManager]).

CAUTION: FIXME The application id is described as "associated with the job." in TaskScheduler, but I think it is "associated with the application" and you can have many jobs per application.

Right before SparkContext is fully initialized, <<postStartHook, TaskScheduler.postStartHook>> is called.

The internal `_taskScheduler` is cleared (i.e. set to `null`) while link:spark-sparkcontext.adoc#stop[SparkContext is being stopped].

<<stop, TaskScheduler is stopped>> while link:spark-dagscheduler.adoc#stop[DAGScheduler is being stopped].

WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now?

=== [[start]] Starting TaskScheduler -- `start` Method

[source, scala]
----
start(): Unit
----

`start` is currently called while link:spark-sparkcontext.adoc#creating-instance[SparkContext is being created].

=== [[stop]] Stopping TaskScheduler -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` is currently called while link:spark-dagscheduler.adoc#stop[DAGScheduler is being stopped].

=== [[postStartHook]] Post-Start Initialization -- `postStartHook` Method

[source, scala]
----
postStartHook() {}
----

`postStartHook` does nothing by default, but allows custom implementations to do some post-start initialization.

NOTE: It is currently called right before link:spark-sparkcontext.adoc#creating-instance[SparkContext's initialization finishes].

=== [[submitTasks]] Submitting TaskSets for Execution -- `submitTasks` Method

[source, scala]
----
submitTasks(taskSet: TaskSet): Unit
----

`submitTasks` accepts a link:spark-taskscheduler-tasksets.adoc[TaskSet] for execution.

NOTE: It is currently called by link:spark-dagscheduler.adoc#submitMissingTasks[DAGScheduler when there are tasks to be executed for a stage].

=== [[cancelTasks]] Cancelling Tasks for Stage -- `cancelTasks` Method

[source, scala]
----
cancelTasks(stageId: Int, interruptThread: Boolean): Unit
----

`cancelTasks` cancels all tasks submitted for execution in a stage `stageId`.

NOTE: It is currently called by link:spark-dagscheduler.adoc#failJobAndIndependentStages[`DAGScheduler` when it cancels a stage].

=== [[setDAGScheduler]] Setting DAGScheduler -- `setDAGScheduler` Method

[source, scala]
----
setDAGScheduler(dagScheduler: DAGScheduler): Unit
----

`setDAGScheduler` sets the current `DAGScheduler`.

NOTE: It is currently called by link:spark-dagscheduler.adoc#creating-instance[DAGScheduler when it is created].

=== [[defaultParallelism]] Calculating Default Level of Parallelism -- `defaultParallelism` Method

[source, scala]
----
defaultParallelism(): Int
----

`defaultParallelism` calculates the default level of parallelism to use in a Spark application as the number of partitions in RDDs and also as a hint for sizing jobs.

NOTE: It is called by link:spark-sparkcontext.adoc#defaultParallelism[`SparkContext` for its `defaultParallelism`].

TIP: Read more in link:spark-taskschedulerimpl.adoc#defaultParallelism[Calculating Default Level of Parallelism (defaultParallelism method)] for the one and only implementation of the `TaskScheduler` contract -- `TaskSchedulerImpl`.

=== [[applicationId]] Calculating Application ID -- `applicationId` Method

[source, scala]
----
applicationId(): String
----

`applicationId` gives the current application's id. It is in the format `spark-application-[System.currentTimeMillis]` by default.

NOTE: It is currently used in link:spark-sparkcontext.adoc#creating-instance[SparkContext while it is being initialized].

=== [[applicationAttemptId]] Calculating Application Attempt ID -- `applicationAttemptId` Method

[source, scala]
----
applicationAttemptId(): Option[String]
----

`applicationAttemptId` gives the current application's attempt id.

NOTE: It is currently used in link:spark-sparkcontext.adoc#creating-instance[SparkContext while it is being initialized].

=== [[executorHeartbeatReceived]] Handling Executor's Heartbeats -- `executorHeartbeatReceived` Method

[source, scala]
----
executorHeartbeatReceived(
  execId: String,
  accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
  blockManagerId: BlockManagerId): Boolean
----

`executorHeartbeatReceived` handles heartbeats from an executor `execId` with the partial values of accumulators and `BlockManagerId`.

It is expected to be positive (i.e. return `true`) when the executor `execId` is managed by the `TaskScheduler`.

NOTE: It is currently used in link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint in SparkContext to handle heartbeats from executors].

=== [[executorLost]] Handling Executor Lost Events -- `executorLost` Method

[source, scala]
----
executorLost(executorId: String, reason: ExecutorLossReason): Unit
----

`executorLost` handles events about an executor `executorId` being lost for a given `reason`.

NOTE: It is currently used in link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint in SparkContext to process host expiration events] and to remove executors in scheduler backends.

=== [[implementations]] Available Implementations

Spark comes with the following task schedulers:

* link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl]
* link:yarn/spark-yarn-yarnscheduler.adoc[YarnScheduler] - the TaskScheduler for link:yarn/README.adoc[Spark on YARN] in link:spark-submit.adoc#deploy-mode[client deploy mode].
* link:yarn/spark-yarn-yarnclusterscheduler.adoc[YarnClusterScheduler] - the TaskScheduler for link:yarn/README.adoc[Spark on YARN] in link:spark-submit.adoc#deploy-mode[cluster deploy mode].
