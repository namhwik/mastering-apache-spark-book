== Deployment Environments -- Run Modes

Spark Deployment Environments (aka Run Modes):

* link:spark-local.adoc[local]
* link:spark-cluster.adoc[clustered]
** link:spark-standalone.adoc[Spark Standalone]
** link:spark-mesos.adoc[Spark on Apache Mesos]
** link:yarn/README.adoc[Spark on Hadoop YARN]

A Spark application can run locally (on a single JVM) or on the cluster which uses a cluster manager and the deploy mode (`--deploy-mode`). See link:spark-submit.adoc[spark-submit script].

=== [[master-urls]] Master URLs

Spark supports the following *master URLs* (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2729-L2742[private object SparkMasterRegex]):

* *local*, *local[N]* and *local[{asterisk}]* for link:spark-local.adoc#masterURL[Spark local]
* *local[N, maxRetries]* for link:spark-local.adoc#masterURL[Spark local-with-retries]
* *local-cluster[N, cores, memory]* for simulating a Spark cluster of [N, cores, memory] locally
* *spark://host:port,host1:port1,...* for connecting to link:spark-standalone.adoc[Spark Standalone cluster(s)]
* *mesos://* or *zk://* for link:spark-mesos.adoc[Spark on Mesos cluster]
* *yarn-cluster* (deprecated: *yarn-standalone*) for link:yarn/README.adoc[Spark on YARN (cluster mode)]
* *yarn-client* for link:yarn/README.adoc[Spark on YARN cluster (client mode)]

You use a master URL with link:spark-submit.adoc[spark-submit as the value of `--master` command-line option] or <<creating-instance, when creating SparkContext using `setMaster` method>>.
