== [[DAGSchedulerEventProcessLoop]] DAGSchedulerEventProcessLoop -- dag-scheduler-event-loop DAGScheduler Event Bus

`DAGSchedulerEventProcessLoop` (*dag-scheduler-event-loop*) is a `EventLoop` single "business logic" thread for processing `DAGSchedulerEvent` events.

.``DAGSchedulerEvent``s and Event Handlers
[width="100%",cols="1,1,2",frame="topbot",options="header"]
|======================
| DAGSchedulerEvent | Event Handler | Reason
| <<JobSubmitted, JobSubmitted>> | <<handleJobSubmitted, handleJobSubmitted>> | A Spark job was submitted to DAGScheduler using link:spark-dagscheduler.adoc#submitJob[submitJob] or `runApproximateJob`.
| <<MapStageSubmitted, MapStageSubmitted>> | <<handleMapStageSubmitted, handleMapStageSubmitted>> | A link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] was submitted using `submitMapStage`.

| <<StageCancelled, StageCancelled>> | <<handleStageCancellation, handleStageCancellation>> | `DAGScheduler` was requested to link:spark-dagscheduler.adoc#cancelStage[cancel a stage].

| <<JobCancelled, JobCancelled>> | <<handleJobCancellation, handleJobCancellation>> | `DAGScheduler` was requested to link:spark-dagscheduler.adoc#cancelJob[cancel a job].

| <<JobGroupCancelled, JobGroupCancelled>> | <<handleJobGroupCancelled, handleJobGroupCancelled>> | `DAGScheduler` was requested to link:spark-dagscheduler.adoc#cancelJobGroup[cancel a job group].

| <<AllJobsCancelled, AllJobsCancelled>> | | `DAGScheduler` was requested to link:spark-dagscheduler.adoc#cancelAllJobs[cancel all running or waiting jobs].

| <<BeginEvent, BeginEvent>> | <<handleBeginEvent, handleBeginEvent>> | link:spark-tasksetmanager.adoc[TaskSetManager] informs `DAGScheduler` that a task is starting (through link:spark-dagscheduler.adoc#taskStarted[taskStarted]).

| <<GettingResultEvent, GettingResultEvent>> | |  link:spark-tasksetmanager.adoc[TaskSetManager] informs `DAGScheduler` (through link:spark-dagscheduler.adoc#taskGettingResult[taskGettingResult]) that a task has completed and results are being fetched remotely.

| <<CompletionEvent, CompletionEvent>> | <<handleTaskCompletion, handleTaskCompletion>> | link:spark-tasksetmanager.adoc[TaskSetManager] informs `DAGScheduler` (through link:spark-dagscheduler.adoc#taskEnded[taskEnded]) that a task has completed successfully or failed.

| <<ExecutorAdded, ExecutorAdded>> | <<handleExecutorAdded, handleExecutorAdded>> | `DAGScheduler` was informed (through link:spark-dagscheduler.adoc#executorAdded[executorAdded]) that an executor was spun up on a host.

| <<ExecutorLost, ExecutorLost>> | <<handleExecutorLost, handleExecutorLost>> | `DAGScheduler` was informed (through link:spark-dagscheduler.adoc#executorLost[executorLost]) that an executor was lost.

| <<TaskSetFailed, TaskSetFailed>> | <<handleTaskSetFailed, handleTaskSetFailed>> | `DAGScheduler` was requested to link:spark-dagscheduler.adoc#taskSetFailed[cancel a `TaskSet`]

| <<ResubmitFailedStages, ResubmitFailedStages>> | <<resubmitFailedStages, resubmitFailedStages>> | `DAGScheduler` was informed (through link:spark-dagscheduler.adoc#handleTaskCompletion[handleTaskCompletion]) that a task has finished with a `FetchFailed`.
|======================

When created, `DAGSchedulerEventProcessLoop` gets the reference to the owning link:spark-dagscheduler.adoc[DAGScheduler] that it uses to call event handler methods on.

NOTE: `DAGSchedulerEventProcessLoop` uses https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque] blocking deque that grows indefinitely, i.e. up to https://docs.oracle.com/javase/8/docs/api/java/lang/Integer.html#MAX_VALUE[Integer.MAX_VALUE] events.

=== [[AllJobsCancelled]] `AllJobsCancelled` Event and...

CAUTION: FIXME

=== [[GettingResultEvent]] `GettingResultEvent` Event and `handleGetTaskResult` Handler

[source, scala]
----
GettingResultEvent(taskInfo: TaskInfo) extends DAGSchedulerEvent
----

`GettingResultEvent` is a `DAGSchedulerEvent` that triggers <<handleGetTaskResult, handleGetTaskResult>> (on a separate thread).

NOTE: `GettingResultEvent` is posted to inform `DAGScheduler` (through link:spark-dagscheduler.adoc#taskGettingResult[taskGettingResult]) that a link:spark-tasksetmanager.adoc#handleTaskGettingResult[task fetches results].

==== [[handleGetTaskResult]] `handleGetTaskResult` Handler

[source, scala]
----
handleGetTaskResult(taskInfo: TaskInfo): Unit
----

`handleGetTaskResult` merely posts link:spark-SparkListener.adoc#SparkListenerTaskGettingResult[SparkListenerTaskGettingResult] (to link:spark-dagscheduler.adoc#listenerBus[`LiveListenerBus` Event Bus]).

=== [[BeginEvent]] `BeginEvent` Event and `handleBeginEvent` Handler

[source, scala]
----
BeginEvent(task: Task[_], taskInfo: TaskInfo) extends DAGSchedulerEvent
----

`BeginEvent` is a `DAGSchedulerEvent` that triggers <<handleBeginEvent, handleBeginEvent>> (on a separate thread).

NOTE: `BeginEvent` is posted to inform `DAGScheduler` (through link:spark-dagscheduler.adoc#taskStarted[taskStarted]) that a link:spark-tasksetmanager.adoc#resourceOffer[`TaskSetManager` starts a task].

==== [[handleBeginEvent]] `handleBeginEvent` Handler

[source, scala]
----
handleBeginEvent(task: Task[_], taskInfo: TaskInfo): Unit
----

`handleBeginEvent` looks the stage of `task` up in link:spark-dagscheduler.adoc#stageIdToStage[stageIdToStage] internal registry to compute the last attempt id (or `-1` if not available) and posts link:spark-SparkListener.adoc#SparkListenerTaskStart[SparkListenerTaskStart] (to link:spark-dagscheduler.adoc#listenerBus[listenerBus] event bus).

=== [[JobGroupCancelled]] `JobGroupCancelled` Event and `handleJobGroupCancelled` Handler

[source, scala]
----
JobGroupCancelled(groupId: String) extends DAGSchedulerEvent
----

`JobGroupCancelled` is a `DAGSchedulerEvent` that triggers <<handleJobGroupCancelled, handleJobGroupCancelled>> (on a separate thread).

NOTE: `JobGroupCancelled` is posted when `DAGScheduler` is informed (through link:spark-dagscheduler.adoc#cancelJobGroup[cancelJobGroup]) that link:spark-sparkcontext.adoc#cancelJobGroup[`SparkContext` was requested to cancel a job group].

==== [[handleJobGroupCancelled]] `handleJobGroupCancelled` Handler

[source, scala]
----
handleJobGroupCancelled(groupId: String): Unit
----

`handleJobGroupCancelled` finds active jobs in a group and cancels them.

Internally, `handleJobGroupCancelled` computes all the active jobs (registered in the internal link:spark-dagscheduler.adoc#activeJobs[collection of active jobs]) that have `spark.jobGroup.id` scheduling property set to `groupId`.

`handleJobGroupCancelled` then <<handleJobCancellation, cancels every active job>> in the group one by one and the cancellation reason: "part of cancelled job group [groupId]".

=== [[MapStageSubmitted]] `MapStageSubmitted` Event and `handleMapStageSubmitted` Handler

[source, scala]
----
MapStageSubmitted(
  jobId: Int,
  dependency: ShuffleDependency[_, _, _],
  callSite: CallSite,
  listener: JobListener,
  properties: Properties = null)
extends DAGSchedulerEvent
----

`MapStageSubmitted` is a `DAGSchedulerEvent` that triggers <<handleMapStageSubmitted, handleMapStageSubmitted>> (on a separate thread).

.`MapStageSubmitted` Event Handling
image::diagrams/scheduler-handlemapstagesubmitted.png[align="center"]

NOTE: `MapStageSubmitted` is posted when `DAGScheduler` is informed (through link:spark-dagscheduler.adoc#submitMapStage[submitMapStage]) that link:spark-sparkcontext.adoc#submitMapStage[SparkContext.submitMapStage].

==== [[handleMapStageSubmitted]] `handleMapStageSubmitted` Handler

[source, scala]
----
handleMapStageSubmitted(jobId: Int,
  dependency: ShuffleDependency[_, _, _],
  callSite: CallSite,
  listener: JobListener,
  properties: Properties): Unit
----

It is called with a job id (for a new job to be created), a link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency], and a link:spark-dagscheduler-JobListener.adoc[JobListener].

You should see the following INFOs in the logs:

```
Got map stage job %s (%s) with %d output partitions
Final stage: [finalStage] ([finalStage.name])
Parents of final stage: [finalStage.parents]
Missing parents: [list of stages]
```

link:spark-SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] event is posted to link:spark-LiveListenerBus.adoc[LiveListenerBus] (so other event listeners know about the event - not only DAGScheduler).

The execution procedure of MapStageSubmitted events is then exactly (FIXME ?) as for link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#JobSubmitted[JobSubmitted].

[TIP]
====
The difference between `handleMapStageSubmitted` and link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[handleJobSubmitted]:

* `handleMapStageSubmitted` has `ShuffleDependency` among the input parameters while `handleJobSubmitted` has `finalRDD`, `func`, and `partitions`.
* `handleMapStageSubmitted` initializes `finalStage` as `getShuffleMapStage(dependency, jobId)` while `handleJobSubmitted` as `finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)`
* `handleMapStageSubmitted` INFO logs `Got map stage job %s (%s) with %d output partitions` with `dependency.rdd.partitions.length` while `handleJobSubmitted` does `Got job %s (%s) with %d output partitions` with `partitions.length`.
* FIXME: Could the above be cut to `ActiveJob.numPartitions`?
* `handleMapStageSubmitted` adds a new job with `finalStage.addActiveJob(job)` while `handleJobSubmitted` sets with `finalStage.setActiveJob(job)`.
* `handleMapStageSubmitted` checks if the final stage has already finished, tells the listener and removes it using the code:
+
[source, scala]
----
if (finalStage.isAvailable) {
  markMapStageJobAsFinished(job, mapOutputTracker.getStatistics(dependency))
}
----
====

=== [[TaskSetFailed]] `TaskSetFailed` Event and `handleTaskSetFailed` Handler

[source, scala]
----
TaskSetFailed(
  taskSet: TaskSet,
  reason: String,
  exception: Option[Throwable])
extends DAGSchedulerEvent
----

`TaskSetFailed` is a `DAGSchedulerEvent` that triggers <<handleTaskSetFailed, handleTaskSetFailed>> method.

NOTE: `TaskSetFailed` is posted when link:spark-dagscheduler.adoc#taskSetFailed[`DAGScheduler` is requested to cancel a `TaskSet`].

==== [[handleTaskSetFailed]] `handleTaskSetFailed` Handler

[source, scala]
----
handleTaskSetFailed(
  taskSet: TaskSet,
  reason: String,
  exception: Option[Throwable]): Unit
----

`handleTaskSetFailed` looks the stage (of the input `taskSet`) up in the internal <<stageIdToStage, stageIdToStage>> registry and link:spark-dagscheduler.adoc#abortStage[aborts] it.

=== [[ResubmitFailedStages]] `ResubmitFailedStages` Event and `resubmitFailedStages` Handler

[source, scala]
----
ResubmitFailedStages extends DAGSchedulerEvent
----

`ResubmitFailedStages` is a `DAGSchedulerEvent` that triggers <<resubmitFailedStages, resubmitFailedStages>> method.

NOTE: `ResubmitFailedStages` is posted for <<handleTaskCompletion-FetchFailed, `FetchFailed` case in `handleTaskCompletion`>>.

==== [[resubmitFailedStages]] `resubmitFailedStages` Handler

[source, scala]
----
resubmitFailedStages(): Unit
----

`resubmitFailedStages` iterates over the internal link:spark-dagscheduler.adoc#failedStages[collection of failed stages] and link:spark-dagscheduler.adoc#submitStage[submits] them.

NOTE: `resubmitFailedStages` does nothing when there are no link:spark-dagscheduler.adoc#failedStages[failed stages reported].

You should see the following INFO message in the logs:

```
INFO Resubmitting failed stages
```

`resubmitFailedStages` link:spark-dagscheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations] first. It then makes a copy of the link:spark-dagscheduler.adoc#failedStages[collection of failed stages] so `DAGScheduler` can track failed stages afresh.

NOTE: At this point `DAGScheduler` has no failed stages reported.

The previously-reported failed stages are sorted by the corresponding job ids in incremental order and link:spark-dagscheduler.adoc#submitStage[resubmitted].

=== [[ExecutorLost]] `ExecutorLost` Event and `handleExecutorLost` Handler -- `fetchFailed` Disabled Case

[source, scala]
----
ExecutorLost(
  execId: String,
  reason: ExecutorLossReason)
extends DAGSchedulerEvent
----

`ExecutorLost` is a `DAGSchedulerEvent` that triggers <<handleExecutorLost, handleExecutorLost>> method with `fetchFailed` disabled, i.e. `false`.

[NOTE]
====
`handleExecutorLost` recognizes two cases (by means of `fetchFailed`):

* fetch failures (`fetchFailed` is `true`) from executors that are indirectly assumed lost. See <<handleTaskCompletion-FetchFailed, FetchFailed case in handleTaskCompletion>>.
* lost executors (`fetchFailed` is `false`) for executors that did not report being alive in a given timeframe
====

==== [[handleExecutorLost]] `handleExecutorLost` Handler

[source, scala]
----
handleExecutorLost(
  execId: String,
  filesLost: Boolean,
  maybeEpoch: Option[Long] = None): Unit
----

The current epoch could be provided (as the input `maybeEpoch`) or is requested from  link:spark-service-MapOutputTrackerMaster.adoc#getEpoch[MapOutputTrackerMaster].

CAUTION: FIXME When is `maybeEpoch` passed in?

.DAGScheduler.handleExecutorLost
image::images/dagscheduler-handleExecutorLost.png[align="center"]

Recurring `ExecutorLost` events lead to the following repeating DEBUG message in the logs:

```
DEBUG Additional executor lost message for [execId] (epoch [currentEpoch])
```

NOTE: `handleExecutorLost` handler uses ``DAGScheduler``'s `failedEpoch` and FIXME internal registries.

Otherwise, when the executor `execId` is not in the link:spark-dagscheduler.adoc#failedEpoch[list of executor lost] or the executor failure's epoch is smaller than the input `maybeEpoch`, the executor's lost event is recorded in link:spark-dagscheduler.adoc#failedEpoch[`failedEpoch` internal registry].

CAUTION: FIXME Describe the case above in simpler non-technical words. Perhaps change the order, too.

You should see the following INFO message in the logs:

```
INFO Executor lost: [execId] (epoch [epoch])
```

link:spark-BlockManagerMaster.adoc#removeExecutor[`BlockManagerMaster` is requested to remove the lost executor `execId`].

CAUTION: FIXME Review what's `filesLost`.

`handleExecutorLost` exits unless the `ExecutorLost` event was for a map output fetch operation (and the input `filesLost` is `true`) or link:spark-ExternalShuffleService.adoc[external shuffle service] is _not_ used.

In such a case, you should see the following INFO message in the logs:

```
INFO Shuffle files lost for executor: [execId] (epoch [epoch])
```

`handleExecutorLost` walks over all link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage]s in link:spark-dagscheduler.adoc#shuffleToMapStage[DAGScheduler's `shuffleToMapStage` internal registry] and do the following (in order):

1. `ShuffleMapStage.removeOutputsOnExecutor(execId)` is called
2. link:spark-service-MapOutputTrackerMaster.adoc#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs(shuffleId, stage.outputLocInMapOutputTrackerFormat(), changeEpoch = true)] is called.

In case link:spark-dagscheduler.adoc#shuffleToMapStage[DAGScheduler's `shuffleToMapStage` internal registry] has no shuffles registered,  link:spark-service-MapOutputTrackerMaster.adoc#incrementEpoch[`MapOutputTrackerMaster` is requested to increment epoch].

Ultimatelly, `DAGScheduler` link:spark-dagscheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations].

=== [[JobCancelled]] `JobCancelled` Event and `handleJobCancellation` Handler

[source, scala]
----
JobCancelled(jobId: Int) extends DAGSchedulerEvent
----

`JobCancelled` is a `DAGSchedulerEvent` that triggers <<handleJobCancellation, handleJobCancellation>> method (on a separate thread).

NOTE: `JobCancelled` is posted when link:spark-dagscheduler.adoc#cancelJob[`DAGScheduler` is requested to cancel a job].

==== [[handleJobCancellation]] `handleJobCancellation` Handler

[source, scala]
----
handleJobCancellation(jobId: Int, reason: String = "")
----

`handleJobCancellation` first makes sure that the input `jobId` has been registered earlier (using link:spark-dagscheduler.adoc#jobIdToStageIds[jobIdToStageIds] internal registry).

If the input `jobId` is not known to `DAGScheduler`, you should see the following DEBUG message in the logs:

```
DEBUG DAGScheduler: Trying to cancel unregistered job [jobId]
```

Otherwise, `handleJobCancellation` link:spark-dagscheduler.adoc#failJobAndIndependentStages[fails the active job and all independent stages] (by looking up the active job using link:spark-dagscheduler.adoc#jobIdToActiveJob[jobIdToActiveJob]) with failure reason:

```
Job [jobId] cancelled [reason]
```

=== [[CompletionEvent]][[handleTaskCompletion]] `CompletionEvent` Event and `handleTaskCompletion` Handler

`CompletionEvent` event informs `DAGScheduler` about task completions. It is handled by `handleTaskCompletion` method.

[source, scala]
----
handleTaskCompletion(event: CompletionEvent): Unit
----

.DAGScheduler and CompletionEvent
image::images/dagscheduler-tasksetmanager.png[align="center"]

NOTE: `CompletionEvent` holds contextual information about the completed task.

The task knows about the stage it belongs to (using `Task.stageId`), the partition it works on (using `Task.partitionId`), and the stage attempt (using `Task.stageAttemptId`).

`OutputCommitCoordinator.taskCompleted` is called.

If the reason for task completion is not `Success`, link:spark-SparkListener.adoc#SparkListenerTaskEnd[SparkListenerTaskEnd] is posted to link:spark-LiveListenerBus.adoc[LiveListenerBus]. The only difference with <<handleTaskCompletion-Success, TaskEndReason: Success>> is how the stage attempt id is calculated. Here, it is `Task.stageAttemptId` (not `Stage.latestInfo.attemptId`).

CAUTION: FIXME What is the difference between stage attempt ids?

If the stage the task belongs to has been cancelled, `stageIdToStage` should not contain it, and the method quits.

The main processing depends on the `TaskEndReason` - the reason for task completion (using `event.reason`). The method skips processing `TaskEndReasons`: `TaskCommitDenied`, `ExceptionFailure`, `TaskResultLost`, `ExecutorLostFailure`, `TaskKilled`, and `UnknownReason`, i.e. it does nothing.

==== [[handleTaskCompletion-Success]] TaskEndReason: Success

link:spark-SparkListener.adoc#SparkListenerTaskEnd[SparkListenerTaskEnd] is posted to link:spark-LiveListenerBus.adoc[LiveListenerBus].

The partition the task worked on is removed from `pendingPartitions` of the stage.

The processing splits per task type - ResultTask or ShuffleMapTask - and link:spark-dagscheduler.adoc#submitWaitingStages[DAGScheduler.submitWaitingStages] is called.

===== [[handleTaskCompletion-Success-ResultTask]] ResultTask

For `ResultTask`, the stage is link:spark-dagscheduler-ResultStage.adoc[ResultStage]. If there is no job active for the stage (using `resultStage.activeJob`), the following INFO message appears in the logs:

```
INFO Ignoring result from [task] because its job has finished
```

Otherwise, check whether the task is marked as running for the job (using `job.finished`) and proceed. The method skips execution when the task has already been marked as completed in the job.

CAUTION: FIXME When could a task that has just finished be ignored, i.e. the job has already marked `finished`? Could it be for stragglers?

link:spark-dagscheduler.adoc#updateAccumulators[DAGScheduler.updateAccumulators(event)] is called.

The partition is marked as `finished` (using `job.finished`) and the number of partitions calculated increased (using `job.numFinished`).

If the whole job has finished (when `job.numFinished == job.numPartitions`), then:

* `markStageAsFinished` is called
* `cleanupStateForJobAndIndependentStages(job)`
* link:spark-SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to link:spark-LiveListenerBus.adoc[LiveListenerBus] with `JobSucceeded`

The link:spark-dagscheduler-JobListener.adoc#taskSucceeded[`JobListener` of the job is notified about the task's successful completion]. In case the step fails, i.e. throws an exception, the link:spark-dagscheduler-JobListener.adoc#jobFailed[`JobListener` is notified about the failure].

CAUTION: FIXME When would `job.listener.taskSucceeded` throw an exception? How?

===== [[handleTaskCompletion-Success-ShuffleMapTask]] ShuffleMapTask

For ShuffleMapTask, the stage is link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage].

link:spark-dagscheduler.adoc#updateAccumulators[DAGScheduler.updateAccumulators(event)] is called.

`event.result` is `MapStatus` that knows the executor id where the task has finished (using `status.location.executorId`).

You should see the following DEBUG message in the logs:

```
DEBUG ShuffleMapTask finished on [execId]
```

If link:spark-dagscheduler.adoc#internal-registries[failedEpoch] contains the executor and the epoch of the ShuffleMapTask is not greater than that in `failedEpoch`, you should see the following INFO message in the logs:

```
INFO Ignoring possibly bogus [task] completion from executor [executorId]
```

Otherwise, `shuffleStage.addOutputLoc(smt.partitionId, status)` is called.

The method does more processing only if the internal `runningStages` contains the link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] with no more pending partitions to compute (using `shuffleStage.pendingPartitions`).

`markStageAsFinished(shuffleStage)` is called.

The following INFO logs appear in the logs:

```
INFO looking for newly runnable stages
INFO running: [runningStages]
INFO waiting: [waitingStages]
INFO failed: [failedStages]
```

link:spark-service-mapoutputtracker.adoc#registerMapOutputs[mapOutputTracker.registerMapOutputs] with `changeEpoch` is called.

The internal link:spark-dagscheduler.adoc#clearCacheLocs[cache of RDD partition locations is cleared].

If the map stage is ready, i.e. all partitions have shuffle outputs, map-stage jobs waiting on this stage (using `shuffleStage.mapStageJobs`) are marked as finished. link:spark-service-MapOutputTrackerMaster.adoc#getStatistics[`MapOutputTrackerMaster` is requested for statistics (for `shuffleStage.shuffleDep`)] and every map-stage job is `markMapStageJobAsFinished(job, stats)`.

Otherwise, if the map stage is _not_ ready, the following INFO message appears in the logs:

```
INFO Resubmitting [shuffleStage] ([shuffleStage.name]) because some of its tasks had failed: [missingPartitions]
```

`shuffleStage` is link:spark-dagscheduler.adoc#submitStage[submitted to `DAGScheduler` for execution].

==== [[TaskEndReason-Resubmitted]] TaskEndReason: Resubmitted

For `Resubmitted` case, you should see the following INFO message in the logs:

```
INFO Resubmitted [task], so marking it as still running
```

The task (by `task.partitionId`) is added to the collection of pending partitions of the stage (using `stage.pendingPartitions`).

TIP: A stage knows how many partitions are yet to be calculated. A task knows about the partition id for which it was launched.

==== [[handleTaskCompletion-FetchFailed]] TaskEndReason: FetchFailed

`FetchFailed(bmAddress, shuffleId, mapId, reduceId, failureMessage)` comes with `BlockManagerId` (as `bmAddress`) and the other self-explanatory values.

NOTE: A task knows about the id of the stage it belongs to.

When `FetchFailed` happens, `stageIdToStage` is used to access the failed stage (using `task.stageId` and the `task` is available in `event` in `handleTaskCompletion(event: CompletionEvent)`). `shuffleToMapStage` is used to access the map stage (using `shuffleId`).

If `failedStage.latestInfo.attemptId != task.stageAttemptId`, you should see the following INFO in the logs:

```
INFO Ignoring fetch failure from [task] as it's from [failedStage] attempt [task.stageAttemptId] and there is a more recent attempt for that stage (attempt ID [failedStage.latestInfo.attemptId]) running
```

CAUTION: FIXME What does `failedStage.latestInfo.attemptId != task.stageAttemptId` mean?

And the case finishes. Otherwise, the case continues.

If the failed stage is in `runningStages`, the following INFO message shows in the logs:

```
INFO Marking [failedStage] ([failedStage.name]) as failed due to a fetch failure from [mapStage] ([mapStage.name])
```

`markStageAsFinished(failedStage, Some(failureMessage))` is called.

CAUTION: FIXME What does `markStageAsFinished` do?

If the failed stage is not in `runningStages`, the following DEBUG message shows in the logs:

```
DEBUG Received fetch failure from [task], but its from [failedStage] which is no longer running
```

When `disallowStageRetryForTest` is set, `abortStage(failedStage, "Fetch failure will not retry stage due to testing config", None)` is called.

CAUTION: FIXME Describe `disallowStageRetryForTest` and `abortStage`.

If the link:spark-dagscheduler-stages.adoc#failedOnFetchAndShouldAbort[number of fetch failed attempts for the stage exceeds the allowed number], the link:spark-dagscheduler.adoc#abortStage[failed stage is aborted] with the reason:

```
[failedStage] ([name]) has failed the maximum allowable number of times: 4. Most recent failure reason: [failureMessage]
```

If there are no failed stages reported (link:spark-dagscheduler.adoc#internal-registries[DAGScheduler.failedStages] is empty), the following INFO shows in the logs:

```
INFO Resubmitting [mapStage] ([mapStage.name]) and [failedStage] ([failedStage.name]) due to fetch failure
```

And the following code is executed:

```
messageScheduler.schedule(
  new Runnable {
    override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages)
  }, DAGScheduler.RESUBMIT_TIMEOUT, TimeUnit.MILLISECONDS)
```

CAUTION: FIXME What does the above code do?

For all the cases, the failed stage and map stages are both added to the internal link:spark-dagscheduler.adoc#internal-registries[collection of failed stages].

If `mapId` (in the `FetchFailed` object for the case) is provided, the map stage output is cleaned up (as it is broken) using `mapStage.removeOutputLoc(mapId, bmAddress)` and link:spark-service-mapoutputtracker.adoc#unregisterMapOutput[MapOutputTrackerMaster.unregisterMapOutput(shuffleId, mapId, bmAddress)] methods.

CAUTION: FIXME What does `mapStage.removeOutputLoc` do?

If `bmAddress` (in the `FetchFailed` object for the case) is provided, <<handleExecutorLost, handleExecutorLost (with `fetchFailed` enabled)>> is called.

=== [[StageCancelled]] `StageCancelled` Event and `handleStageCancellation` Handler

[source, scala]
----
StageCancelled(stageId: Int) extends DAGSchedulerEvent
----

`StageCancelled` is a `DAGSchedulerEvent` that triggers <<handleStageCancellation, handleStageCancellation>> (on a separate thread).

==== [[handleStageCancellation]] `handleStageCancellation` Handler

[source, scala]
----
handleStageCancellation(stageId: Int): Unit
----

`handleStageCancellation` checks if the input `stageId` was registered earlier (in the internal link:spark-dagscheduler.adoc#stageIdToStage[stageIdToStage] registry) and if it was attempts to <<handleJobCancellation, cancel the associated jobs>> (with "because Stage [stageId] was cancelled" cancellation reason).

NOTE: A stage tracks the jobs it belongs to using `jobIds` property.

If the stage `stageId` was not registered earlier, you should see the following INFO message in the logs:

```
INFO No active jobs to kill for Stage [stageId]
```

NOTE: `handleStageCancellation` is the result of executing `SparkContext.cancelStage(stageId: Int)` that is called from the web UI (controlled by link:spark-webui.adoc#spark_ui_killEnabled[spark.ui.killEnabled]).

=== [[JobSubmitted]] `JobSubmitted` Event and `handleJobSubmitted` Handler

[source, scala]
----
JobSubmitted(
  jobId: Int,
  finalRDD: RDD[_],
  func: (TaskContext, Iterator[_]) => _,
  partitions: Array[Int],
  callSite: CallSite,
  listener: JobListener,
  properties: Properties = null)
extends DAGSchedulerEvent
----

`JobSubmitted` is a `DAGSchedulerEvent` that triggers <<handleJobSubmitted, handleJobSubmitted>> method (on a separate thread).

==== [[handleJobSubmitted]] `handleJobSubmitted` Handler

[source, scala]
----
handleJobSubmitted(
  jobId: Int,
  finalRDD: RDD[_],
  func: (TaskContext, Iterator[_]) => _,
  partitions: Array[Int],
  callSite: CallSite,
  listener: JobListener,
  properties: Properties)
----

`handleJobSubmitted` link:spark-dagscheduler.adoc#createResultStage[creates a new `ResultStage`] (as `finalStage` in the picture above) and a `ActiveJob`.

.DAGScheduler.handleJobSubmitted Method
image::images/dagscheduler-handleJobSubmitted.png[align="center"]

You should see the following INFO messages in the logs:

```
INFO DAGScheduler: Got job [jobId] ([callSite.shortForm]) with [partitions.length] output partitions
INFO DAGScheduler: Final stage: [finalStage] ([name])
INFO DAGScheduler: Parents of final stage: [parents]
INFO DAGScheduler: Missing parents: [getMissingParentStages(finalStage)]
```

`handleJobSubmitted` then registers the job in the internal registries, i.e. link:spark-dagscheduler.adoc#jobIdToActiveJob[jobIdToActiveJob] and link:spark-dagscheduler.adoc#activeJobs[activeJobs], and sets the job for the stage (using `setActiveJob`).

Ultimately, `handleJobSubmitted` posts  link:spark-SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] message to link:spark-LiveListenerBus.adoc[LiveListenerBus] and link:spark-dagscheduler.adoc#submitStage[submits the stage].

=== [[ExecutorAdded]] `ExecutorAdded` Event and `handleExecutorAdded` Handler

[source, scala]
----
ExecutorAdded(execId: String, host: String) extends DAGSchedulerEvent
----

`ExecutorAdded` is a `DAGSchedulerEvent` that triggers <<handleExecutorAdded, handleExecutorAdded>> method (on a separate thread).

==== [[handleExecutorAdded]] Removing Executor From `failedEpoch` Registry -- `handleExecutorAdded` Handler

[source, scala]
----
handleExecutorAdded(execId: String, host: String)
----

`handleExecutorAdded` checks if the input `execId` executor was registered in link:spark-dagscheduler.adoc#failedEpoch[failedEpoch] and, if it was, removes it from the `failedEpoch` registry.

You should see the following INFO message in the logs:

```
INFO Host added was in lost list earlier: [host]
```
