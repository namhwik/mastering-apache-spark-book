== SQLConf

`SQLConf` is a key-value configuration store for <<parameters, parameters and hints used in Spark SQL>>. It offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> their values.

You can access the current `SQLConf` using link:spark-sql-sparksession.adoc#conf[sparkSession.conf].

NOTE: `SQLConf` is a `private[sql]` serializable class in `org.apache.spark.sql.internal` package.

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.

=== [[parameters]] Parameters and Hints

CAUTION: FIXME

==== [[spark.sql.streaming.fileSink.log.deletion]] spark.sql.streaming.fileSink.log.deletion

`spark.sql.streaming.fileSink.log.deletion` (default: `true`) is an internal flag to control whether to delete the expired log files in link:spark-sql-streaming-sink.adoc#FileStreamSink[file stream sink].

==== [[spark.sql.streaming.fileSink.log.compactInterval]] spark.sql.streaming.fileSink.log.compactInterval

`spark.sql.streaming.fileSink.log.compactInterval`

==== [[spark.sql.streaming.fileSink.log.cleanupDelay]] spark.sql.streaming.fileSink.log.cleanupDelay

`spark.sql.streaming.fileSink.log.cleanupDelay`

==== [[spark.sql.streaming.schemaInference]] spark.sql.streaming.schemaInference

`spark.sql.streaming.schemaInference`
