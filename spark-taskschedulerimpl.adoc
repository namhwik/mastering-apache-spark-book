== [[TaskSchedulerImpl]] TaskSchedulerImpl - Default TaskScheduler

`TaskSchedulerImpl` is the default implementation of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract] and extends it <<getRackForHost, to track racks per host and port>>. It can schedule tasks for multiple types of cluster managers by means of link:spark-scheduler-backends.adoc[Scheduler Backends].

Using <<spark.scheduler.mode, spark.scheduler.mode>> setting you can select the link:spark-taskscheduler-schedulingmode.adoc[scheduling policy].

It <<submitTasks, submits tasks>> using link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilders].

When a Spark application starts (and an instance of link:spark-sparkcontext.adoc#creating-instance[SparkContext is created]) `TaskSchedulerImpl` with a link:spark-scheduler-backends.adoc[SchedulerBackend] and link:spark-dagscheduler.adoc[DAGScheduler] are created and soon started.

.TaskSchedulerImpl and Other Services
image::images/taskschedulerimpl-sparkcontext-schedulerbackend-dagscheduler.png[align="center"]

NOTE: `TaskSchedulerImpl` is a `private[spark]` class with the source code in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[org.apache.spark.scheduler.TaskSchedulerImpl].

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.scheduler.TaskSchedulerImpl` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=DEBUG
```
====

=== [[applicationAttemptId]] applicationAttemptId method

[source, scala]
----
applicationAttemptId(): Option[String]
----

CAUTION: FIXME

=== [[schedulableBuilder]] schedulableBuilder Attribute

`schedulableBuilder` is a link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilder] for the `TaskSchedulerImpl`.

It is set up when a <<initialize, `TaskSchedulerImpl` is initialized>> and can be one of two available builders:

* link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy).

* link:spark-taskscheduler-FairSchedulableBuilder.adoc[FairSchedulableBuilder] for FAIR scheduling policy.

NOTE: Use <<spark.scheduler.mode, spark.scheduler.mode>> setting to select the scheduling policy.

=== [[getRackForHost]] Tracking Racks per Hosts and Ports (getRackForHost method)

[source, scala]
----
getRackForHost(value: String): Option[String]
----

`getRackForHost` is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns `None`).

NOTE: It is overriden by the YARN-specific TaskScheduler link:yarn/spark-yarn-yarnscheduler.adoc[YarnScheduler].

`getRackForHost` is currently used in two places:

* <<resourceOffers, TaskSchedulerImpl.resourceOffers>> to track hosts per rack (using the <<internal-registries, internal `hostsByRack` registry>>) while processing resource offers.

* <<removeExecutor, TaskSchedulerImpl.removeExecutor>> to...FIXME

* link:spark-tasksetmanager.adoc#addPendingTask[TaskSetManager.addPendingTask], link:spark-tasksetmanager.adoc#[TaskSetManager.dequeueTask], and link:spark-tasksetmanager.adoc#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask]

=== [[creating-instance]] Creating TaskSchedulerImpl

Creating a `TaskSchedulerImpl` object requires a link:spark-sparkcontext.adoc[SparkContext] object, the <<maxTaskFailures, acceptable number of task failures>> (`maxTaskFailures`) and optional <<isLocal, isLocal>> flag (disabled by default, i.e. `false`).

NOTE: There is another `TaskSchedulerImpl` constructor that requires a link:spark-sparkcontext.adoc[SparkContext] object only and sets <<maxTaskFailures, maxTaskFailures>> to <<spark.task.maxFailures, spark.task.maxFailures>> or, if `spark.task.maxFailures` is not set, defaults to `4`.

While being created, it initializes <<internal-registries, internal registries>> to their default values.

It then sets link:spark-taskscheduler.adoc#contract[schedulingMode] to the value of <<spark.scheduler.mode, spark.scheduler.mode>> setting or `FIFO`.

NOTE: `schedulingMode` is part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

Failure to set `schedulingMode` results in a `SparkException`:

```
Unrecognized spark.scheduler.mode: [schedulingModeConf]
```

It sets `taskResultGetter` as a <<TaskResultGetter, TaskResultGetter>>.

CAUTION: FIXME Where is `taskResultGetter` used?

=== [[maxTaskFailures]] Acceptable Number of Task Failures (maxTaskFailures attribute)

The acceptable number of task failures (`maxTaskFailures`) can be explicitly defined when <<creating-instance, creating TaskSchedulerImpl instance>> or based on <<spark.task.maxFailures, spark.task.maxFailures>> setting that defaults to 4 failures.

NOTE: It is exclusively used when <<submitTasks, submitting tasks>> through link:spark-tasksetmanager.adoc[TaskSetManager].

=== [[removeExecutor]] Internal Cleanup After Removing Executor (removeExecutor method)

[source, scala]
----
removeExecutor(executorId: String, reason: ExecutorLossReason): Unit
----

`removeExecutor` removes the `executorId` executor from the <<internal-registries, internal registries>>: `executorIdToTaskCount`, `executorIdToHost`, `executorsByHost`, and `hostsByRack`. If the affected hosts and racks are the last entries in `executorsByHost` and `hostsByRack`, appropriately, they are removed from the registries.

Unless `reason` is `LossReasonPending`, the executor is removed from `executorIdToHost` registry and link:spark-taskscheduler-schedulable.adoc#executorLost[TaskSetManagers get notified].

NOTE: The internal `removeExecutor` is called as part of <<statusUpdate, statusUpdate>> and link:spark-taskscheduler.adoc#executorLost[executorLost].

=== [[isLocal]] Local vs Non-Local Mode (isLocal attribute)

CAUTION: FIXME

=== [[initialization]][[initialize]] Initializing TaskSchedulerImpl (initialize method)

[source, scala]
----
initialize(backend: SchedulerBackend): Unit
----

`initialize` initializes a `TaskSchedulerImpl` object.

.TaskSchedulerImpl initialization
image::images/TaskSchedulerImpl-initialize.png[align="center"]

NOTE: `initialize` is called while link:spark-sparkcontext-creating-instance-internals.adoc#createTaskScheduler[SparkContext is being created and creates `SchedulerBackend` and `TaskScheduler`].

`initialize` saves the reference to the current link:spark-scheduler-backends.adoc[SchedulerBackend] (as `backend`) and sets `rootPool` to be an empty-named link:spark-taskscheduler-pool.adoc[Pool] with already-initialized `schedulingMode` (while <<creating-instance, creating a TaskSchedulerImpl object>>), `initMinShare` and `initWeight` as `0`.

NOTE: `schedulingMode` and `rootPool` are a part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

It then creates the internal link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilder] object (as `schedulableBuilder`) based on `schedulingMode`:

* link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFOSchedulableBuilder] for `FIFO` scheduling mode
* link:spark-taskscheduler-FairSchedulableBuilder.adoc[FairSchedulableBuilder] for `FAIR` scheduling mode

With the `schedulableBuilder` object created, `initialize` requests it to link:spark-taskscheduler-schedulablebuilders.adoc#buildPools[build pools].

CAUTION: FIXME Why are `rootPool` and `schedulableBuilder` created only now? What do they need that it is not available when `TaskSchedulerImpl` is created?

=== [[start]] Starting TaskSchedulerImpl (start method)

As part of link:spark-sparkcontext-creating-instance-internals.adoc[initialization of a `SparkContext`], `TaskSchedulerImpl` is started (using `start` from the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]).

[source, scala]
----
start(): Unit
----

It starts the link:spark-scheduler-backends.adoc[scheduler backend] it manages.

Below is a figure of the method calls in Spark Standalone mode.

.Starting TaskSchedulerImpl in Spark Standalone mode
image::images/taskschedulerimpl-start-standalone.png[align="center"]

It also starts the *task-scheduler-speculation* executor pool. See <<speculative-execution, Speculative Execution of Tasks>>.

=== [[postStartHook]] Post-Start Initialization (using postStartHook)

`postStartHook` is a custom implementation of link:spark-taskscheduler.adoc#contract[postStartHook from the TaskScheduler Contract] that waits until a scheduler backend is ready (using the internal blocking <<waitBackendReady, waitBackendReady>>).

NOTE: `postStartHook` is used when link:spark-sparkcontext.adoc#creating-instance[SparkContext is created] (before it is fully created) and link:yarn/spark-yarn-yarnclusterscheduler.adoc#postStartHook[YarnClusterScheduler.postStartHook].

=== [[waitBackendReady]] Waiting Until SchedulerBackend is Ready (waitBackendReady method)

The private `waitBackendReady` method waits until a link:spark-scheduler-backends.adoc#contract[SchedulerBackend is ready].

It keeps on checking the status every 100 milliseconds until the SchedulerBackend is ready or the link:spark-sparkcontext.adoc#stop[SparkContext is stopped].

If the SparkContext happens to be stopped while doing the waiting, a `IllegalStateException` is thrown with the message:

```
Spark context stopped while waiting for backend
```

=== [[stop]][[stopping]] Stopping TaskSchedulerImpl (stop method)

When `TaskSchedulerImpl` is stopped (using `stop()` method), it does the following:

* Shuts down the internal `task-scheduler-speculation` thread pool executor (used for <<speculative-execution, Speculative execution of tasks>>).
* Stops link:spark-scheduler-backends.adoc[SchedulerBackend].
* Stops link:spark-taskscheduler.adoc#TaskResultGetter[TaskResultGetter].
* Cancels `starvationTimer` timer.

=== [[speculative-execution]] Speculative Execution of Tasks

*Speculative tasks* (also *speculatable tasks* or *task strugglers*) are tasks that run slower than most (FIXME the setting) of the all tasks in a job.

*Speculative execution of tasks* is a health-check procedure that checks for tasks to be *speculated*, i.e. running slower in a stage than the median of all successfully completed tasks in a taskset (FIXME the setting). Such slow tasks will be re-launched in another worker. It will not stop the slow tasks, but run a new copy in parallel.

The thread starts as `TaskSchedulerImpl` starts in link:spark-cluster.adoc[clustered deployment modes] with link:spark-tasksetmanager.adoc#settings[spark.speculation] enabled. It executes periodically every <<settings, spark.speculation.interval>> after <<settings, spark.speculation.interval>> passes.

When enabled, you should see the following INFO message in the logs:

```
INFO Starting speculative execution thread
```

It works as *task-scheduler-speculation* daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`.

The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet.

It uses `checkSpeculatableTasks` method that asks `rootPool` to check for speculatable tasks. If there are any, SchedulerBackend is called for link:spark-scheduler-backends.adoc#reviveOffers[reviveOffers].

CAUTION: FIXME How does Spark handle repeated results of speculative tasks since there are copies launched?

=== [[defaultParallelism]] Calculating Default Level of Parallelism (defaultParallelism method)

*Default level of parallelism* is a hint for sizing jobs. It is a part of the link:spark-taskscheduler.adoc#contract[TaskScheduler contract] and link:spark-sparkcontext.adoc#defaultParallelism[used by SparkContext] to create RDDs with the right number of partitions when not specified explicitly.

`TaskSchedulerImpl` uses link:spark-scheduler-backends.adoc#defaultParallelism[SchedulerBackend.defaultParallelism()] to calculate the value, i.e. it just passes it along to a scheduler backend.

=== [[submitTasks]] Submitting Tasks (using submitTasks)

NOTE: `submitTasks` is a part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

[source, scala]
----
submitTasks(taskSet: TaskSet): Unit
----

`submitTasks` creates a link:spark-tasksetmanager.adoc[TaskSetManager] for the input link:spark-taskscheduler-tasksets.adoc[TaskSet] and link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[adds it to the `Schedulable` root pool].

NOTE: The link:spark-taskscheduler.adoc#rootPool[root pool] can be a single flat linked queue (in link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFO scheduling mode]) or a hierarchy of pools of `Schedulables` (in link:spark-taskscheduler-FairSchedulableBuilder.adoc[FAIR scheduling mode]).

It makes sure that the requested resources, i.e. CPU and memory, are assigned to the Spark application for a non-local environment before requesting the current link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` to revive offers].

.TaskSchedulerImpl.submitTasks
image::images/taskschedulerImpl-submitTasks.png[align="center"]

NOTE: If there are tasks to launch for missing partitions in a stage, DAGScheduler executes `submitTasks` (see link:spark-dagscheduler.adoc#submitMissingTasks[submitMissingTasks for Stage and Job]).

When `submitTasks` is called, you should see the following INFO message in the logs:

```
INFO TaskSchedulerImpl: Adding task set [taskSet.id] with [tasks.length] tasks
```

It creates a new link:spark-tasksetmanager.adoc[TaskSetManager] for the input `taskSet` and the <<maxTaskFailures, acceptable number of task failures>>.

NOTE: The acceptable number of task failures is specified when a <<creating-instance, TaskSchedulerImpl is created>>.

NOTE: A `TaskSet` knows the tasks to execute (as `tasks`) and stage id (as `stageId`) the tasks belong to. Read link:spark-taskscheduler-tasksets.adoc[TaskSets].

The `TaskSet` is registered in the internal <<taskSetsByStageIdAndAttempt, taskSetsByStageIdAndAttempt>> registry with the `TaskSetManager`.

If there is more than one active link:spark-tasksetmanager.adoc[TaskSetManager] for the stage, a `IllegalStateException` is thrown with the message:

```
more than one active taskSet for stage [stage]: [TaskSet ids]
```

NOTE: `TaskSetManager` is considered *active* when it is not a *zombie*.

The `TaskSetManager` is link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[added to the `Schedulable` pool (via `SchedulableBuilder`)].

When the method is called the very first time (`hasReceivedTask` is `false`) in cluster mode only (i.e. `isLocal` of the `TaskSchedulerImpl` is `false`), `starvationTimer` is scheduled to execute after <<settings, spark.starvation.timeout>>  to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager.

NOTE: After the first <<settings, spark.starvation.timeout>> passes, the internal `hasReceivedTask` flag becomes `true`.

Every time the starvation timer thread is executed and `hasLaunchedTask` flag is `false`, the following WARN message is printed out to the logs:

```
WARN Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
```

Otherwise, when the `hasLaunchedTask` flag is `true` the timer thread cancels itself.

Ultimately, `submitTasks` requests the link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` to revive offers].

TIP: Use `dag-scheduler-event-loop` thread to step through the code in a debugger.

=== [[taskSetsByStageIdAndAttempt]] taskSetsByStageIdAndAttempt Registry

CAUTION: FIXME

A mapping between stages and a collection of attempt ids and TaskSetManagers.

=== [[resourceOffers]] Processing Executor Resource Offers (using resourceOffers)

[source, scala]
----
resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]]
----

`resourceOffers` method is called by link:spark-scheduler-backends.adoc[SchedulerBackend] (for clustered environments) or link:spark-local.adoc#LocalBackend[LocalBackend] (for local mode) with `WorkerOffer` resource offers that represent cores (CPUs) available on all the active executors with one `WorkerOffer` per active executor.

.Processing Executor Resource Offers
image::images/taskscheduler-resourceOffers.png[align="center"]

NOTE: `resourceOffers` is a mechanism to propagate information about active executors to `TaskSchedulerImpl` with the hosts and racks (if supported by the cluster manager).

A `WorkerOffer` is a 3-tuple with executor id, host, and the number of free cores available.

[source, scala]
----
WorkerOffer(executorId: String, host: String, cores: Int)
----

For each `WorkerOffer` (that represents free cores on an executor) `resourceOffers` method records the host per executor id (using the internal `executorIdToHost`) and sets `0` as the number of tasks running on the executor if there are no tasks on the executor (using `executorIdToTaskCount`). It also records hosts (with executors in the internal `executorsByHost` registry).

WARNING: FIXME BUG? Why is the executor id *not* added to `executorsByHost`?

For the offers with a host that has not been recorded yet (in the internal `executorsByHost` registry) the following occurs:

1. The host is recorded in the internal `executorsByHost` registry.
2. <<executorAdded, executorAdded>> callback is called (with the executor id and the host from the offer).
3. `newExecAvail` flag is enabled (it is later used to inform `TaskSetManagers` about the new executor).

CAUTION: FIXME a picture with `executorAdded` call from TaskSchedulerImpl to DAGScheduler.

It shuffles the input `offers` that is supposed to help evenly distributing tasks across executors (that the input `offers` represent) and builds internal structures like `tasks` and `availableCpus`.

.Internal Structures of resourceOffers with 5 WorkerOffers
image::images/TaskSchedulerImpl-resourceOffers-internal-structures.png[align="center"]

The root pool is requested for link:spark-taskscheduler-pool.adoc#getSortedTaskSetQueue[TaskSetManagers sorted appropriately] (according to the link:spark-taskscheduler-schedulingmode.adoc[scheduling order]).

NOTE: `rootPool` is a part of the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract] and is exclusively managed by link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilders] (that  link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[add `TaskSetManagers` to the root pool].

For every `TaskSetManager` in the `TaskSetManager` sorted queue, the following DEBUG message is printed out to the logs:

```
DEBUG TaskSchedulerImpl: parentName: [taskSet.parent.name], name: [taskSet.name], runningTasks: [taskSet.runningTasks]
```

NOTE: The internal `rootPool` is configured while <<initialize, TaskSchedulerImpl is being initialized>>.

While traversing over the sorted collection of `TaskSetManagers`, if a new host (with an executor) was registered, i.e. the `newExecAvail` flag is enabled, `TaskSetManagers` are link:spark-tasksetmanager.adoc#executorAdded[informed about the new executor added].

NOTE: A `TaskSetManager` will be informed about one or more new executors once per host regardless of the number of executors registered on the host.

For each `TaskSetManager` (in `sortedTaskSets`) and for each preferred locality level (ascending), <<resourceOfferSingleTaskSet, resourceOfferSingleTaskSet>> is called until `launchedTask` flag is `false`.

CAUTION: FIXME `resourceOfferSingleTaskSet` + the sentence above less code-centric.

Check whether the number of cores in an offer is greater than the <<spark.task.cpus, number of cores needed for a task>>.

When `resourceOffers` managed to launch a task (i.e. `tasks` collection is not empty), the internal `hasLaunchedTask` flag becomes `true` (that effectively means what the name says _"There were executors and I managed to launch a task"_).

`resourceOffers` returns the `tasks` collection.

NOTE: `resourceOffers` is called when link:spark-scheduler-backends-coarse-grained.adoc#makeOffers[`CoarseGrainedSchedulerBackend` makes resource offers].

==== [[resourceOfferSingleTaskSet]] resourceOfferSingleTaskSet method

[source, scala]
----
resourceOfferSingleTaskSet(
  taskSet: TaskSetManager,
  maxLocality: TaskLocality,
  shuffledOffers: Seq[WorkerOffer],
  availableCpus: Array[Int],
  tasks: Seq[ArrayBuffer[TaskDescription]]): Boolean
----

`resourceOfferSingleTaskSet` is a private helper method that is executed when...

=== [[TaskResultGetter]] TaskResultGetter

`TaskResultGetter` is a helper class for <<statusUpdate, TaskSchedulerImpl.statusUpdate>>. It _asynchronously_ fetches the task results of tasks that have finished successfully (using <<enqueueSuccessfulTask, enqueueSuccessfulTask>>) or fetches the reasons of failures for failed tasks (using <<enqueueFailedTask, enqueueFailedTask>>). It then sends the "results" back to `TaskSchedulerImpl`.

CAUTION: FIXME Image with the dependencies

TIP: Consult link:spark-taskscheduler-tasks.adoc#states[Task States] in Tasks to learn about the different task states.

NOTE: The only instance of `TaskResultGetter` is created while <<creating-instance, TaskSchedulerImpl is being created>> (as `taskResultGetter`). It requires a `SparkEnv` and `TaskSchedulerImpl`. It is stopped when `TaskSchedulerImpl` stops.

`TaskResultGetter` offers the following methods:

* <<enqueueSuccessfulTask, enqueueSuccessfulTask>>
* <<enqueueFailedTask, enqueueFailedTask>>

The methods use the internal (daemon thread) thread pool *task-result-getter* (as `getTaskResultExecutor`) with <<settings, spark.resultGetter.threads>> so they can be executed asynchronously.

==== [[enqueueSuccessfulTask]] TaskResultGetter.enqueueSuccessfulTask

`enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer)` starts by deserializing `TaskResult` (from `serializedData` using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

If the result is `DirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(serializedData.limit())` and possibly quits. If not, it deserializes the result (using `SparkEnv.serializer`).

CAUTION: FIXME Review `taskSetManager.canFetchMoreResults(serializedData.limit())`.

If the result is `IndirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(size)` and possibly removes the block id (using `SparkEnv.blockManager.master.removeBlock(blockId)`) and quits. If not, you should see the following DEBUG message in the logs:

```
DEBUG Fetching indirect task result for TID [tid]
```

`scheduler.handleTaskGettingResult(taskSetManager, tid)` gets called. And `sparkEnv.blockManager.getRemoteBytes(blockId)`.

Failure in getting task result from BlockManager results in calling <<handleFailedTask, TaskSchedulerImpl.handleFailedTask(taskSetManager, tid, TaskState.FINISHED, TaskResultLost)>> and quit.

The task result is deserialized to `DirectTaskResult` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]) and `sparkEnv.blockManager.master.removeBlock(blockId)` is called afterwards.

`TaskSchedulerImpl.handleSuccessfulTask(taskSetManager, tid, result)` is called.

CAUTION: FIXME What is `TaskSchedulerImpl.handleSuccessfulTask` doing?

Any `ClassNotFoundException` or non fatal exceptions lead to link:spark-tasksetmanager.adoc#aborting-taskset[TaskSetManager.abort].

==== [[enqueueFailedTask]] TaskResultGetter.enqueueFailedTask

`enqueueFailedTask(taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, serializedData: ByteBuffer)` checks whether `serializedData` contains any data and if it does it deserializes it to a `TaskEndReason` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

Either `UnknownReason` or the deserialized instance is passed on to <<handleFailedTask, TaskSchedulerImpl.handleFailedTask>> as the reason of the failure.

Any `ClassNotFoundException` leads to printing out the ERROR message to the logs:

```
ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader]
```

=== [[statusUpdate]] TaskSchedulerImpl.statusUpdate

`statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer)` is called by link:spark-scheduler-backends.adoc[scheduler backends] to inform about task state changes (see link:spark-taskscheduler-tasks.adoc#states[Task States] in Tasks).

CAUTION: FIXME image with scheduler backends calling `TaskSchedulerImpl.statusUpdate`.

It is called by:

* link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend] when `StatusUpdate(executorId, taskId, state, data)` comes.
* link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend] when `org.apache.mesos.Scheduler.statusUpdate` is called.
* link:spark-local.adoc#LocalEndpoint[LocalEndpoint] when `StatusUpdate(taskId, state, serializedData)` comes.

When `statusUpdate` starts, it checks the current state of the task and act accordingly.

If a task became `TaskState.LOST` and there is still an executor assigned for the task (it seems it may not given the check), the executor is marked as lost (or sometimes called failed). The executor is later announced as such using `DAGScheduler.executorLost` with link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers()] being called afterwards.

CAUTION: FIXME Why is link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers()] called only for lost executors?

The method looks up the link:spark-tasksetmanager.adoc[TaskSetManager] for the task (using `taskIdToTaskSetManager`).

When the TaskSetManager is found and the task is in finished state, the task is removed from the internal data structures, i.e. `taskIdToTaskSetManager` and `taskIdToExecutorId`, and the number of currently running tasks for the executor(s) is decremented (using `executorIdToTaskCount`).

For a `FINISHED` task, link:spark-taskscheduler-tasksets.adoc[TaskSet.removeRunningTask] is called and then <<TaskResultGetter, TaskResultGetter.enqueueSuccessfulTask>>.

For a task in `FAILED`, `KILLED`, or `LOST` state, link:spark-taskscheduler-tasksets.adoc[TaskSet.removeRunningTask] is called (as for the `FINISHED` state) and then <<TaskResultGetter,TaskResultGetter.enqueueFailedTask>>.

If the TaskSetManager could not be found, the following ERROR shows in the logs:

```
ERROR Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates)
```

=== [[handleFailedTask]] TaskSchedulerImpl.handleFailedTask

`TaskSchedulerImpl.handleFailedTask(taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, reason: TaskEndReason)` is called when <<enqueueSuccessfulTask, TaskResultGetter.enqueueSuccessfulTask>> failed to fetch bytes from BlockManager or as part of <<enqueueFailedTask, TaskResultGetter.enqueueFailedTask>>.

Either way there is an error related to task execution.

It calls link:spark-tasksetmanager.adoc#handleFailedTask[TaskSetManager.handleFailedTask].

If link:spark-tasksetmanager.adoc#zombie-state[the TaskSetManager is not a zombie] and the task's state is not `KILLED`, link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers] is called.

=== [[taskSetFinished]] TaskSchedulerImpl.taskSetFinished

`taskSetFinished(manager: TaskSetManager)` method is called to inform TaskSchedulerImpl that all tasks in a TaskSetManager have finished execution.

.TaskSchedulerImpl.taskSetFinished is called when all tasks are finished
image::images/taskschedulerimpl-tasksetmanager-tasksetfinished.png[align="center"]

NOTE: `taskSetFinished` is called by TaskSetManager at the very end of link:spark-tasksetmanager.adoc#handleFailedTask[TaskSetManager.handleSuccessfulTask].

`taskSetsByStageIdAndAttempt` internal mapping is queried by stage id (using `manager.taskSet.stageId`) for the corresponding TaskSets (TaskSetManagers in fact) to remove the currently-finished stage attempt (using `manager.taskSet.stageAttemptId`) and if it was the only attempt, the stage id is completely removed from `taskSetsByStageIdAndAttempt`.

NOTE: A TaskSetManager owns a TaskSet that corresponds to a stage.

`Pool.removeSchedulable(manager)` is called for the `parent` of the TaskSetManager.

You should see the following INFO message in the logs:

```
INFO Removed TaskSet [manager.taskSet.id], whose tasks have all completed, from pool [manager.parent.name]
```

=== [[executorAdded]] TaskSchedulerImpl.executorAdded

[source, scala]
----
executorAdded(execId: String, host: String)
----

`executorAdded` method simply passes the notification on to the `DAGScheduler` (using link:spark-dagscheduler.adoc#executorAdded[DAGScheduler.executorAdded])

CAUTION: FIXME Image with a call from TaskSchedulerImpl to DAGScheduler, please.

=== [[internal-registries]] Internal Registries

CAUTION: FIXME How/where are these mappings used?

`TaskSchedulerImpl` tracks the following information in its internal data structures:

* the number of link:spark-taskscheduler-tasks.adoc[tasks] already scheduled for execution (`nextTaskId`).
* link:spark-taskscheduler-tasksets.adoc[TaskSets] by stage and attempt ids (`taskSetsByStageIdAndAttempt`)
* link:spark-taskscheduler-tasks.adoc[tasks] to their link:spark-tasksetmanager.adoc[TaskSetManagers] (`taskIdToTaskSetManager`)
* link:spark-taskscheduler-tasks.adoc[tasks] to link:spark-executor.adoc[executors] (`taskIdToExecutorId`)
* the number of link:spark-taskscheduler-tasks.adoc[tasks] running per link:spark-executor.adoc[executor] (`executorIdToTaskCount`)
* the set of link:spark-executor.adoc[executors] on each host (`executorsByHost`)
* the set of hosts per rack (`hostsByRack`)
* executor ids to corresponding host (`executorIdToHost`).

=== [[settings]] Settings

==== [[spark.task.maxFailures]] spark.task.maxFailures

`spark.task.maxFailures` (default: `4` for link:spark-cluster.adoc[cluster mode] and `1` for link:spark-local.adoc[local] except link:spark-local.adoc[local-with-retries]) - The number of individual task failures before giving up on the entire link:spark-taskscheduler-tasksets.adoc[TaskSet] and the job afterwards.

It is used in `TaskSchedulerImpl` to initialize a link:spark-tasksetmanager.adoc[TaskSetManager].

==== [[spark.task.cpus]] spark.task.cpus

`spark.task.cpus` (default: `1`) sets how many CPUs to request per task.

==== [[spark.scheduler.mode]] spark.scheduler.mode

`spark.scheduler.mode` (default: `FIFO`) is a case-insensitive name of the link:spark-taskscheduler-schedulingmode.adoc[scheduling mode] and can be one of `FAIR`, `FIFO`, or `NONE`.

NOTE: Only `FAIR` and `FIFO` are supported by `TaskSchedulerImpl`. See <<schedulableBuilder, schedulableBuilder>>.

==== [[spark.speculation.interval]] spark.speculation.interval

`spark.speculation.interval` (default: `100ms`) - how often to check for speculative tasks.

==== [[spark.starvation.timeout]] spark.starvation.timeout

`spark.starvation.timeout` (default: `15s`) - Threshold above which Spark warns a user that an initial TaskSet may be starved.

==== [[spark.resultGetter.threads]] spark.resultGetter.threads

`spark.resultGetter.threads` (default: `4`) - the number of threads for <<TaskResultGetter, TaskResultGetter>>.
