== [[TaskSchedulerImpl]] TaskSchedulerImpl -- Default TaskScheduler

`TaskSchedulerImpl` is the default implementation of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract] and extends it <<getRackForHost, to track racks per host and port>>. It can schedule tasks for multiple types of cluster managers by means of link:spark-scheduler-backends.adoc[Scheduler Backends].

Using <<spark_scheduler_mode, spark.scheduler.mode>> setting you can select the link:spark-taskscheduler-schedulingmode.adoc[scheduling policy].

It <<submitTasks, submits tasks>> using link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilders].

When a Spark application starts (and an instance of link:spark-sparkcontext.adoc#creating-instance[SparkContext is created]) `TaskSchedulerImpl` with a link:spark-scheduler-backends.adoc[SchedulerBackend] and link:spark-dagscheduler.adoc[DAGScheduler] are created and soon started.

.TaskSchedulerImpl and Other Services
image::images/taskschedulerimpl-sparkcontext-schedulerbackend-dagscheduler.png[align="center"]

NOTE: `TaskSchedulerImpl` is a `private[spark]` class with the source code in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[org.apache.spark.scheduler.TaskSchedulerImpl].

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.scheduler.TaskSchedulerImpl` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[starvationTimer]] `starvationTimer`

CAUTION: FIXME

=== [[executorHeartbeatReceived]] `executorHeartbeatReceived` Method

[source, scala]
----
executorHeartbeatReceived(
  execId: String,
  accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
  blockManagerId: BlockManagerId): Boolean
----

`executorHeartbeatReceived` is...

CAUTION: FIXME

NOTE: `executorHeartbeatReceived` is a part of the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

=== [[cancelTasks]] Cancelling Tasks for Stage -- `cancelTasks` Method

[source, scala]
----
cancelTasks(stageId: Int, interruptThread: Boolean): Unit
----

`cancelTasks` cancels all tasks submitted for execution in a stage `stageId`.

NOTE: It is currently called by link:spark-dagscheduler.adoc#failJobAndIndependentStages[`DAGScheduler` when it cancels a stage].

=== [[handleSuccessfulTask]] `handleSuccessfulTask` Method

[source, scala]
----
handleSuccessfulTask(
  taskSetManager: TaskSetManager,
  tid: Long,
  taskResult: DirectTaskResult[_]): Unit
----

`handleSuccessfulTask` simply link:spark-tasksetmanager.adoc#handleSuccessfulTask[forwards the call to the input `taskSetManager`] (passing `tid` and `taskResult`).

NOTE: `handleSuccessfulTask` is called when link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueSuccessfulTask[`TaskSchedulerGetter` has managed to deserialize the task result of a task that finished successfully].

=== [[handleTaskGettingResult]] `handleTaskGettingResult` Method

[source, scala]
----
handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long): Unit
----

`handleTaskGettingResult` simply link:spark-tasksetmanager.adoc#handleTaskGettingResult[forwards the call to the `taskSetManager`].

NOTE: `handleTaskGettingResult` is used to inform that link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueSuccessfulTask[`TaskResultGetter` enqueues a successful task with `IndirectTaskResult` task result (and so is about to fetch a remote block from a `BlockManager`)].

=== [[applicationAttemptId]] `applicationAttemptId` Method

[source, scala]
----
applicationAttemptId(): Option[String]
----

CAUTION: FIXME

=== [[schedulableBuilder]] `schedulableBuilder` Attribute

`schedulableBuilder` is a link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilder] for the `TaskSchedulerImpl`.

It is set up when a <<initialize, `TaskSchedulerImpl` is initialized>> and can be one of two available builders:

* link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy).

* link:spark-taskscheduler-FairSchedulableBuilder.adoc[FairSchedulableBuilder] for FAIR scheduling policy.

NOTE: Use <<spark_scheduler_mode, spark.scheduler.mode>> setting to select the scheduling policy.

=== [[getRackForHost]] Tracking Racks per Hosts and Ports -- `getRackForHost` Method

[source, scala]
----
getRackForHost(value: String): Option[String]
----

`getRackForHost` is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns `None`).

NOTE: It is overriden by the YARN-specific TaskScheduler link:yarn/spark-yarn-yarnscheduler.adoc[YarnScheduler].

`getRackForHost` is currently used in two places:

* <<resourceOffers, TaskSchedulerImpl.resourceOffers>> to track hosts per rack (using the <<internal-registries, internal `hostsByRack` registry>>) while processing resource offers.

* <<removeExecutor, TaskSchedulerImpl.removeExecutor>> to...FIXME

* link:spark-tasksetmanager.adoc#addPendingTask[TaskSetManager.addPendingTask], link:spark-tasksetmanager.adoc#[TaskSetManager.dequeueTask], and link:spark-tasksetmanager.adoc#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask]

=== [[internal-registries]] Internal Registries and Counters

.Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Name | Description
| `nextTaskId` | The next link:spark-taskscheduler-tasks.adoc[task] id counting from `0`.

Used when `TaskSchedulerImpl`...

| [[taskSetsByStageIdAndAttempt]] `taskSetsByStageIdAndAttempt` | Lookup table of link:spark-taskscheduler-tasksets.adoc[TaskSet] by stage and attempt ids.

| [[taskIdToTaskSetManager]] `taskIdToTaskSetManager` | Lookup table of link:spark-tasksetmanager.adoc[TaskSetManager] by task id.

| [[taskIdToExecutorId]] `taskIdToExecutorId` | Lookup table of link:spark-executor.adoc[executor] by task id.

| [[executorIdToTaskCount]] `executorIdToTaskCount` | Lookup table of the number of running tasks by link:spark-executor.adoc[executor].

| `executorsByHost` | Collection of link:spark-executor.adoc[executors] per host

| `hostsByRack` | Collection of hosts per rack

| `executorIdToHost` | Lookup table of hosts per executor
|======================

=== [[creating-instance]] Creating `TaskSchedulerImpl` Instance

[source, scala]
----
class TaskSchedulerImpl(
  val sc: SparkContext,
  val maxTaskFailures: Int,
  isLocal: Boolean = false)
extends TaskScheduler
----

Creating a `TaskSchedulerImpl` object requires a link:spark-sparkcontext.adoc[SparkContext] object with <<maxTaskFailures, acceptable number of task failures>> and optional <<isLocal, isLocal>> flag (disabled by default, i.e. `false`).

NOTE: There is another `TaskSchedulerImpl` constructor that requires a link:spark-sparkcontext.adoc[SparkContext] object only and sets <<maxTaskFailures, maxTaskFailures>> to <<spark_task_maxFailures, spark.task.maxFailures>> or, if `spark.task.maxFailures` is not set, defaults to `4`.

While being created, `TaskSchedulerImpl` initializes <<internal-registries, internal registries and counters>> to their default values.

`TaskSchedulerImpl` then sets link:spark-taskscheduler.adoc#contract[schedulingMode] to the value of <<spark_scheduler_mode, spark.scheduler.mode>> setting (defaults to `FIFO`).

NOTE: `schedulingMode` is part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

Failure to set `schedulingMode` results in a `SparkException`:

```
Unrecognized spark.scheduler.mode: [schedulingModeConf]
```

Ultimately, `TaskSchedulerImpl` creates a link:spark-taskschedulerimpl-TaskResultGetter.adoc[TaskResultGetter].

=== [[initialization]][[initialize]] Initializing `TaskSchedulerImpl` -- `initialize` Method

[source, scala]
----
initialize(backend: SchedulerBackend): Unit
----

`initialize` initializes a `TaskSchedulerImpl` object.

.TaskSchedulerImpl initialization
image::images/TaskSchedulerImpl-initialize.png[align="center"]

NOTE: `initialize` is called while link:spark-sparkcontext-creating-instance-internals.adoc#createTaskScheduler[SparkContext is being created and creates `SchedulerBackend` and `TaskScheduler`].

`initialize` saves the reference to the current link:spark-scheduler-backends.adoc[SchedulerBackend] (as `backend`) and sets `rootPool` to be an empty-named link:spark-taskscheduler-pool.adoc[Pool] with already-initialized `schedulingMode` (while <<creating-instance, creating a TaskSchedulerImpl object>>), `initMinShare` and `initWeight` as `0`.

NOTE: `schedulingMode` and `rootPool` are a part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

It then creates the internal link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilder] object (as `schedulableBuilder`) based on `schedulingMode`:

* link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFOSchedulableBuilder] for `FIFO` scheduling mode
* link:spark-taskscheduler-FairSchedulableBuilder.adoc[FairSchedulableBuilder] for `FAIR` scheduling mode

With the `schedulableBuilder` object created, `initialize` requests it to link:spark-taskscheduler-schedulablebuilders.adoc#buildPools[build pools].

CAUTION: FIXME Why are `rootPool` and `schedulableBuilder` created only now? What do they need that it is not available when `TaskSchedulerImpl` is created?

=== [[start]] Starting `TaskSchedulerImpl` -- `start` Method

As part of link:spark-sparkcontext-creating-instance-internals.adoc[initialization of a `SparkContext`], `TaskSchedulerImpl` is started (using `start` from the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]).

[source, scala]
----
start(): Unit
----

`start` starts the link:spark-scheduler-backends.adoc[scheduler backend].

.Starting `TaskSchedulerImpl` in Spark Standalone
image::images/taskschedulerimpl-start-standalone.png[align="center"]

`start` also starts <<task-scheduler-speculation, `task-scheduler-speculation` executor service>>.

=== [[speculationScheduler]][[task-scheduler-speculation]] `task-scheduler-speculation` Scheduled Executor Service -- `speculationScheduler` Internal Attribute

`speculationScheduler` is a http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService] with the name  *task-scheduler-speculation* for link:spark-taskschedulerimpl-speculative-execution.adoc[speculative execution of tasks].

When <<start, `TaskSchedulerImpl` starts>> (in non-local run mode) with link:spark-taskschedulerimpl-speculative-execution.adoc#spark_speculation[spark.speculation] enabled, `speculationScheduler` is used to schedule <<checkSpeculatableTasks, checkSpeculatableTasks>> to execute periodically every link:spark-taskschedulerimpl-speculative-execution.adoc#spark_speculation_interval[spark.speculation.interval] after the initial `spark.speculation.interval` passes.

`speculationScheduler` is shut down when <<stop, `TaskSchedulerImpl` stops>>.

=== [[checkSpeculatableTasks]] Checking for Speculatable Tasks -- `checkSpeculatableTasks` Method

[source, scala]
----
checkSpeculatableTasks(): Unit
----

`checkSpeculatableTasks` requests `rootPool` to check for speculatable tasks (if they ran for more than `100` ms) and, if there any, requests link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` to revive offers].

NOTE: `checkSpeculatableTasks` is executed periodically as part of link:spark-taskschedulerimpl-speculative-execution.adoc[speculative execution of tasks].

=== [[maxTaskFailures]] Acceptable Number of Task Failures -- `maxTaskFailures` Attribute

The acceptable number of task failures (`maxTaskFailures`) can be explicitly defined when <<creating-instance, creating TaskSchedulerImpl instance>> or based on <<spark_task_maxFailures, spark.task.maxFailures>> setting that defaults to 4 failures.

NOTE: It is exclusively used when <<submitTasks, submitting tasks>> through link:spark-tasksetmanager.adoc[TaskSetManager].

=== [[removeExecutor]] Cleaning up After Removing Executor -- `removeExecutor` Internal Method

[source, scala]
----
removeExecutor(executorId: String, reason: ExecutorLossReason): Unit
----

`removeExecutor` removes the `executorId` executor from the following <<internal-registries, internal registries>>: <<executorIdToTaskCount, executorIdToTaskCount>>, `executorIdToHost`, `executorsByHost`, and `hostsByRack`. If the affected hosts and racks are the last entries in `executorsByHost` and `hostsByRack`, appropriately, they are removed from the registries.

Unless `reason` is `LossReasonPending`, the executor is removed from `executorIdToHost` registry and link:spark-taskscheduler-schedulable.adoc#executorLost[TaskSetManagers get notified].

NOTE: The internal `removeExecutor` is called as part of <<statusUpdate, statusUpdate>> and link:spark-taskscheduler.adoc#executorLost[executorLost].

=== [[isLocal]] Local vs Non-Local Mode -- `isLocal` Attribute

CAUTION: FIXME

=== [[postStartHook]] Post-Start Initialization -- `postStartHook` Method

`postStartHook` is a custom implementation of link:spark-taskscheduler.adoc#contract[postStartHook from the TaskScheduler Contract] that waits until a scheduler backend is ready (using the internal blocking <<waitBackendReady, waitBackendReady>>).

NOTE: `postStartHook` is used when link:spark-sparkcontext.adoc#creating-instance[SparkContext is created] (before it is fully created) and link:yarn/spark-yarn-yarnclusterscheduler.adoc#postStartHook[YarnClusterScheduler.postStartHook].

=== [[waitBackendReady]] Waiting Until SchedulerBackend is Ready -- `waitBackendReady` Method

The private `waitBackendReady` method waits until a link:spark-scheduler-backends.adoc#contract[SchedulerBackend is ready].

It keeps on checking the status every 100 milliseconds until the SchedulerBackend is ready or the link:spark-sparkcontext.adoc#stop[SparkContext is stopped].

If the SparkContext happens to be stopped while doing the waiting, a `IllegalStateException` is thrown with the message:

```
Spark context stopped while waiting for backend
```

=== [[stop]] Stopping TaskSchedulerImpl -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop()` stops all the internal services, i.e. <<task-scheduler-speculation, `task-scheduler-speculation` executor service>>, link:spark-scheduler-backends.adoc[SchedulerBackend], link:spark-taskschedulerimpl-TaskResultGetter.adoc[TaskResultGetter], and <<starvationTimer, starvationTimer>> timer.

=== [[defaultParallelism]] Calculating Default Level of Parallelism -- `defaultParallelism` Method

*Default level of parallelism* is a hint for sizing jobs. It is a part of the link:spark-taskscheduler.adoc#contract[TaskScheduler contract] and link:spark-sparkcontext.adoc#defaultParallelism[used by SparkContext] to create RDDs with the right number of partitions when not specified explicitly.

`TaskSchedulerImpl` uses link:spark-scheduler-backends.adoc#defaultParallelism[SchedulerBackend.defaultParallelism()] to calculate the value, i.e. it just passes it along to a scheduler backend.

=== [[submitTasks]] Submitting Tasks -- `submitTasks` Method

NOTE: `submitTasks` is a part of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

[source, scala]
----
submitTasks(taskSet: TaskSet): Unit
----

`submitTasks` creates a link:spark-tasksetmanager.adoc[TaskSetManager] for the input link:spark-taskscheduler-tasksets.adoc[TaskSet] and link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[adds it to the `Schedulable` root pool].

NOTE: The link:spark-taskscheduler.adoc#rootPool[root pool] can be a single flat linked queue (in link:spark-taskscheduler-FIFOSchedulableBuilder.adoc[FIFO scheduling mode]) or a hierarchy of pools of `Schedulables` (in link:spark-taskscheduler-FairSchedulableBuilder.adoc[FAIR scheduling mode]).

It makes sure that the requested resources, i.e. CPU and memory, are assigned to the Spark application for a non-local environment before requesting the current link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` to revive offers].

.TaskSchedulerImpl.submitTasks
image::images/taskschedulerImpl-submitTasks.png[align="center"]

NOTE: If there are tasks to launch for missing partitions in a stage, DAGScheduler executes `submitTasks` (see link:spark-dagscheduler.adoc#submitMissingTasks[submitMissingTasks for Stage and Job]).

When `submitTasks` is called, you should see the following INFO message in the logs:

```
INFO TaskSchedulerImpl: Adding task set [taskSet.id] with [tasks.length] tasks
```

It creates a new link:spark-tasksetmanager.adoc[TaskSetManager] for the input `taskSet` and the <<maxTaskFailures, acceptable number of task failures>>.

NOTE: The acceptable number of task failures is specified when a <<creating-instance, TaskSchedulerImpl is created>>.

NOTE: A `TaskSet` knows the tasks to execute (as `tasks`) and stage id (as `stageId`) the tasks belong to. Read link:spark-taskscheduler-tasksets.adoc[TaskSets].

The `TaskSet` is registered in the internal <<taskSetsByStageIdAndAttempt, taskSetsByStageIdAndAttempt>> registry with the `TaskSetManager`.

If there is more than one active link:spark-tasksetmanager.adoc[TaskSetManager] for the stage, a `IllegalStateException` is thrown with the message:

```
more than one active taskSet for stage [stage]: [TaskSet ids]
```

NOTE: `TaskSetManager` is considered *active* when it is not a *zombie*.

The `TaskSetManager` is link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[added to the `Schedulable` pool (via `SchedulableBuilder`)].

When the method is called the very first time (`hasReceivedTask` is `false`) in cluster mode only (i.e. `isLocal` of the `TaskSchedulerImpl` is `false`), `starvationTimer` is scheduled to execute after <<spark_starvation_timeout, spark.starvation.timeout>>  to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager.

NOTE: After the first `spark.starvation.timeout` passes, the internal `hasReceivedTask` flag becomes `true`.

Every time the starvation timer thread is executed and `hasLaunchedTask` flag is `false`, the following WARN message is printed out to the logs:

```
WARN Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
```

Otherwise, when the `hasLaunchedTask` flag is `true` the timer thread cancels itself.

Ultimately, `submitTasks` requests the link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` to revive offers].

TIP: Use `dag-scheduler-event-loop` thread to step through the code in a debugger.

=== [[resourceOffers]] Processing Executor Resource Offers -- `resourceOffers` Method

[source, scala]
----
resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]]
----

`resourceOffers` method is called by link:spark-scheduler-backends.adoc[SchedulerBackend] (for clustered environments) or link:spark-local.adoc#LocalBackend[LocalBackend] (for local mode) with `WorkerOffer` resource offers that represent cores (CPUs) available on all the active executors with one `WorkerOffer` per active executor.

.Processing Executor Resource Offers
image::images/taskscheduler-resourceOffers.png[align="center"]

NOTE: `resourceOffers` is a mechanism to propagate information about active executors to `TaskSchedulerImpl` with the hosts and racks (if supported by the cluster manager).

A `WorkerOffer` is a 3-tuple with executor id, host, and the number of free cores available.

[source, scala]
----
WorkerOffer(executorId: String, host: String, cores: Int)
----

For each `WorkerOffer` (that represents free cores on an executor) `resourceOffers` method records the host per executor id (using the internal `executorIdToHost`) and sets `0` as the number of tasks running on the executor if there are no tasks on the executor (using <<executorIdToTaskCount, executorIdToTaskCount>>). It also records hosts (with executors in the internal `executorsByHost` registry).

WARNING: FIXME BUG? Why is the executor id *not* added to `executorsByHost`?

For the offers with a host that has not been recorded yet (in the internal `executorsByHost` registry) the following occurs:

1. The host is recorded in the internal `executorsByHost` registry.
2. <<executorAdded, executorAdded>> callback is called (with the executor id and the host from the offer).
3. `newExecAvail` flag is enabled (it is later used to inform `TaskSetManagers` about the new executor).

CAUTION: FIXME a picture with `executorAdded` call from TaskSchedulerImpl to DAGScheduler.

It shuffles the input `offers` that is supposed to help evenly distributing tasks across executors (that the input `offers` represent) and builds internal structures like `tasks` and `availableCpus`.

.Internal Structures of resourceOffers with 5 WorkerOffers
image::images/TaskSchedulerImpl-resourceOffers-internal-structures.png[align="center"]

The root pool is requested for link:spark-taskscheduler-pool.adoc#getSortedTaskSetQueue[TaskSetManagers sorted appropriately] (according to the link:spark-taskscheduler-schedulingmode.adoc[scheduling order]).

NOTE: `rootPool` is a part of the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract] and is exclusively managed by link:spark-taskscheduler-schedulablebuilders.adoc[SchedulableBuilders] (that  link:spark-taskscheduler-schedulablebuilders.adoc#addTaskSetManager[add `TaskSetManagers` to the root pool].

For every `TaskSetManager` in the `TaskSetManager` sorted queue, the following DEBUG message is printed out to the logs:

```
DEBUG TaskSchedulerImpl: parentName: [taskSet.parent.name], name: [taskSet.name], runningTasks: [taskSet.runningTasks]
```

NOTE: The internal `rootPool` is configured while <<initialize, TaskSchedulerImpl is being initialized>>.

While traversing over the sorted collection of `TaskSetManagers`, if a new host (with an executor) was registered, i.e. the `newExecAvail` flag is enabled, `TaskSetManagers` are link:spark-tasksetmanager.adoc#executorAdded[informed about the new executor added].

NOTE: A `TaskSetManager` will be informed about one or more new executors once per host regardless of the number of executors registered on the host.

For each `TaskSetManager` (in `sortedTaskSets`) and for each preferred locality level (ascending), <<resourceOfferSingleTaskSet, resourceOfferSingleTaskSet>> is called until `launchedTask` flag is `false`.

CAUTION: FIXME `resourceOfferSingleTaskSet` + the sentence above less code-centric.

Check whether the number of cores in an offer is greater than the <<spark_task_cpus, number of cores needed for a task>>.

When `resourceOffers` managed to launch a task (i.e. `tasks` collection is not empty), the internal `hasLaunchedTask` flag becomes `true` (that effectively means what the name says _"There were executors and I managed to launch a task"_).

`resourceOffers` returns the `tasks` collection.

NOTE: `resourceOffers` is called when link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#makeOffers[`CoarseGrainedSchedulerBackend` makes resource offers].

==== [[resourceOfferSingleTaskSet]] `resourceOfferSingleTaskSet` Method

[source, scala]
----
resourceOfferSingleTaskSet(
  taskSet: TaskSetManager,
  maxLocality: TaskLocality,
  shuffledOffers: Seq[WorkerOffer],
  availableCpus: Array[Int],
  tasks: Seq[ArrayBuffer[TaskDescription]]): Boolean
----

`resourceOfferSingleTaskSet` is a private helper method that is executed when...

=== [[statusUpdate]] `statusUpdate` Method

[source, scala]
----
statusUpdate(
  tid: Long,
  state: TaskState.TaskState,
  serializedData: ByteBuffer): Unit
----

`statusUpdate` removes a lost executor when a `tid` task has failed. For all task states, `statusUpdate` removes the `tid` task from the internal registries, i.e. <<taskIdToTaskSetManager, taskIdToTaskSetManager>> and <<taskIdToExecutorId, taskIdToExecutorId>>, and decrements the number of running tasks in <<executorIdToTaskCount, executorIdToTaskCount>> registry. For `tid` in `FINISHED`, `FAILED`, `KILLED` or `LOST` states, `statusUpdate` link:spark-tasksetmanager.adoc#removeRunningTask[informs the `TaskSetManager` that the task can be removed from the running tasks]. For `tid` in `FINISHED` state `statusUpdate` link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueSuccessfulTask[schedules an asynchrounous task to deserialize the task result (and notify `TaskSchedulerImpl`)] while for `FAILED`, `KILLED` or `LOST` states it calls link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueFailedTask[TaskResultGetter.enqueueFailedTask]. Ultimately, given an executor that has been lost, `statusUpdate` informs link:spark-dagscheduler.adoc#executorLost[informs `DAGScheduler` that the executor was lost] and link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` is requested to revive offers].

For `tid` task in `LOST` state and an executor still <<taskIdToExecutorId, assigned for the task>> and tracked in <<executorIdToTaskCount, executorIdToTaskCount>> registry, the executor is <<removeExecutor, removed>> (with reason `Task [tid] was lost, so marking the executor as lost as well.`).

CAUTION: FIXME Why is link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers()] called only for lost executors?

`statusUpdate` looks up the link:spark-tasksetmanager.adoc[TaskSetManager] for `tid` (in <<taskIdToTaskSetManager, taskIdToTaskSetManager>> registry).

When the `TaskSetManager` is found and the task is in a link:spark-taskscheduler-tasks.adoc#states[finished state], the task is removed from the internal registries, i.e. <<taskIdToTaskSetManager, taskIdToTaskSetManager>> and <<taskIdToExecutorId, taskIdToExecutorId>>, and the number of currently running tasks for the executor is decremented (in <<executorIdToTaskCount, executorIdToTaskCount>> registry).

For a task in `FINISHED` state, the task is link:spark-tasksetmanager.adoc#removeRunningTask[removed from the running tasks] and link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueSuccessfulTask[an asynchrounous task is scheduled to deserialize the task result (and notify `TaskSchedulerImpl`)].

For a task in `FAILED`, `KILLED`, or `LOST` state, the task is link:spark-tasksetmanager.adoc#removeRunningTask[removed from the running tasks] (as for the `FINISHED` state) and then link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueFailedTask[TaskResultGetter.enqueueFailedTask] is called.

If the `TaskSetManager` for `tid` could not be found (in <<taskIdToTaskSetManager, taskIdToTaskSetManager>> registry), you should see the following ERROR message in the logs:

```
ERROR Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates)
```

Any exception is caught and reported as ERROR message in the logs:

```
ERROR Exception in statusUpdate
```

Ultimately, for `tid` task with an executor marked as lost, `statusUpdate` link:spark-dagscheduler.adoc#executorLost[informs `DAGScheduler` that the executor was lost] (with `SlaveLost` and the reason `Task [tid] was lost, so marking the executor as lost as well.`) and link:spark-scheduler-backends.adoc#reviveOffers[`SchedulerBackend` is requested to revive offers].

CAUTION: FIXME image with scheduler backends calling `TaskSchedulerImpl.statusUpdate`.

NOTE: `statusUpdate` is called when link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#StatusUpdate[CoarseGrainedSchedulerBackend], `LocalSchedulerBackend` and `MesosFineGrainedSchedulerBackend` inform about task state changes.

=== [[handleFailedTask]] Notifying TaskSetManager that Task Failed -- `handleFailedTask` Method

[source, scala]
----
handleFailedTask(
  taskSetManager: TaskSetManager,
  tid: Long,
  taskState: TaskState,
  reason: TaskFailedReason): Unit
----

`handleFailedTask` link:spark-tasksetmanager.adoc#handleFailedTask[notifies `taskSetManager` that `tid` task has failed] and, only when link:spark-tasksetmanager.adoc#zombie-state[`taskSetManager` is not in zombie state] and `tid` is not in `KILLED` state, link:spark-scheduler-backends.adoc#reviveOffers[requests `SchedulerBackend` to revive offers].

NOTE: `handleFailedTask` is called when link:spark-taskschedulerimpl-TaskResultGetter.adoc#enqueueSuccessfulTask[`TaskResultGetter` deserializes a `TaskFailedReason`] for a failed task.

=== [[taskSetFinished]] `taskSetFinished` Method

[source, scala]
----
taskSetFinished(manager: TaskSetManager): Unit
----

`taskSetFinished` looks all link:spark-taskscheduler-tasksets.adoc[TaskSet]s up by the stage id (in <<taskSetsByStageIdAndAttempt, taskSetsByStageIdAndAttempt>> registry) and removes the stage attempt from them, possibly with removing the entire stage record from `taskSetsByStageIdAndAttempt` registry completely (if there are no other attempts registered).

.TaskSchedulerImpl.taskSetFinished is called when all tasks are finished
image::images/taskschedulerimpl-tasksetmanager-tasksetfinished.png[align="center"]

NOTE: A `TaskSetManager` manages a `TaskSet` for a stage.

`taskSetFinished` then link:spark-taskscheduler-pool.adoc#removeSchedulable[removes `manager` from the parent's schedulable pool].

You should see the following INFO message in the logs:

```
INFO Removed TaskSet [id], whose tasks have all completed, from pool [name]
```

NOTE: `taskSetFinished` method is called when link:spark-tasksetmanager.adoc#maybeFinishTaskSet[`TaskSetManager` has received the results of all the tasks in a `TaskSet`].

=== [[executorAdded]] `executorAdded` Method

[source, scala]
----
executorAdded(execId: String, host: String)
----

`executorAdded` method simply passes the notification on to the `DAGScheduler` (using link:spark-dagscheduler.adoc#executorAdded[DAGScheduler.executorAdded])

CAUTION: FIXME Image with a call from TaskSchedulerImpl to DAGScheduler, please.

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property | Default Value | Description
| [[spark_task_maxFailures]] `spark.task.maxFailures` | `4` in link:spark-cluster.adoc[cluster mode]

`1` in link:spark-local.adoc[local] except link:spark-local.adoc[local-with-retries] | The number of individual task failures before giving up on the entire link:spark-taskscheduler-tasksets.adoc[TaskSet] and the job afterwards.

| [[spark_task_cpus]] `spark.task.cpus` | `1` | The number of CPUs to request per task.

| [[spark_starvation_timeout]] `spark.starvation.timeout` | `15s` | Threshold above which Spark warns a user that an initial TaskSet may be starved.

| [[spark_scheduler_mode]] `spark.scheduler.mode` | `FIFO` | A case-insensitive name of the link:spark-taskscheduler-schedulingmode.adoc[scheduling mode] -- `FAIR`, `FIFO`, or `NONE`.

NOTE: Only `FAIR` and `FIFO` are supported by `TaskSchedulerImpl`. See <<schedulableBuilder, schedulableBuilder>>.
|======================
