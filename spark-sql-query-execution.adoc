== [[QueryExecution]] Query Execution

`QueryExecution` is a part of the public link:spark-sql-dataset.adoc[Dataset] API and represents the query execution that will eventually produce the data in a `Dataset`.

[NOTE]
====
You can access the `QueryExecution` of a `Dataset` using link:spark-sql-dataset.adoc#queryExecution[queryExecution] attribute.

[source, scala]
----
val spark: SparkSession = ...
spark.emptyDataset[Long].queryExecution
----
====

`QueryExecution` is the result of link:spark-sql-sessionstate.adoc#executePlan[executing a `LogicalPlan` in a `SparkSession`] (and so you could create a `Dataset` from a `LogicalPlan` or use the `QueryExecution` after executing the `LogicalPlan`).

`QueryExecution` uses the input `SparkSession` to access the current link:spark-sql-queryplanner.adoc#SparkPlanner[SparkPlanner] (through link:spark-sql-sessionstate.adoc[SessionState] that could also return a link:spark-sql-queryplanner.adoc#HiveSessionState[HiveSessionState]) when <<creating-instance, it is created>>. It then computes a link:spark-sql-catalyst-SparkPlan.adoc[SparkPlan] (a `PhysicalPlan` exactly) using the planner. It is available as the `sparkPlan` (lazy) attribute.

A streaming variant of `QueryExecution` is <<IncrementalExecution, IncrementalExecution>>.

link:spark-sql-debugging-execution.adoc[`debug` package object] contains methods for *debugging query execution* that you can use to do the full analysis of your queries (as `Dataset` objects).

CAUTION: FIXME What's `planner`? `analyzed`? Why do we need `assertAnalyzed` and `assertSupported`?

It belongs to `org.apache.spark.sql.execution` package.

NOTE: `QueryExecution` is a transient feature of a link:spark-sql-dataset.adoc[Dataset], i.e. it is not preserved across serializations.

[source, scala]
----
val ds = spark.range(5)
scala> ds.queryExecution
res17: org.apache.spark.sql.execution.QueryExecution =
== Parsed Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 5, 1, 8, [id#39L]

== Optimized Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 5, [id#39L]
----

=== [[creating-instance]] Creating QueryExecution Instance

[source, scala]
----
class QueryExecution(
  val sparkSession: SparkSession,
  val logical: LogicalPlan)
----

`QueryExecution` requires a link:spark-sql-sparksession.adoc[SparkSession] and a link:spark-sql-catalyst-LogicalPlan.adoc[LogicalPlan].

=== [[planner]] Accessing SparkPlanner (planner method)

[source, scala]
----
planner: SparkPlanner
----

`planner` returns link:spark-sql-queryplanner.adoc#SparkPlanner[SparkPlanner].

`planner` is merely to expose internal link:spark-sql-sessionstate.adoc#planner[planner] (in the current link:spark-sql-sessionstate.adoc[SessionState]).

=== [[lazy-attributes]] Lazy Attributes

`QueryExecution` holds lazy attributes that are initializes at the first read.

==== [[analyzed]] analyzed Attribute

`analyzed` lazy value is a link:spark-sql-catalyst-SparkPlan.adoc[SparkPlan] that is a result of executing the input `logical` logical plan by the current `Analyzer` (of the current link:spark-sql-sparksession.adoc[SparkSession]).

NOTE: `Analyzer` resolves unresolved attributes and relations to typed objects using information in a `SessionCatalog` and `FunctionRegistry`.

==== [[withCachedData]] withCachedData Attribute

`withCachedData` being a `LogicalPlan` that is the `analyzed` plan after being analyzed, checked (for unsupported operations) and replaced with cached segments.

==== [[optimizedPlan]] Optimized Logical Query Plan -- `optimizedPlan` Attribute

`optimizedPlan` is the `LogicalPlan` (of a structured query) being the result of executing the session-owned link:spark-sql-sessionstate.adoc#optimizer[Catalyst Query Optimizer] to <<withCachedData, withCachedData>>.

==== [[sparkPlan]] sparkPlan Attribute

`sparkPlan` is the link:spark-sql-catalyst-SparkPlan.adoc[SparkPlan] that is the result of requesting <<planner, SparkPlanner>> to plan a <<optimizedPlan, optimized logical query plan>>.

NOTE: In fact, the result `SparkPlan` is the first Spark query plan from the collection of possible query plans from <<planner, SparkPlanner>>.

==== [[executedPlan]] executedPlan Attribute

`executedPlan` attribute is computed lazily and represents a link:spark-sql-catalyst-SparkPlan.adoc[SparkPlan] ready for execution. It is the <<sparkPlan, sparkPlan>> plan with all the <<preparations, preparation rules>> applied.

[source, scala]
----
val dataset: Dataset[Long] = ...
dataset.queryExecution.executedPlan
----

==== [[toRdd]] toRdd Attribute

`toRdd` is the `RDD[InternalRow]` that is the result of executing <<executedPlan, executedPlan>> plan, i.e.

[source, scala]
----
executedPlan.execute()
----

TIP: link:spark-sql-InternalRow.adoc[InternalRow] is the internal optimized binary row format.

=== [[preparations]] preparations -- SparkPlan Optimization Rules (to apply before Query Execution)

`preparations` is a sequence of link:spark-sql-catalyst-SparkPlan.adoc[SparkPlan] optimization rules.

TIP: A `SparkPlan` optimization rule transforms a `SparkPlan` to another `SparkPlan`.

This collection is an intermediate phase of query execution that developers can used to introduce further optimizations.

The current list of `SparkPlan` transformations in `preparations` is as follows:

1. `ExtractPythonUDFs`
2. `PlanSubqueries`
3. `EnsureRequirements`
4. `CollapseCodegenStages`
5. `ReuseExchange`
6. `ReuseSubquery`

NOTE: The transformation rules applied in order to the physical plan before execution, i.e. they generate a `SparkPlan` when <<executedPlan, executedPlan>> lazy value is accessed.

=== [[IncrementalExecution]] IncrementalExecution

`IncrementalExecution` is a custom `QueryExecution` with `OutputMode`, `checkpointLocation`, and `currentBatchId`.

It lives in `org.apache.spark.sql.execution.streaming` package.

CAUTION: FIXME What is `stateStrategy`?

Stateful operators in the query plan are numbered using `operatorId` that starts with `0`.

`IncrementalExecution` adds one `Rule[SparkPlan]` called `state` to <<preparations, preparations>> sequence of rules as the first element.

CAUTION: FIXME What does `IncrementalExecution` do? Where is it used?
