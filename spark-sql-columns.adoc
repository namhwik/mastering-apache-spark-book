== [[Column]] Column Operators

`Column` type represents a column in a dataset.

You can add new columns do a `Dataset` using link:spark-sql-dataset.adoc#withColumn[withColumn] method.

[source, scala]
----
val spark: SparkSession = ...
val dataset = spark.range(5)

// Add a new column called "group"
scala> dataset.withColumn("group", 'id % 2).show
+---+-----+
| id|group|
+---+-----+
|  0|    0|
|  1|    1|
|  2|    0|
|  3|    1|
|  4|    0|
+---+-----+
----

With the link:spark-sql-sparksession.adoc#implicits[implicits] converstions imported, you can create columns using Scala's symbols.

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

import org.apache.spark.sql.Column
scala> val nameCol: Column = 'name
nameCol: org.apache.spark.sql.Column = name
----

You can also create columns from ``$``-prefixed strings.

[source, scala]
----
// Note that $ alone creates a ColumnName
scala> val idCol = $"id"
idCol: org.apache.spark.sql.ColumnName = id

import org.apache.spark.sql.Column

// The target type triggers the implicit conversion to Column
scala> val idCol: Column = $"id"
idCol: org.apache.spark.sql.Column = id
----

Beside using the `implicits` conversions to create columns, you can use link:spark-sql-functions.adoc#col[col] and link:spark-sql-functions.adoc#column[column] methods from link:spark-sql-functions.adoc[functions] object.

[source, scala]
----
import org.apache.spark.sql.functions._

scala> val nameCol = col("name")
nameCol: org.apache.spark.sql.Column = name

scala> val cityCol = column("city")
cityCol: org.apache.spark.sql.Column = city
----

Finally, you can create a `Column` using the `Dataset` it belongs to using link:spark-sql-dataset.adoc#apply[Dataset.apply] factory method or link:spark-sql-dataset.adoc#col[Dataset.col] method.

[source, scala]
----
scala> val textCol = dataset.col("text")
textCol: org.apache.spark.sql.Column = text

scala> val idCol = dataset.apply("id")
idCol: org.apache.spark.sql.Column = id

scala> val idCol = dataset("id")
idCol: org.apache.spark.sql.Column = id
----

You can reference nested columns using `.` (dot).

CAUTION: FIXME

=== [[like]] like

CAUTION: FIXME

[source, scala]
----
scala> df("id") like "0"
res0: org.apache.spark.sql.Column = id LIKE 0

scala> df.filter('id like "0").show
+---+-----+
| id| text|
+---+-----+
|  0|hello|
+---+-----+
----

=== [[symbols-as-column-names]] Symbols As Column Names

[source, scala]
----
scala> val df = Seq((0, "hello"), (1, "world")).toDF("id", "text")
df: org.apache.spark.sql.DataFrame = [id: int, text: string]

scala> df.select('id)
res0: org.apache.spark.sql.DataFrame = [id: int]

scala> df.select('id).show
+---+
| id|
+---+
|  0|
|  1|
+---+
----

=== [[over]] over function

[source, scala]
----
over(window: expressions.WindowSpec): Column
----

`over` function defines a *windowing column* that allows for window computations to be applied to a window. Window functions are defined using link:spark-sql-windows.adoc#WindowSpec[WindowSpec].

TIP: Read about Windows in link:spark-sql-windows.adoc[Windows].

=== [[cast]] cast

`cast` method casts a column to a data type. It makes for type-safe maps with link:spark-sql-dataframe-row.adoc[Row] objects of the proper type (not `Any`).

[source,scala]
----
cast(to: String): Column
cast(to: DataType): Column
----

It uses link:spark-sql-sql-parsers.adoc#CatalystSqlParser[CatalystSqlParser] to parse the data type from its canonical string representation.

==== [[cast-example]] cast Example

[source, scala]
----
scala> val df = Seq((0f, "hello")).toDF("label", "text")
df: org.apache.spark.sql.DataFrame = [label: float, text: string]

scala> df.printSchema
root
 |-- label: float (nullable = false)
 |-- text: string (nullable = true)

// without cast
import org.apache.spark.sql.Row
scala> df.select("label").map { case Row(label) => label.getClass.getName }.show(false)
+---------------+
|value          |
+---------------+
|java.lang.Float|
+---------------+

// with cast
import org.apache.spark.sql.types.DoubleType
scala> df.select(col("label").cast(DoubleType)).map { case Row(label) => label.getClass.getName }.show(false)
+----------------+
|value           |
+----------------+
|java.lang.Double|
+----------------+
----
