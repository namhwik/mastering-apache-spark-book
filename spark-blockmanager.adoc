== BlockManager

`BlockManager` is a key-value store for blocks of data in Spark. `BlockManager` acts as a local cache that runs on every node in Spark cluster, i.e. the link:spark-driver.adoc[driver] and link:spark-executor.adoc[executors]. It provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. memory, disk, and off-heap. See <<stores, Stores>> in this document.

A `BlockManager` is a link:spark-blockdatamanager.adoc[BlockDataManager], i.e. manages the storage for blocks that can represent cached RDD partitions, intermediate shuffle outputs, broadcasts, etc. It is also a <<BlockEvictionHandler, BlockEvictionHandler>> that drops a block from memory and storing it on a disk if applicable.

*Cached blocks* are blocks with non-zero sum of memory and disk sizes.

`BlockManager` is created as a link:spark-sparkenv.adoc#create[Spark application starts].

A `BlockManager` must be <<initialize, initialized>> before it is fully operable.

When the <<externalShuffleServiceEnabled, External Shuffle Service is enabled>>, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

[TIP]
====
Enable `INFO`, `DEBUG` or `TRACE` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManager=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

[TIP]
====
You may want to shut off WARN messages being printed out about the current state of blocks using the following line to cut the noise:

```
log4j.logger.org.apache.spark.storage.BlockManager=OFF
```
====

=== [[externalShuffleServiceEnabled]] Using External Shuffle Service (externalShuffleServiceEnabled flag)

When the link:spark-ExternalShuffleService.adoc[External Shuffle Service] is enabled for a Spark application, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

CAUTION: FIXME How is `shuffleClient` used?

=== [[registerTask]] registerTask

CAUTION: FIXME

=== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* link:spark-MemoryStore.adoc[MemoryStore] for memory storage level.
* link:spark-DiskStore.adoc[DiskStore] for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

=== [[putBytes]] Storing Block (putBytes method)

[source, scala]
----
putBytes(
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

`putBytes` puts the `blockId` block of `bytes` bytes and `level` storage level to `BlockManager`.

It simply passes the call on to the internal <<doPutBytes, doPutBytes>>.

==== [[doPutBytes]] doPutBytes

[source, scala]
----
def doPutBytes[T](
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  classTag: ClassTag[T],
  tellMaster: Boolean = true,
  keepReadLock: Boolean = false): Boolean
----

`doPutBytes` is an internal method that calls the internal helper <<doPut, doPut>> with `putBody` being a function that accepts a `BlockInfo` and does the uploading.

If the replication storage level is greater than 1, replication starts in a separate thread (using the internal <<replicate, replicate>> method).

CAUTION: FIXME When is replication storage level greater than 1?

For a memory storage level, depending on whether it is a deserialized one or not, `putIteratorAsValues` or `putBytes` of link:spark-MemoryStore.adoc[MemoryStore] are used, respectively. If the put did not succeed and the storage level is also a disk one, you should see the following WARN message in the logs:

```
WARN BlockManager: Persisting block [blockId] to disk instead.
```

link:spark-DiskStore.adoc#putBytes[DiskStore.putBytes] is called.

NOTE: link:spark-DiskStore.adoc[DiskStore] is only used when link:spark-MemoryStore.adoc[MemoryStore] has failed for memory and disk storage levels.

If the storage level is a disk one only, link:spark-DiskStore.adoc#putBytes[DiskStore.putBytes] is called.

`doPutBytes` requests <<getCurrentBlockStatus, current block status>> and if the block was successfully stored, and the driver should know about it (`tellMaster`), it <<reportBlockStatus, reports current storage status of the block to the driver>>. The link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status].

Regardless of the block being successfully stored or not, you should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Put block [blockId] locally took [time] ms
```

For replication level greater than `1`, `doPutBytes` waits for the earlier asynchronous replication to finish.

The final result of `doPutBytes` is the result of storing the block successful or not (as computed earlier).

=== [[replicate]] replicate

CAUTION: FIXME

=== [[doPutIterator]] doPutIterator

CAUTION: FIXME

=== [[doPut]] doPut

[source, scala]
----
doPut[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[_],
  tellMaster: Boolean,
  keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T]
----

`doPut` is an internal helper method for <<doPutBytes, doPutBytes>> and <<doPutIterator, doPutIterator>>.

`doPut` executes the input `putBody` function with a link:spark-BlockInfo.adoc[BlockInfo] being a new `BlockInfo` object that link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[BlockInfoManager managed to create a lock for writing].

If the block has already been created, the following WARN message is printed out to the logs:

```
WARN Block [blockId] already exists on this machine; not re-adding it
```

It <<releaseLock, releases the read lock for the block>> when `keepReadLock` flag is disabled. `doPut` returns `None` immediately.

`putBody` is executed.

If the result of `putBody` is `None` the block is considered saved successfully.

For successful save and `keepReadLock` enabled, `blockInfoManager.downgradeLock(blockId)` is called.

For successful save and `keepReadLock` disabled, `blockInfoManager.unlock(blockId)` is called.

For unsuccessful save, `blockInfoManager.removeBlock(blockId)` is called and the following WARN message is printed out to the logs:

```
WARN Putting block [blockId] failed
```

Ultimately, the following DEBUG message is printed out to the logs:

```
DEBUG Putting block [blockId] [withOrWithout] replication took [usedTime] ms
```

=== [[removeBlock]] Removing Block From Memory and Disk (removeBlock method)

[source, scala]
----
removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit
----

`removeBlock` removes the `blockId` block from the link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

When executed, it prints out the following DEBUG message to the logs:

```
DEBUG Removing block [blockId]
```

It requests link:spark-BlockInfoManager.adoc[BlockInfoManager] for lock for writing for the `blockId` block. If it receives none, it prints out the following WARN message to the logs and quits.

```
WARN Asked to remove block [blockId], which does not exist
```

Otherwise, with a write lock for the block, the block is removed from link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] (see link:spark-MemoryStore.adoc#remove[Removing Block in `MemoryStore`] and link:spark-DiskStore.adoc#remove[Removing Block in `DiskStore`]).

If both removals fail, it prints out the following WARN message:

```
WARN Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store
```

The block is removed from link:spark-BlockInfoManager.adoc[BlockInfoManager].

It then <<getCurrentBlockStatus, calculates the current block status>> that is used to <<reportBlockStatus, report the block status to the driver>> (if the input `tellMaster` and the info's `tellMaster` are both enabled, i.e. `true`) and the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

NOTE: It is used to <<removeRdd, remove RDDs>> and <<removeBroadcast, broadcast>> as well as in <<BlockManagerSlaveEndpoint-RemoveBlock, BlockManagerSlaveEndpoint while handling `RemoveBlock` messages>>.

=== [[removeRdd]] Removing RDD Blocks (removeRdd method)

[source, scala]
----
removeRdd(rddId: Int): Int
----

`removeRdd` removes all the blocks that belong to the `rddId` RDD.

It prints out the following INFO message to the logs:

```
INFO Removing RDD [rddId]
```

It then requests RDD blocks from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>> (without informing the driver).

The number of blocks removed is the final result.

NOTE: It is used by <<BlockManagerSlaveEndpoint-RemoveRdd, BlockManagerSlaveEndpoint while handling `RemoveRdd` messages>>.

=== [[removeBroadcast]] Removing Broadcast Blocks (removeBroadcast method)

[source, scala]
----
removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int
----

`removeBroadcast` removes all the blocks of the input `broadcastId` broadcast.

Internally, it starts by printing out the following DEBUG message to the logs:

```
DEBUG Removing broadcast [broadcastId]
```

It then requests all the link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the `broadcastId` broadcast from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>>.

The number of blocks removed is the final result.

NOTE: It is used by <<BlockManagerSlaveEndpoint-RemoveBroadcast, BlockManagerSlaveEndpoint while handling `RemoveBroadcast` messages>>.

=== [[getStatus]] Getting Block Status (getStatus method)

CAUTION: FIXME

=== [[creating-instance]] Creating BlockManager Instance

A `BlockManager` needs the following services to be created:

* `executorId` (for the driver and executors)
* link:spark-rpc.adoc[RpcEnv]
* link:spark-BlockManagerMaster.adoc[BlockManagerMaster]
* link:spark-SerializerManager.adoc[SerializerManager]
* link:spark-configuration.adoc[SparkConf]
* link:spark-MemoryManager.adoc[MemoryManager]
* link:spark-service-mapoutputtracker.adoc[MapOutputTracker]
* link:spark-shuffle-manager.adoc[ShuffleManager]
* link:spark-blocktransferservice.adoc[BlockTransferService]
* `SecurityManager`

NOTE: `executorId` is `SparkContext.DRIVER_IDENTIFIER`, i.e. `driver` for the driver and the value of link:spark-executor-backends-coarse-grained.adoc#executor-id[--executor-id] command-line argument for link:spark-executor-backends-coarse-grained.adoc[CoarseGrainedExecutorBackend] executors or link:spark-executor-backends.adoc#MesosExecutorBackend[MesosExecutorBackend].

CAUTION: FIXME Elaborate on the executor backends and executor ids.

When a `BlockManager` instance is created it sets the internal `externalShuffleServiceEnabled` flag to the value of link:spark-ExternalShuffleService.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] setting.

It creates an instance of <<DiskBlockManager, DiskBlockManager>> (requesting `deleteFilesOnStop` when an external shuffle service is not in use).

It creates an instance of link:spark-BlockInfoManager.adoc[BlockInfoManager] (as `blockInfoManager`).

It creates *block-manager-future* daemon cached thread pool with 128 threads maximum (as `futureExecutionContext`).

It creates a link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

link:spark-MemoryManager.adoc[MemoryManager] gets the link:spark-MemoryStore.adoc[MemoryStore] object assigned.

It requests the current maximum memory from `MemoryManager` (using `maxOnHeapStorageMemory` as `maxMemory`).

It calculates the port used by the external shuffle service (as `externalShuffleServicePort`).

NOTE: It is computed specially in Spark on YARN.

CAUTION: FIXME Describe the YARN-specific part.

It creates a client to read other executors' shuffle files (as `shuffleClient`). If the external shuffle service is used an link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] is created or the input link:spark-blocktransferservice.adoc[BlockTransferService] is used.

It sets <<spark.block.failures.beforeLocationRefresh, the maximum number of failures before this block manager refreshes the block locations from the driver>> (as `maxFailuresBeforeLocationRefresh`).

It <<BlockManagerSlaveEndpoint, registers BlockManagerSlaveEndpoint>> with the input link:spark-rpc.adoc[RpcEnv], itself, and link:spark-service-mapoutputtracker.adoc[MapOutputTracker] (as `slaveEndpoint`).

NOTE: A `BlockManager` instance is created while link:spark-sparkenv.adoc#create[SparkEnv is being created].

=== [[shuffleClient]] shuffleClient

CAUTION: FIXME

(that is assumed to be a link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient])

=== [[shuffleServerId]] shuffleServerId

CAUTION: FIXME

=== [[initialize]] Initializing BlockManager (initialize method)

[source, scala]
----
initialize(appId: String): Unit
----

`initialize` method is called to initialize the `BlockManager` instance on the driver and executors (see link:spark-sparkcontext.adoc#creating-instance[Creating SparkContext Instance] and link:spark-executor.adoc#creating-instance[Creating Executor Instance], respectively).

NOTE: The method must be called before a `BlockManager` can be considered fully operable.

It does the following:

1. It initializes link:spark-blocktransferservice.adoc[BlockTransferService].
2. It initializes a shuffle client, be it link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] or link:spark-blocktransferservice.adoc[BlockTransferService].
3. It sets <<shuffleServerId, shuffleServerId>> to an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for link:spark-blocktransferservice.adoc[BlockTransferService].
4. It creates the address of the server that serves this executor's shuffle files (using <<shuffleServerId, shuffleServerId>>)

CAUTION: FIXME Describe `shuffleServerId`. Where is it used?

If the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

It link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

Ultimately, if the initialization happens on an executor and the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, it <<registerWithExternalShuffleServer, registers to the shuffle service>>.

NOTE: The method is called when the link:spark-sparkcontext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and `SparkContext` is created)] and when an link:spark-executor.adoc#creating-instance[Executor is launched].

==== [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server (registerWithExternalShuffleServer method)

[source, scala]
----
registerWithExternalShuffleServer(): Unit
----

`registerWithExternalShuffleServer` is an internal helper method to register the `BlockManager` for an executor with an link:spark-ExternalShuffleService.adoc[external shuffle server].

NOTE: It is executed when a <<initialize, `BlockManager` is initialized on an executor and an external shuffle service is used>>.

When executed, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

It uses <<shuffleClient, shuffleClient>> to link:spark-shuffleclient.adoc#ExternalShuffleClient-registerWithShuffleServer[register the block manager] using <<shuffleServerId, shuffleServerId>> (i.e. the host, the port and the executorId) and a `ExecutorShuffleInfo`.

NOTE: The `ExecutorShuffleInfo` uses `localDirs` and `subDirsPerLocalDir` from <<DiskBlockManager, DiskBlockManager>> and the class name of the constructor link:spark-shuffle-manager.adoc[ShuffleManager].

It tries to register at most 3 times with 5-second sleeps in-between.

NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured.

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds...
```

=== [[reregister]] Re-registering Blocks to Driver (reregister method)

[source, scala]
----
reregister(): Unit
----

When is called, you should see the following INFO in the logs:

```
INFO BlockManager: BlockManager re-registering with master
```

It link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] (just as it was when <<initialize, BlockManager was initializing>>). It passes the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

CAUTION: FIXME Where is `maxMemory` used once passed to the driver?

`reregister` will then report all the local blocks to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster].

You should see the following INFO message in the logs:

```
INFO BlockManager: Reporting [blockInfoManager.size] blocks to the master.
```

For each block metadata (in link:spark-BlockInfoManager.adoc[BlockInfoManager]) it <<getCurrentBlockStatus, gets block current status>> and <<tryToReportBlockStatus, tries to send it to the BlockManagerMaster>>.

If there is an issue communicating to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs:

```
ERROR BlockManager: Failed to report [blockId] to master; giving up.
```

After the ERROR message, `reregister` stops reporting.

NOTE: `reregister` is called by link:spark-executor.adoc#heartbeats-and-active-task-metrics[Executor when it was told to re-register while sending heartbeats].

=== [[getCurrentBlockStatus]] Calculate Current Block Status (getCurrentBlockStatus method)

[source, scala]
----
getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus
----

`getCurrentBlockStatus` returns the current `BlockStatus` of the `BlockId` block (with the block's current link:spark-rdd-caching.adoc#StorageLevel[StorageLevel], memory and disk sizes). It uses link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] for size and other information.

NOTE: Most of the information to build `BlockStatus` is already in `BlockInfo` except that it may not necessarily reflect the current state per link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

Internally, it uses the input link:spark-BlockInfo.adoc[BlockInfo] to know about the block's storage level. If the storage level is not set (i.e. `null`), the returned `BlockStatus` assumes the link:spark-rdd-caching.adoc#StorageLevel[default NONE storage level] and the memory and disk sizes being `0`.

If however the storage level is set, `getCurrentBlockStatus` uses link:spark-MemoryStore.adoc[MemoryStore] or link:spark-DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their `getSize` or assume `0`).

NOTE: It is acceptable that the `BlockInfo` says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status.

NOTE: `getCurrentBlockStatus` is used when <<reregister, executor's BlockManager is requested to report the current status of the local blocks to the master>>, <<doPutBytes, saving a block to a storage>> or <<dropFromMemory, removing a block from memory only>> or <<removeBlock, both, i.e. from memory and disk>>.

=== [[dropFromMemory]] Removing Blocks From Memory Only (dropFromMemory method)

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

When `dropFromMemory` is executed, you should see the following INFO message in the logs:

```
INFO BlockManager: Dropping block [blockId] from memory
```

It then asserts that the `blockId` block is link:spark-BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing].

If the block's StorageLevel uses disks and the internal link:spark-DiskStore.adoc[DiskStore] object (`diskStore`) does not contain the block, it is saved then. You should see the following INFO message in the logs:

```
INFO BlockManager: Writing block [blockId] to disk
```

CAUTION: FIXME Describe the case with saving a block to disk.

The block's memory size is fetched and recorded (using `MemoryStore.getSize`).

The block is link:spark-MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs:

```
WARN BlockManager: Block [blockId] could not be dropped from memory as it does not exist
```

It then <<getCurrentBlockStatus, calculates the current storage status of the block>> and <<reportBlockStatus, reports it to the driver>>. It only happens when `info.tellMaster`.

CAUTION: FIXME When would `info.tellMaster` be `true`?

A block is considered updated when it was written to disk or removed from memory or both. If either happened, the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

Ultimately, `dropFromMemory` returns the current storage level of the block.

NOTE: `dropFromMemory` is part of the single-method <<BlockEvictionHandler, BlockEvictionHandler>> interface.

=== [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver (reportBlockStatus method)

[source, scala]
----
reportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Unit
----

`reportBlockStatus` is an internal method for <<tryToReportBlockStatus, reporting a block status to the driver>> and if told to re-register it prints out the following INFO message to the logs:

```
INFO BlockManager: Got told to re-register updating block [blockId]
```

It does asynchronous reregistration (using `asyncReregister`).

In either case, it prints out the following DEBUG message to the logs:

```
DEBUG BlockManager: Told master about block [blockId]
```

NOTE: `reportBlockStatus` is called by <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, dropFromMemory>>, and <<removeBlock, removeBlock>>.

==== [[tryToReportBlockStatus]] tryToReportBlockStatus

[source, scala]
----
def tryToReportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Boolean
----

`tryToReportBlockStatus` is an internal method to report block status to the driver.

It executes link:spark-BlockManagerMaster.adoc#updateBlockInfo[BlockManagerMaster.updateBlockInfo] only if the state changes should be reported to the driver (i.e. `info.tellMaster` is enabled).

It returns `true` or link:spark-BlockManagerMaster.adoc#updateBlockInfo[BlockManagerMaster.updateBlockInfo]'s response.

=== [[BlockEvictionHandler]] BlockEvictionHandler

`BlockEvictionHandler` is a `private[storage]` Scala trait with a single method <<BlockEvictionHandler-dropFromMemory, dropFromMemory>>.

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

NOTE: A `BlockManager` is a `BlockEvictionHandler`.

NOTE: `dropFromMemory` is called when  link:spark-MemoryStore.adoc#evictBlocksToFreeSpace[`MemoryStore` evicts blocks from memory to free space].

=== [[BlockManagerSlaveEndpoint]] BlockManagerSlaveEndpoint

`BlockManagerSlaveEndpoint` is a link:spark-rpc.adoc#ThreadSafeRpcEndpoint[thread-safe RPC endpoint] for remote communication between executors and the driver.

CAUTION: FIXME the intro needs more love.

While a <<creating-instance, BlockManager is being created>> so is the `BlockManagerSlaveEndpoint` RPC endpoint with the name *BlockManagerEndpoint[randomId]* to handle <<BlockManagerSlaveEndpoint-messages, RPC messages>>.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.storage.BlockManagerSlaveEndpoint` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

==== [[BlockManagerSlaveEndpoint-RemoveBlock]] RemoveBlock Message

[source, scala]
----
RemoveBlock(blockId: BlockId)
----

When a `RemoveBlock` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing block [blockId]
```

It then calls <<removeBlock, BlockManager to remove `blockId` block>>.

NOTE: Handling `RemoveBlock` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing block [blockId], response is [response]
```

And `true` response is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: true to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing block [blockId]
```

==== [[BlockManagerSlaveEndpoint-RemoveRdd]] RemoveRdd Message

[source, scala]
----
RemoveRdd(rddId: Int)
----

When a `RemoveRdd` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing RDD [rddId]
```

It then calls <<removeRdd, BlockManager to remove `rddId` RDD>>.

NOTE: Handling `RemoveRdd` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing RDD [rddId], response is [response]
```

And the number of blocks removed is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [#blocks] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing RDD [rddId]
```

==== [[BlockManagerSlaveEndpoint-RemoveShuffle]] RemoveShuffle Message

[source, scala]
----
RemoveShuffle(shuffleId: Int)
----

When a `RemoveShuffle` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing shuffle [shuffleId]
```

If link:spark-service-mapoutputtracker.adoc[MapOutputTracker] was given (when the RPC endpoint was created), it calls link:spark-service-mapoutputtracker.adoc#unregisterShuffle[MapOutputTracker to unregister the `shuffleId` shuffle].

It then calls link:spark-shuffle-manager.adoc#unregisterShuffle[ShuffleManager to unregister the `shuffleId` shuffle].

NOTE: Handling `RemoveShuffle` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing shuffle [shuffleId], response is [response]
```

And the result is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [response] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing shuffle [shuffleId]
```

==== [[BlockManagerSlaveEndpoint-RemoveBroadcast]] RemoveBroadcast Message

[source, scala]
----
RemoveBroadcast(broadcastId: Long)
----

When a `RemoveBroadcast` message comes in, you should see the following DEBUG message in the logs:

```
DEBUG BlockManagerSlaveEndpoint: removing broadcast [broadcastId]
```

It then calls <<removeBroadcast, BlockManager to remove the `broadcastId` broadcast>>.

NOTE: Handling `RemoveBroadcast` messages happens on a separate thread. See <<BlockManagerSlaveEndpoint-asyncThreadPool, BlockManagerSlaveEndpoint Thread Pool>>.

When the computation is successful, you should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Done removing broadcast [broadcastId], response is [response]
```

And the result is sent back. You should see the following DEBUG in the logs:

```
DEBUG BlockManagerSlaveEndpoint: Sent response: [response] to [senderAddress]
```

In case of failure, you should see the following ERROR in the logs and the stack trace.

```
ERROR BlockManagerSlaveEndpoint: Error in removing broadcast [broadcastId]
```

==== [[BlockManagerSlaveEndpoint-GetBlockStatus]] GetBlockStatus Message

[source, scala]
----
GetBlockStatus(blockId: BlockId)
----

When a `GetBlockStatus` message comes in, it responds with the result of <<getStatus, calling BlockManager about the status of `blockId`>>.

==== [[BlockManagerSlaveEndpoint-GetMatchingBlockIds]] GetMatchingBlockIds Message

[source, scala]
----
GetMatchingBlockIds(filter: BlockId => Boolean)
----

When a `GetMatchingBlockIds` message comes in, it responds with the result of <<getMatchingBlockIds, calling BlockManager for matching blocks for `filter`>>.

==== [[BlockManagerSlaveEndpoint-TriggerThreadDump]] TriggerThreadDump Message

When a `TriggerThreadDump` message comes in, a thread dump is generated and sent back.

==== [[BlockManagerSlaveEndpoint-asyncThreadPool]] BlockManagerSlaveEndpoint Thread Pool

`BlockManagerSlaveEndpoint` uses *block-manager-slave-async-thread-pool* daemon thread pool (`asyncThreadPool`) for some messages to talk to other Spark services, i.e. `BlockManager`, link:spark-service-mapoutputtracker.adoc[MapOutputTracker], link:spark-shuffle-manager.adoc[ShuffleManager] in a non-blocking, asynchronous way.

The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients.

NOTE: `BlockManagerSlaveEndpoint` uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor].

=== [[broadcast]] Broadcast Values

When a new broadcast value is created, link:spark-broadcast.adoc#TorrentBroadcast[TorrentBroadcast] blocks are put in the block manager.

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[DiskBlockManager]] DiskBlockManager

DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations.

By default, one block is mapped to one file with a name given by its BlockId. It is however possible to have a block map to only a segment of a file.

Block files are hashed among the directories listed in `spark.local.dir` (or in `SPARK_LOCAL_DIRS` if set).

CAUTION: FIXME Review me.

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== [[metrics]] Metrics

Block Manager uses link:spark-metrics.adoc[Spark Metrics System] (via `BlockManagerSource`) to report metrics about internal status.

The name of the source is *BlockManager*.

It emits the following numbers:

* memory / maxMem_MB - the maximum memory configured
* memory / remainingMem_MB - the remaining memory
* memory / memUsed_MB - the memory used
* memory / diskSpaceUsed_MB - the disk used

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a `BlockManager` object for the `spark.app.id` id.
