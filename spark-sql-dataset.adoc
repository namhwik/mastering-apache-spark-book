== Dataset

*Dataset* is the Spark SQL API for working with structured data, i.e. records with a known schema.

Datasets are "lazy" and computations are only triggered when an action is invoked. Internally, a `Dataset` represents a link:spark-sql-logical-plan.adoc[logical plan] that describes the computation required to produce the data (for a given link:spark-sql-sparksession.adoc[Spark SQL session]).

.Dataset's Internals
image::images/spark-sql-Dataset.png[align="center"]

It is fair to say that a Dataset is a result of executing an expression against data storage like files or databases. The expression can be described by a SQL query or a Scala/Java lambda function.

Dataset API comes with declarative and type-safe operators (that improves on the earlier experience of data processing using link:spark-sql-dataframe.adoc[DataFrames] that were a set of link:spark-sql-dataframe-row.adoc[Rows]).

[NOTE]
====
`Dataset` was first introduced in Apache Spark *1.6.0* as an experimental feature, and has since turned itself into a fully supported API.

As of Spark *2.0.0*, link:spark-sql-dataframe.adoc[DataFrame] - the flagship data abstraction of previous versions of Spark SQL - is currently a _mere_ type alias for `Dataset[Row]`:

[source, scala]
----
type DataFrame = Dataset[Row]
----

See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45[package object sql].
====

`Dataset` offers convenience of RDDs with the performance optimizations of DataFrames and the strong static type-safety of Scala. The last feature of bringing the strong type-safety to link:spark-sql-dataframe.adoc[DataFrame] makes Dataset so appealing. All the features together give you a more functional programming interface to work with structured data.

It is only with Datasets to have syntax and analysis checks at compile time (that was not possible using link:spark-sql-dataframe.adoc[DataFrame], regular SQL queries or even RDDs).

Using `Dataset` objects turns `DataFrames` of link:spark-sql-dataframe-row.adoc[Row] instances into a `DataFrames` of case classes with proper names and types (following their equivalents in the case classes). Instead of using indices to access respective fields in a DataFrame and cast it to a type, all this is automatically handled by Datasets and checked by the Scala compiler.

Datasets use link:spark-sql-catalyst.adoc[Catalyst Query Optimizer] and link:spark-sql-tungsten.adoc[Tungsten] to optimize query performance.

A `Dataset` object requires a link:spark-sql-sqlcontext.adoc[SQLContext], a link:spark-sql-query-execution.adoc[QueryExecution], and an link:spark-sql-Encoder.adoc[Encoder]. link:spark-sql-sqlcontext.adoc#creating-datasets[In same cases], a `Dataset` can also be seen as a pair of link:spark-sql-logical-plan.adoc[LogicalPlan] in a given link:spark-sql-sqlcontext.adoc[SQLContext].

NOTE: `SQLContext` and link:spark-sql-query-execution.adoc[QueryExecution] are transient and hence do not participate in Dataset serialization. The only _firmly-tied_ feature of a Dataset is the link:spark-sql-Encoder.adoc[Encoder].

A `Dataset` is <<Queryable, Queryable>> and `Serializable`, i.e. can be saved to a persistent storage.

It also has a <<schema, schema>>.

You can convert a type-safe Dataset to a "untyped" DataFrame (see <<implicits, Type Conversions to Dataset[T]>>) or access the RDD that sits underneath (see <<rdd, Converting Datasets into RDDs (using rdd method)>>). It is supposed to give you a more pleasant experience while transitioning from legacy RDD-based or DataFrame-based APIs.

The default storage level for `Datasets` is link:spark-rdd-caching.adoc[MEMORY_AND_DISK] because recomputing the in-memory columnar representation of the underlying table is expensive. See <<persist, Persisting Dataset (persist method)>> in this document.

Spark 2.0 has introduced a new query model called link:spark-sql-structured-streaming.adoc[Structured Streaming] for continuous incremental execution of structured queries. That made possible to consider Datasets a static and bounded data as well as streaming and unbounded with one single API.

=== [[join]] join

CAUTION: FIXME

=== [[where]] where

CAUTION: FIXME

=== [[groupBy]] groupBy

CAUTION: FIXME

=== [[foreachPartition]] foreachPartition method

[source, scala]
----
foreachPartition(f: Iterator[T] => Unit): Unit
----

`foreachPartition` applies the `f` function to each partition of the `Dataset`.

[source, scala]
----
case class Record(id: Int, city: String)
val ds = Seq(Record(0, "Warsaw"), Record(1, "London")).toDS

ds.foreachPartition { iter: Iterator[Record] => iter.foreach(println) }
----

NOTE: `foreachPartition` is used to link:spark-sql-dataframewriter.adoc#jdbc[save a `DataFrame` to a JDBC table] (indirectly through `JdbcUtils.saveTable`) and link:spark-sql-streaming-ForeachSink.adoc[ForeachSink].

=== [[mapPartitions]] mapPartitions method

[source, scala]
----
mapPartitions[U: Encoder](func: Iterator[T] => Iterator[U]): Dataset[U]
----

`mapPartitions` returns a new `Dataset` (of type `U`) with the function `func` applied to each partition.

CAUTION: FIXME Example

=== [[flatMap]] Creating Zero or More Records (flatMap method)

[source, scala]
----
flatMap[U: Encoder](func: T => TraversableOnce[U]): Dataset[U]
----

`flatMap` returns a new `Dataset` (of type `U`) with all records (of type `T`) mapped over using the function `func` and then flattening the results.

NOTE: `flatMap` can create new records. It deprecated `explode`.

[source, scala]
----
final case class Sentence(id: Long, text: String)
val sentences = Seq(Sentence(0, "hello world"), Sentence(1, "witaj swiecie")).toDS

scala> sentences.flatMap(s => s.text.split("\\s+")).show
+-------+
|  value|
+-------+
|  hello|
|  world|
|  witaj|
|swiecie|
+-------+
----

Internally, `flatMap` calls <<mapPartitions, mapPartitions>> with the partitions `flatMap(ped)`.

=== [[cache]] Caching Dataset (cache method)

[source, scala]
----
cache(): this.type
----

`cache` merely passes the calls to no-argument <<persist, persist>> method.

=== [[persist]] Persisting Dataset (persist method)

[source, scala]
----
persist(): this.type
persist(newLevel: StorageLevel): this.type
----

`persist` caches the `Dataset` using the default storage level `MEMORY_AND_DISK` or `newLevel`.

Internally, it requests the link:spark-cachemanager.adoc#cacheQuery[`CacheManager` to cache the query].

=== [[unpersist]] Unpersisting Dataset (unpersist method)

[source, scala]
----
unpersist(blocking: Boolean): this.type
----

`unpersist` uncache the `Dataset` possibly by `blocking` the call.

Internally, it requests the link:spark-cachemanager.adoc#uncacheQuery[`CacheManager` to uncache the query].

=== [[repartition]] Repartitioning Dataset (repartition method)

[source, scala]
----
repartition(numPartitions: Int): Dataset[T]
----

`repartition` repartition the `Dataset` to exactly `numPartitions` partitions.

=== [[features]] Features of Dataset API

The features of the Dataset API in Spark SQL:

* **Type-safety** as Datasets are Scala domain objects and operations operate on their attributes. All is checked by the Scala compiler at compile time.

=== [[implicits]][[toDS]][[toDF]] Type Conversions to Dataset[T] (and DataFrame) (toDS and toDF methods)

`DatasetHolder` case class offers three methods that do the conversions from `Seq[T]` or `RDD[T]` type to `Dataset[T]`:

* `toDS(): Dataset[T]`
* `toDF(): DataFrame`
* `toDF(colNames: String*): DataFrame`

NOTE: `DataFrame` is a _mere_ type alias for `Dataset[Row]` since Spark *2.0.0*.

`DatasetHolder` is used by `SQLImplicits` that is available to use after link:spark-sql-sqlcontext.adoc#implicits[importing implicits object of SQLContext].

[source, scala]
----
scala> val ds = Seq("I am a shiny Dataset!").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> val df = Seq("I am an old grumpy DataFrame!").toDF
df: org.apache.spark.sql.DataFrame = [value: string]

scala> val df = Seq("I am an old grumpy DataFrame!").toDF("text")
df: org.apache.spark.sql.DataFrame = [text: string]

scala> val ds = sc.parallelize(Seq("hello")).toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]
----

[NOTE]
====
This import is automatically executed in link:spark-shell.adoc[Spark Shell].

```
scala> sc.version
res11: String = 2.0.0-SNAPSHOT

scala> :imports
 1) import spark.implicits._  (59 terms, 38 are implicit)
 2) import spark.sql          (1 terms)
```
====

[source, scala]
----
import spark.implicits._

case class Token(name: String, productId: Int, score: Double)
val data = Seq(
  Token("aaa", 100, 0.12),
  Token("aaa", 200, 0.29),
  Token("bbb", 200, 0.53),
  Token("bbb", 300, 0.42))

// Transform data to a Dataset[Token]
// It doesn't work with type annotation yet
// https://issues.apache.org/jira/browse/SPARK-13456
val ds: Dataset[Token] = data.toDS

// Transform data into a DataFrame with no explicit schema
val df = data.toDF

// Transform DataFrame into a Dataset
val ds = df.as[Token]

scala> ds.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> ds.printSchema
root
 |-- name: string (nullable = true)
 |-- productId: integer (nullable = false)
 |-- score: double (nullable = false)

// In DataFrames we work with Row instances
scala> df.map(_.getClass.getName).show(false)
+--------------------------------------------------------------+
|value                                                         |
+--------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
+--------------------------------------------------------------+

// In Datasets we work with case class instances
scala> ds.map(_.getClass.getName).show(false)
+---------------------------+
|value                      |
+---------------------------+
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
+---------------------------+

scala> ds.map(_.name).show
+-----+
|value|
+-----+
|  aaa|
|  aaa|
|  bbb|
|  bbb|
+-----+
----

=== [[rdd]] Converting Datasets into RDDs (using rdd method)

Whenever you are in need to convert a `Dataset` into a `RDD`, executing `rdd` method gives you a RDD of the proper input object type (not link:spark-sql-dataframe.adoc#features[Row as in DataFrames]).

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

=== [[schema]] Schema

A `Dataset` has a *schema*.

[source, scala]
----
schema: StructType
----

[TIP]
====
You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* <<explain, explain>>
====

=== [[types]] Supported Types

CAUTION: FIXME What types are supported by Encoders

=== [[toJSON]] toJSON

`toJSON` maps the content of `Dataset` to a `Dataset` of JSON strings.

NOTE: A new feature in Spark **2.0.0**.

[source, scala]
----
scala> val ds = Seq("hello", "world", "foo bar").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> ds.toJSON.show
+-------------------+
|              value|
+-------------------+
|  {"value":"hello"}|
|  {"value":"world"}|
|{"value":"foo bar"}|
+-------------------+
----

=== [[explain]] explain

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

`explain` prints the link:spark-sql-logical-plan.adoc[logical] and physical plans to the console. You can use it for debugging.

TIP: If you are serious about query debugging you could also use the link:spark-sql-query-execution.adoc#debug[Debugging Query Execution facility].

Internally, `explain` uses `SQLContext.executePlan(logicalPlan)`.

[source, scala]
----
val ds = spark.range(10)

scala> ds.explain(extended = true)
== Parsed Logical Plan ==
Range 0, 10, 1, 8, [id#9L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 10, 1, 8, [id#9L]

== Optimized Logical Plan ==
Range 0, 10, 1, 8, [id#9L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 10, [id#9L]
----

=== [[select]] select

[source, scala]
----
select[U1: Encoder](c1: TypedColumn[T, U1]): Dataset[U1]
select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]
select[U1, U2, U3](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]
select[U1, U2, U3, U4](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]
select[U1, U2, U3, U4, U5](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4],
  c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]
----

CAUTION: FIXME

=== [[selectExpr]] selectExpr

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

`selectExpr` is like `select`, but accepts SQL expressions `exprs`.

[source, scala]
----
val ds = spark.range(5)

scala> ds.selectExpr("rand() as random").show
16/04/14 23:16:06 INFO HiveSqlParser: Parsing command: rand() as random
+-------------------+
|             random|
+-------------------+
|  0.887675894185651|
|0.36766085091074086|
| 0.2700020856675186|
| 0.1489033635529543|
| 0.5862990791950973|
+-------------------+
----

Internally, it executes `select` with every expression in `exprs` mapped to link:spark-sql-columns.adoc[Column] (using link:spark-sql-sql-parsers.adoc[SparkSqlParser.parseExpression]).

[source, scala]
----
scala> ds.select(expr("rand() as random")).show
+------------------+
|            random|
+------------------+
|0.5514319279894851|
|0.2876221510433741|
|0.4599999092045741|
|0.5708558868374893|
|0.6223314406247136|
+------------------+
----

NOTE: A new feature in Spark **2.0.0**.

=== [[isStreaming]] isStreaming

`isStreaming` returns `true` when `Dataset` contains link:spark-sql-streamingrelation.adoc[StreamingRelation] or link:spark-sql-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] *streaming sources*.

NOTE: Streaming datasets are created using link:spark-sql-dataframereader.adoc#stream[DataFrameReader.stream] method (for link:spark-sql-streamingrelation.adoc[StreamingRelation]) and contain link:spark-sql-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] after link:spark-sql-dataframewriter.adoc#startStream[DataFrameWriter.startStream].

[source, scala]
----
val reader = spark.read
val helloStream = reader.stream("hello")

scala> helloStream.isStreaming
res9: Boolean = true
----

NOTE: A new feature in Spark **2.0.0**.

=== [[randomSplit]] randomSplit

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

`randomSplit` randomly splits the `Dataset` per `weights`.

`weights` doubles should sum up to `1` and will be normalized if they do not.

You can define `seed` and if you don't, a random `seed` will be used.

NOTE: It is used in link:spark-mllib-estimators.adoc#TrainValidationSplit[TrainValidationSplit] to split dataset into training and validation datasets.

[source, scala]
----
val ds = spark.range(10)
scala> ds.randomSplit(Array[Double](2, 3)).foreach(_.show)
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+

+---+
| id|
+---+
|  3|
|  4|
|  5|
|  6|
|  7|
|  8|
|  9|
+---+
----

NOTE: A new feature in Spark **2.0.0**.

=== [[Queryable]] Queryable

CAUTION: FIXME

=== [[withNewExecutionId]] Tracking Multi-Job SQL Query Executions (withNewExecutionId method)

[source, scala]
----
withNewExecutionId[U](body: => U): U
----

`withNewExecutionId` is a `private[sql]` method that executes the input `body` action using link:spark-sql-SQLExecution.adoc#withNewExecutionId[SQLExecution.withNewExecutionId] that sets the *execution id* local property set.

NOTE: It is used in `foreach`, <<foreachPartition, foreachPartition>>, and (private) `collect`.

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/i7l3JQRx7Qw[Structuring Spark: DataFrames, Datasets, and Streaming]
