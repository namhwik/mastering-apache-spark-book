== [[LogicalPlan]] Logical Query Plan

*Logical Query Plan* is the logical representation of a structured query expression (that yields a link:spark-sql-dataset.adoc[Dataset]).

It is modelled as the `LogicalPlan` abstract class which is a custom link:spark-sql-query-plan.adoc[QueryPlan].

[source, scala]
----
// an example dataset to work with
val dataset = spark.range(1)

scala> val plan = dataset.queryExecution.logical
plan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Range (0, 1, splits=8)
----

A logical plan can be *analyzed* which is to say that the plan (including children) has gone through analysis and verification.

[source, scala]
----
scala> plan.analyzed
res1: Boolean = true
----

A logical plan can also be *resolved* to a specific schema.

[source, scala]
----
scala> plan.resolved
res2: Boolean = true
----

A logical plan knows the size of objects that are results of query operators, like `join`, through `Statistics` object.

[source, scala]
----
scala> val stats = plan.statistics
stats: org.apache.spark.sql.catalyst.plans.logical.Statistics = Statistics(8,false)
----

A logical plan knows the maximum number of records it can compute.

[source, scala]
----
scala> val maxRows = plan.maxRows
maxRows: Option[Long] = None
----

A logical plan can be <<isStreaming, streaming>> if it contains one or more link:spark-sql-streaming-source.adoc[structured streaming sources].

=== [[resolveQuoted]] resolveQuoted method

CAUTION: FIXME

=== [[LeafNode]][[UnaryNode]][[specialized-logical-plans]] Specialized LogicalPlans

* `LeafNode`
* `UnaryNode`
* `BinaryNode`

=== [[LocalRelation]] LocalRelation Logical Plan

`LocalRelation` is a leaf logical plan that...

CAUTION: FIXME

It can only be constructed with the output attributes being all resolved.

The size of the objects (in `statistics`) is the sum of the default size of the attributes multiplied by the number of records.

=== [[Join]] Join Logical Plan

`Join` is a `LogicalPlan` that acts on two `LogicalPlan` objects. It has a join type and an optional expression for the join.

The following is a list of join types:

* `INNER`
* `LEFT OUTER`
* `RIGHT OUTER`
* `FULL OUTER`
* `LEFT SEMI`
* `NATURAL`

=== [[ExplainCommand]] ExplainCommand Logical Command

`ExplainCommand` <<RunnableCommand, logical command>> allows users to see how a structured query will be executed. It takes in a `LogicalPlan` and creates a link:spark-sql-query-execution.adoc[QueryExecution] that is used to output a single-column `DataFrame` with the following:

1. _codegen explain_, i.e. link:spark-sql-whole-stage-codegen.adoc[WholeStageCodegen] subtrees if `codegen` flag is enabled.

2. _extended explain_, i.e. the parsed, analyzed, optimized logical plans with the physical plan if `extended` flag is enabled.

3. _simple explain_, i.e. the physical plan only when no `codegen` and `extended` flags are enabled.

`ExplainCommand` is used for link:spark-sql-dataset.adoc#explain[explain operator] and `EXPLAIN` SQL statement (accepting `EXTENDED` and `CODEGEN` options).

[source, scala]
----
// Explain in SQL

scala> sql("EXPLAIN EXTENDED show tables").show(truncate = false)
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|plan                                                                                                                                                                                                                                           |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|== Parsed Logical Plan ==
ShowTablesCommand

== Analyzed Logical Plan ==
tableName: string, isTemporary: boolean
ShowTablesCommand

== Optimized Logical Plan ==
ShowTablesCommand

== Physical Plan ==
ExecutedCommand
   +- ShowTablesCommand|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
----

The following EXPLAIN variants in SQL queries are not supported:

* `EXPLAIN FORMATTED`
* `EXPLAIN LOGICAL`

[source, scala]
----
scala> sql("EXPLAIN LOGICAL show tables")
org.apache.spark.sql.catalyst.parser.ParseException:
Operation not allowed: EXPLAIN LOGICAL(line 1, pos 0)

== SQL ==
EXPLAIN LOGICAL show tables
^^^
...
----

=== [[RunnableCommand]] RunnableCommand -- Base Trait for Logical Commands

`RunnableCommand` is the base `trait` for side-effecting ``LogicalPlan``s, i.e. that can be executed for side-effects.

`RunnableCommand` defines one abstract method `run` that returns a collection of link:spark-sql-dataframe-row.adoc[Row]s.

[source, scala]
----
run(sparkSession: SparkSession): Seq[Row]
----

`RunnableCommand` has by default no `output` attributes and `children` logical plans.

=== [[isStreaming]] Is Logical Plan Structured Streaming (isStreaming method)

[source, scala]
----
isStreaming: Boolean
----

`isStreaming` is a part of the public API of `LogicalPlan` and is enabled (i.e. `true`) when a logical plan is a link:spark-sql-streaming-source.adoc[streaming source].

By default, it walks over subtrees and calls itself, i.e. `isStreaming`, on every child node to find a streaming source.

[source, scala]
----
val spark: SparkSession = ...

// Regular dataset
scala> val ints = spark.createDataset(0 to 9)
ints: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> ints.queryExecution.logical.isStreaming
res1: Boolean = false

// Streaming dataset
scala> val logs = spark.readStream.format("text").load("logs/*.out")
logs: org.apache.spark.sql.DataFrame = [value: string]

scala> logs.queryExecution.logical.isStreaming
res2: Boolean = true
----
