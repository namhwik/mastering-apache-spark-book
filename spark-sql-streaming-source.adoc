== [[Source]] Streaming Sources

A *Streaming Source* represents a continuous stream of data for a streaming query. It generates batches of link:spark-sql-dataframe.adoc[DataFrame] for given start and end offsets. For fault tolerance, a source must be able to replay data given a start offset.

A streaming source should be able to replay an arbitrary sequence of past data in the stream using a range of offsets. Streaming sources like Apache Kafka and Amazon Kinesis (with their per-record offsets) fit into this model nicely. This is the assumption so structured streaming can achieve end-to-end exactly-once guarantees.

A streaming source is described by <<contract, Source contract>> in `org.apache.spark.sql.execution.streaming` package.

[source, scala]
----
import org.apache.spark.sql.execution.streaming.Source
----

There are the following `Source` implementations available:

1. link:spark-sql-streaming-FileStreamSource.adoc[FileStreamSource]

2. link:spark-sql-streaming-MemoryStream.adoc[MemoryStream]

3. link:spark-sql-streaming-TextSocketSource.adoc[TextSocketSource]

=== [[contract]] Source Contract

`Source` contract requires that the streaming sources provide the following features:

1. The link:spark-sql-schema.adoc[schema] of the datasets using `schema` method.

2. The maximum *offset* (of type `Offset`) using `getOffset` method.

3. A *batch* for start and end offsets (of type link:spark-sql-dataframe.adoc[DataFrame]).
