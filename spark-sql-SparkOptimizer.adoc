== [[SparkOptimizer]] SparkOptimizer -- Logical Query Optimizer

`SparkOptimizer` is the one and only custom <<Optimizer, logical query plan optimizer>> in Spark SQL that comes with the <<batches, additional logical plan optimizations>>.

NOTE: You can extend the available logical plan optimizations and register yours using <<experimentalMethods, ExperimentalMethods>>.

`SparkOptimizer` is available as link:spark-sql-SessionState.adoc#optimizer[optimizer] attribute of `SessionState`.

[source, scala]
----
sparkSession.sessionState.optimizer
----

[NOTE]
====
The result of applying the <<batches, batches>> of `SparkOptimizer` to a link:spark-sql-LogicalPlan.adoc[LogicalPlan] is called *optimized logical plan*.

Optimized logical plan of a structured query is available as link:spark-sql-QueryExecution.adoc#optimizedPlan[optimizedPlan] attribute of `QueryExecution`.

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----
====

[[batches]]
.SparkOptimizer's Optimization Rules (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

^.^| Optimize Metadata Only Query
^.^| `Once`
| OptimizeMetadataOnlyQuery
|

^.^| Extract Python UDF from Aggregate
^.^| `Once`
| ExtractPythonUDFFromAggregate
|

^.^| Prune File Source Table Partitions
^.^| `Once`
| PruneFileSourcePartitions
|

^.^| [[User-Provided-Optimizers]] User Provided Optimizers
^.^| link:spark-sql-Optimizer.adoc#fixedPoint[FixedPoint]
| link:spark-sql-ExperimentalMethods.adoc#extraOptimizations[extraOptimizations]
|
|===

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating SparkOptimizer Instance

`SparkOptimizer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-SQLConf.adoc[SQLConf]
* [[experimentalMethods]] link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods]

NOTE: `SparkOptimizer` is created when `SessionState` link:spark-sql-SessionState.adoc#creating-instance[is created] (that initializes link:spark-sql-SessionState.adoc#optimizer[optimizer] property).

=== [[i-want-more]] Further reading or watching

1. https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]

2. (video) https://youtu.be/_1byVWTEK1s?t=19m7s[Modern Spark DataFrame and Dataset (Intermediate Tutorial)] by https://twitter.com/adbreind[Adam Breindel] from Databricks.
