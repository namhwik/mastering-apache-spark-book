== [[CatalogImpl]] CatalogImpl

`CatalogImpl` is the one and only link:spark-sql-Catalog.adoc[Catalog] that...FIXME

.CatalogImpl uses SessionCatalog (through SparkSession)
image::images/spark-sql-CatalogImpl.png[align="center"]

NOTE: `CatalogImpl` is in `org.apache.spark.sql.internal` package.

=== [[functionExists]] `functionExists` Method

CAUTION: FIXME

=== [[refreshTable]] `refreshTable` Method

CAUTION: FIXME

=== [[cacheTable]] Caching Table or View In-Memory -- `cacheTable` Method

[source, scala]
----
cacheTable(tableName: String): Unit
----

Internally, `cacheTable` first link:spark-sql-SparkSession.adoc#table[creates a DataFrame for the table] followed by requesting `CacheManager` to link:spark-sql-CacheManager.adoc#cacheQuery[cache it].

NOTE: `cacheTable` uses the link:spark-sql-SparkSession.adoc#sharedState[session-scoped SharedState] to access the `CacheManager`.

NOTE: `cacheTable` is a part of link:spark-sql-Catalog.adoc#contract[Catalog contract].

=== [[clearCache]] Removing All Cached Tables From In-Memory Cache -- `clearCache` Method

[source, scala]
----
clearCache(): Unit
----

`clearCache` requests `CacheManager` to link:spark-sql-CacheManager.adoc#clearCache[remove all cached tables from in-memory cache].

NOTE: `clearCache` is a part of link:spark-sql-Catalog.adoc#contract[Catalog contract].

=== [[createExternalTable]] Creating External Table From Path -- `createExternalTable` Method

[source, scala]
----
createExternalTable(tableName: String, path: String): DataFrame
createExternalTable(tableName: String, path: String, source: String): DataFrame
createExternalTable(
  tableName: String,
  source: String,
  options: Map[String, String]): DataFrame
createExternalTable(
  tableName: String,
  source: String,
  schema: StructType,
  options: Map[String, String]): DataFrame
----

`createExternalTable` creates an external table `tableName` from the given `path` and returns the corresponding link:spark-sql-DataFrame.adoc[DataFrame].

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

val readmeTable = spark.catalog.createExternalTable("readme", "README.md", "text")
readmeTable: org.apache.spark.sql.DataFrame = [value: string]

scala> spark.catalog.listTables.filter(_.name == "readme").show
+------+--------+-----------+---------+-----------+
|  name|database|description|tableType|isTemporary|
+------+--------+-----------+---------+-----------+
|readme| default|       null| EXTERNAL|      false|
+------+--------+-----------+---------+-----------+

scala> sql("select count(*) as count from readme").show(false)
+-----+
|count|
+-----+
|99   |
+-----+
----

The `source` input parameter is the name of the data source provider for the table, e.g. parquet, json, text. If not specified, `createExternalTable` uses link:spark-sql-settings.adoc#spark.sql.sources.default[spark.sql.sources.default] setting to know the data source format.

NOTE: `source` input parameter must not be `hive` as it leads to a `AnalysisException`.

`createExternalTable` sets the mandatory `path` option when specified explicitly in the input parameter list.

`createExternalTable` parses `tableName` into `TableIdentifier` (using link:spark-sql-SparkSqlParser.adoc[SparkSqlParser]). It creates a `CatalogTable` and then link:spark-sql-SessionState.adoc#executePlan[executes] (by link:spark-sql-QueryExecution.adoc#toRdd[toRDD]) a `CreateTable` link:spark-sql-LogicalPlan.adoc[logical plan]. The result link:spark-sql-DataFrame.adoc[DataFrame] is a `Dataset[Row]` with the link:spark-sql-QueryExecution.adoc[QueryExecution] after executing link:spark-sql-LogicalPlan-SubqueryAlias.adoc[SubqueryAlias] logical plan and link:spark-sql-RowEncoder.adoc[RowEncoder].

.CatalogImpl.createExternalTable
image::images/spark-sql-CatalogImpl-createExternalTable.png[align="center"]

NOTE: `createExternalTable` is a part of link:spark-sql-Catalog.adoc#contract[Catalog contract].
