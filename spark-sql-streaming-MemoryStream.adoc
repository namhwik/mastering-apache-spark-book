== MemoryStream

`MemoryStream` is a link:spark-sql-streaming-source.adoc[streaming source] that produces values (of type `T`) stored in memory.

It uses the internal <<batches, batches>> collection of link:spark-sql-dataset.adoc[datasets].

[CAUTION]
====
This source is *not* for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.

It is designed primarily for unit tests, tutorials and debugging.
====

[source, scala]
----
import org.apache.spark.sql.execution.streaming.MemoryStream

import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder.getOrCreate()

implicit val ctx = spark.sqlContext

// It uses two implicits: Encoder[Int] and SQLContext
val intsInput = MemoryStream[Int]

scala> val memoryQuery = ints.toDF.writeStream.format("memory").queryName("memStream").start
memoryQuery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query - memStream [state = ACTIVE]

scala> val zeroOffset = intsInput.addData(0, 1, 2)
zeroOffset: org.apache.spark.sql.execution.streaming.Offset = #0

memoryQuery.processAllAvailable()
val intsOut = spark.table("memStream").as[Int]
scala> intsOut.show
+-----+
|value|
+-----+
|    0|
|    1|
|    2|
+-----+

memoryQuery.stop()
----

CAUTION: FIXME Finish the example

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.streaming.MemoryStream` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.MemoryStream=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating MemoryStream Instance

[source, scala]
----
apply[A : Encoder](implicit sqlContext: SQLContext): MemoryStream[A]
----

`MemoryStream` object defines `apply` method that you can use to create instances of `MemoryStream` streaming sources.

=== [[addData]] Adding Data to Source (addData methods)

[source, scala]
----
addData(data: A*): Offset
addData(data: TraversableOnce[A]): Offset
----

`addData` methods add the input `data` to <<batches, batches>> internal collection.

When executed, `addData` adds a `DataFrame` (created using link:spark-sql-dataset.adoc#toDS[toDS] implicit method) and increments the internal `currentOffset` offset.

You should see the following DEBUG message in the logs:

```
DEBUG MemoryStream: Adding ds: [ds]
```

=== [[getBatch]] Getting Next Batch (getBatch method)

NOTE: `getBatch` is a part of link:spark-sql-streaming-source.adoc#contract[Streaming Source contract].

When executed, `getBatch` uses the internal <<batches, batches>> collection to return requested offsets.

You should see the following DEBUG message in the logs:

```
DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks]
```

=== [[logicalPlan]] StreamingExecutionRelation Logical Plan

`MemoryStream` uses `StreamingExecutionRelation` logical plan to build link:spark-sql-dataset.adoc[Datasets] or link:spark-sql-dataset.adoc#ofRows[DataFrames] when requested.

`StreamingExecutionRelation` is a link:spark-sql-LogicalPlan.adoc#LeafNode[leaf logical node] that is created for a link:spark-sql-streaming-source.adoc[streaming source] and a given `output` collection of link:spark-sql-catalyst-Attribute.adoc[Attribute]. It is a link:spark-sql-LogicalPlan.adoc#isStreaming[streaming logical plan] with the name being the name of the source.

[source, scala]
----
scala> val ints = MemoryStream[Int]
ints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13]

scala> ints.toDS.queryExecution.logical.isStreaming
res14: Boolean = true

scala> ints.toDS.queryExecution.logical
res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13]
----

=== [[schema]] Schema (schema method)

`MemoryStream` works with the data of the link:spark-sql-schema.adoc[schema] as described by the link:spark-sql-Encoder.adoc[Encoder] (of the `Dataset`).
