== [[SparkPlan]] `SparkPlan` -- Physical Execution Plan

`SparkPlan` is the base link:spark-sql-catalyst-QueryPlan.adoc[QueryPlan] for physical operators in a structured query (which is...a link:spark-sql-dataset.adoc[Dataset]!).

<<contract, SparkPlan contract>> assumes that concrete physical operators define `doExecute` method which is executed when the final `execute` is called.

NOTE: The final `execute` is triggered when the link:spark-sql-query-execution.adoc#toRdd[`QueryExecution` (of a `Dataset`) is requested for a `RDD[InternalRow]`].

When executed, a `SparkPlan` produces link:spark-rdd.adoc[RDDs] of link:spark-sql-InternalRow.adoc[InternalRow] (i.e. ``RDD[InternalRow]``s).

CAUTION: FIXME `SparkPlan` is `Serializable`. Why?

NOTE: The naming convention for physical operators in Spark's source code is to have their names end with the _Exec_ prefix, e.g. `DebugExec` or link:spark-sql-spark-plan-LocalTableScanExec.adoc[LocalTableScanExec].

TIP: Read link:spark-sql-InternalRow.adoc[InternalRow] about the internal binary row format.

`SparkPlan` has the following attributes:

* `metadata`
* <<metrics, metrics>>
* `outputPartitioning`
* `outputOrdering`

`SparkPlan` has the following `final` methods that prepare environment and pass calls on to corresponding methods that constitute <<contract, SparkPlan Contract>>:

* `execute` calls `doExecute`
* `prepare` calls `doPrepare`
* `executeBroadcast` calls `doExecuteBroadcast`

=== [[executeQuery]] Executing Query in Scope (after Preparations) -- `executeQuery` Final Method

[source, scala]
----
executeQuery[T](query: => T): T
----

`executeQuery` executes `query` in a scope (i.e. so that all RDDs created will have the same scope).

Internally, `executeQuery` calls <<prepare, prepare>> and <<waitForSubqueries, waitForSubqueries>> before executing `query`.

NOTE: `executeQuery` is executed as part of <<execute, execute>>, <<executeBroadcast, executeBroadcast>> and when link:spark-sql-whole-stage-codegen.adoc#CodegenSupport[CodegenSupport] produces a Java source code.

=== [[executeBroadcast]] Computing Query Result As Broadcast Variable -- `executeBroadcast` Final Method

[source, scala]
----
executeBroadcast[T](): broadcast.Broadcast[T]
----

`executeBroadcast` returns the results of the query as a broadcast variable.

Internally, `executeBroadcast` executes <<doExecuteBroadcast, doExecuteBroadcast>> inside <<executeQuery, executeQuery>>.

NOTE: `executeBroadcast` is executed in link:spark-sql-spark-plan-BroadcastHashJoinExec.adoc[BroadcastHashJoinExec], `BroadcastNestedLoopJoinExec` and `ReusedExchangeExec`.

=== [[waitForSubqueries]] `waitForSubqueries` Method

=== [[prepare]] `prepare` Method

=== [[contract]][[doExecute]][[doExecuteBroadcast]][[doPrepare]] SparkPlan Contract

The contract of `SparkPlan` requires that concrete implementations define the following method:

* `doExecute(): RDD[InternalRow]`

They may also define their own custom overrides:

* `doPrepare` to prepare execution
* `doExecuteBroadcast`

CAUTION: FIXME Why are there two executes?

=== [[UnaryExecNode]][[LeafExecNode]][[BinaryExecNode]][[specialized-spark-plans]] Specialized ``SparkPlan``s Physical Operators

* `UnaryExecNode` is a physical operator that...FIXME
* `LeafExecNode` is...FIXME
* `BinaryExecNode` is...FIXME

CAUTION: FIXME

=== [[executeCollect]] executeCollect

CAUTION: FIXME

=== [[SQLMetric]] SQLMetric

`SQLMetric` is an link:spark-accumulators.adoc[accumulator] that accumulate and produce long values.

There are three known `SQLMetrics`:

* `sum`
* `size`
* `timing`

=== [[metrics]] metrics Lookup Table

[source, scala]
----
metrics: Map[String, SQLMetric] = Map.empty
----

`metrics` is a `private[sql]` lookup table of supported <<SQLMetric, SQLMetrics>> by their names.
