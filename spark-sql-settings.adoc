== Settings

The following list are the settings used to configure Spark SQL applications.

You can apply them to link:spark-sql-sqlcontext.adoc[SQLContext] using `setConf` method:

[source, scala]
----
spark.setConf("spark.sql.codegen.wholeStage", "false")
----

=== [[spark.sql.warehouse.dir]] spark.sql.warehouse.dir

`spark.sql.warehouse.dir` (default: `${system:user.dir}/spark-warehouse`) is the default location for managed databases and tables.

=== [[spark.sql.parquet.filterPushdown]] spark.sql.parquet.filterPushdown

`spark.sql.parquet.filterPushdown` (default: `true`) is a flag to control the Parquet link:spark-sql-catalyst-predicate-pushdown.adoc[filter predicate push-down optimization].

=== [[spark.sql.catalogImplementation]] spark.sql.catalogImplementation

`spark.sql.catalogImplementation` (default: `in-memory`) is an internal setting to select the active catalog implementation.

There are two acceptable values:

* `in-memory` (default)
* `hive`

TIP: Read link:spark-sql-ExternalCatalog.adoc[ExternalCatalog -- System Catalog of Permanent Entities].

=== [[spark.sql.shuffle.partitions]] spark.sql.shuffle.partitions

`spark.sql.shuffle.partitions` (default: `200`) -- the default number of partitions to use when shuffling data for joins or aggregations.

=== [[spark.sql.allowMultipleContexts]] spark.sql.allowMultipleContexts

`spark.sql.allowMultipleContexts` (default: `true`) controls whether creating multiple SQLContexts/HiveContexts is allowed.

=== [[spark.sql.autoBroadcastJoinThreshold]][[autoBroadcastJoinThreshold]] spark.sql.autoBroadcastJoinThreshold

`spark.sql.autoBroadcastJoinThreshold` (default: `10 * 1024 * 1024`) configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. If the size of the statistics of the logical plan of a DataFrame is at most the setting, the DataFrame is broadcast for join.

Negative values or `0` disable broadcasting.

Consult link:spark-sql-joins.adoc#broadcast-join[Broadcast Join] for more information about the topic.

=== [[spark.sql.columnNameOfCorruptRecord]] spark.sql.columnNameOfCorruptRecord

`spark.sql.columnNameOfCorruptRecord`...FIXME

=== [[spark.sql.dialect]] spark.sql.dialect

`spark.sql.dialect` - FIXME

=== [[spark.sql.sources.default]] spark.sql.sources.default

`spark.sql.sources.default` (default: `parquet`) sets the default data source to use in input/output.

It is used when reading or writing data in link:spark-sql-dataframewriter.adoc[DataFrameWriter] and link:spark-sql-dataframereader.adoc[DataFrameReader], when link:spark-sql-Catalog.adoc#createExternalTable[creating external table from a path] (in `Catalog.createExternalTable`) and in the streaming link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader] and link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter].

=== [[spark.sql.streaming.checkpointLocation]] spark.sql.streaming.checkpointLocation

`spark.sql.streaming.checkpointLocation` is the default location for storing checkpoint data for link:spark-sql-StreamingQuery.adoc[continuously executing queries].

=== [[spark.sql.codegen.wholeStage]] spark.sql.codegen.wholeStage

`spark.sql.codegen.wholeStage` (default: `true`) controls whether the whole stage (of multiple operators) will be compiled into single java method (`true`) or not (`false`).
