== Settings

The following list are the settings used to configure Spark SQL applications.

You can set them in a link:spark-sql-SparkSession.adoc[SparkSession] upon instantiation using link:spark-sql-sparksession-builder.adoc#config[config] method.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .config("spark.sql.warehouse.dir", "c:/Temp") // <1>
  .getOrCreate
----
<1> Sets <<spark_sql_warehouse_dir, spark.sql.warehouse.dir>> for the Spark SQL session

.Spark SQL Properties (in alphabetical order)
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Default
| Description

| [[spark.sql.catalogImplementation]] `spark.sql.catalogImplementation`
| `in-memory`
a| (internal) Selects the active catalog implementation from:

* `in-memory`
* `hive`

TIP: Read link:spark-sql-ExternalCatalog.adoc[ExternalCatalog -- System Catalog of Permanent Entities].

TIP: You can enable Hive support in a `SparkSession` using link:spark-sql-SparkSession.adoc#enableHiveSupport[`enableHiveSupport` builder method].

| [[spark.sql.sources.default]] `spark.sql.sources.default`
| `parquet`
a| Defines the default data source to use for link:spark-sql-DataFrameReader.adoc[DataFrameReader].

Used when:

* Reading (link:spark-sql-DataFrameWriter.adoc[DataFrameWriter]) or writing (link:spark-sql-DataFrameReader.adoc[DataFrameReader]) datasets
* link:spark-sql-Catalog.adoc#createExternalTable[Creating external table from a path] (in `Catalog.createExternalTable`)

* Reading (`DataStreamReader`) or writing (`DataStreamWriter`) in Structured Streaming
|===

=== [[spark_sql_warehouse_dir]] spark.sql.warehouse.dir

`spark.sql.warehouse.dir` (default: `${system:user.dir}/spark-warehouse`) is the default location of Hive warehouse directory (using Derby) with managed databases and tables.

See also the official https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] document.

=== [[spark.sql.parquet.filterPushdown]] spark.sql.parquet.filterPushdown

`spark.sql.parquet.filterPushdown` (default: `true`) is a flag to control the link:spark-sql-Optimizer-PushDownPredicate.adoc[filter predicate push-down optimization] for data sources using parquet file format.

=== [[spark.sql.allowMultipleContexts]] spark.sql.allowMultipleContexts

`spark.sql.allowMultipleContexts` (default: `true`) controls whether creating multiple SQLContexts/HiveContexts is allowed.

=== [[spark.sql.columnNameOfCorruptRecord]] spark.sql.columnNameOfCorruptRecord

`spark.sql.columnNameOfCorruptRecord`...FIXME

=== [[spark.sql.dialect]] spark.sql.dialect

`spark.sql.dialect` - FIXME

=== [[spark.sql.streaming.checkpointLocation]] spark.sql.streaming.checkpointLocation

`spark.sql.streaming.checkpointLocation` is the default location for storing checkpoint data for link:spark-sql-streaming-StreamingQuery.adoc[continuously executing queries].
