== [[KafkaWriteTask]] KafkaWriteTask

`KafkaWriteTask` is used to <<execute, write rows>> (from a structured query) to Apache Kafka.

`KafkaWriteTask` is used exclusively when `KafkaWriter` is requested to link:spark-sql-KafkaWriter.adoc#write[write query results to Kafka] (and creates one per partition).

`KafkaWriteTask` <<execute, writes>> keys and values in their binary format (as JVM's bytes) and so uses the link:spark-sql-UnsafeRow.adoc[raw-memory unsafe row format] only (i.e. `UnsafeRow`). That is supposed to save time for reconstructing the rows to very tiny JVM objects (i.e. byte arrays).

[[internal-properties]]
.KafkaWriteTask's Internal Properties (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[projection]] `projection`
| link:spark-sql-UnsafeProjection.adoc[UnsafeProjection]

<<createProjection, Created>> once when `KafkaWriteTask` is created.
|===

=== [[execute]] Sending Rows to Kafka Asynchronously -- `execute` Method

[source, scala]
----
execute(iterator: Iterator[InternalRow]): Unit
----

`execute` uses Apache Kafka's Producer API to create a https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer] and https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[ProducerRecord] for every row in `iterator`, and sends the rows to Kafka in batches asynchronously.

Internally, `execute` creates a `KafkaProducer` using `Array[Byte]` for the keys and values, and `producerConfiguration` for the producer's configuration.

NOTE: `execute` creates a single `KafkaProducer` for all rows.

For every row in the `iterator`, `execute` uses the internal <<projection, UnsafeProjection>> to _project_ (aka _convert_) link:spark-sql-InternalRow.adoc[binary internal row format] to a link:spark-sql-UnsafeRow.adoc[UnsafeRow] object and take 0th, 1st and 2nd fields for a topic, key and value, respectively.

`execute` then creates a `ProducerRecord` and sends it to Kafka (using the `KafkaProducer`). `execute` registers a asynchronous `Callback` to monitor the writing.

[NOTE]
====
From https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer's documentation]:

> The `send()` method is asynchronous. When called it adds the record to a buffer of pending record sends and immediately returns. This allows the producer to batch together individual records for efficiency.
====

=== [[createProjection]] Creating UnsafeProjection -- `createProjection` Internal Method

[source, scala]
----
createProjection: UnsafeProjection
----

`createProjection` creates a link:spark-sql-UnsafeProjection.adoc[UnsafeProjection] with `topic`, `key` and `value` link:spark-sql-Expression.adoc[expressions] and the `inputSchema`.

`createProjection` makes sure that the following holds (and reports an `IllegalStateException` otherwise):

* `topic` was defined (either as the input `topic` or in `inputSchema`) and is of type `StringType`
* Optional `key` is of type `StringType` or `BinaryType` if defined
* `value` was defined (in `inputSchema`) and is of type `StringType` or `BinaryType`

`createProjection` casts `key` and `value` expressions to `BinaryType` in link:spark-sql-UnsafeProjection.adoc[UnsafeProjection].

NOTE: `createProjection` is used exclusively when `KafkaWriteTask` is created (as <<projection, projection>>).
