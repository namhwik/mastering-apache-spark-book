== [[Optimizer]] Catalyst Query Optimizer

CAUTION: FIXME Review https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala[sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala].

*Catalyst* is a Spark SQL framework for manipulating trees of relational operators and expressions in link:spark-sql-logical-plan.adoc[logical plans] before they end up as link:spark-sql-spark-plan.adoc[physical execution plans].

[source, scala]
----
scala> sql("select 1 + 1 + 1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias(((1 + 1) + 1), None)]
+- OneRowRelation$

== Analyzed Logical Plan ==
((1 + 1) + 1): int
Project [((1 + 1) + 1) AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Optimized Logical Plan ==
Project [3 AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Physical Plan ==
*Project [3 AS ((1 + 1) + 1)#4]
+- Scan OneRowRelation[]
----

Catalyst comes with an extensible *query plan optimizer* with link:spark-sql-catalyst-constant-folding.adoc[constant folding], link:spark-sql-catalyst-predicate-pushdown.adoc[predicate pushdown], link:spark-sql-catalyst-nullability-propagation.adoc[nullability (NULL value) propagation], link:spark-sql-catalyst-vectorized-parquet-decoder.adoc[vectorized Parquet decoder] and link:spark-sql-whole-stage-codegen.adoc[whole-stage code generation] optimization techniques.

Catalyst supports both rule-based and cost-based optimization.

Among the rule-based optimizations is link:spark-sql-catalyst-nullability-propagation.adoc[nullability (NULL value) propagation].

=== [[SparkOptimizer]] SparkOptimizer -- The Default Logical Query Plan Optimizer

`SparkOptimizer` is a custom <<Optimizer, logical query plan optimizer>> for a link:spark-sql-sessionstate.adoc#SessionCatalog[SessionCatalog] and link:spark-sql-SQLConf.adoc[SQLConf] with its own additional optimization batches:

1. *Optimize Metadata Only Query*
2. *Extract Python UDF from Aggregate*
3. *User Provided Optimizers* with the input `ExperimentalMethods`.

CAUTION: FIXME Review the batches. What is `ExperimentalMethods`?

`SparkOptimizer` is the default query optimizer that is available as link:spark-sql-sessionstate.adoc#optimizer[`optimizer` attribute of `SessionState`].

You can see the result of executing `SparkOptimizer` on a query plan using link:spark-sql-query-execution.adoc#optimizedPlan[`optimizedPlan` attribute of `QueryExecution`].

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[i-want-more]] Further reading or watching

* https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]
