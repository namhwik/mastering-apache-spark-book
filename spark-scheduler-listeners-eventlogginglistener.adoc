== Persisting Events using EventLoggingListener

`EventLoggingListener` is a link:spark-SparkListener.adoc[SparkListener] that <<logEvent, logs JSON-encoded events>> to a file.

When <<spark_eventLog_enabled, enabled>>, `EventLoggingListener` writes events to a log file under <<spark_eventLog_dir, spark.eventLog.dir>> directory. All link:spark-SparkListener.adoc[Spark events] are logged (except  link:spark-SparkListener.adoc#SparkListenerBlockUpdated[SparkListenerBlockUpdated] and link:spark-SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate]).

Events can optionally be <<compressing-events, compressed>>.

In-flight log files are with `.inprogress` extension.

TIP: Use link:spark-history-server.adoc[Spark History Server] to view the event logs in a browser.

`EventLoggingListener` is a `private[spark]` class in `org.apache.spark.scheduler` package.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.scheduler.EventLoggingListener` logger to see what happens inside `EventLoggingListener`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.EventLoggingListener=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating EventLoggingListener Instance

`EventLoggingListener` requires an application id (`appId`), the application's optional attempt id (`appAttemptId`), `logBaseDir`, a link:spark-configuration.adoc[SparkConf] (as `sparkConf`) and Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration] (as `hadoopConf`).

NOTE: When initialized with no Hadoop's `Configuration` it calls link:varia/spark-hadoop.adoc[SparkHadoopUtil.get.newConfiguration(sparkConf)].

=== [[start]] Starting EventLoggingListener -- `start` method

`start` checks whether `logBaseDir` is really a directory, and if it is not, it throws a `IllegalArgumentException` with the following message:

```
Log directory [logBaseDir] does not exist.
```

The log file's working name is created based on `appId` with or without the compression codec used and `appAttemptId`, i.e. `local-1461696754069`. It also uses `.inprogress` extension.

If <<spark_eventLog_overwrite, overwrite is enabled>>, you should see the WARN message:

```
WARN EventLoggingListener: Event log [path] already exists. Overwriting...
```

The working log `.inprogress` is attempted to be deleted. In case it could not be deleted, the following WARN message is printed out to the logs:

```
WARN EventLoggingListener: Error deleting [path]
```

The buffered output stream is created with metadata with Spark's version and `SparkListenerLogStart` class' name as the first line.

```
{"Event":"SparkListenerLogStart","Spark Version":"2.0.0-SNAPSHOT"}
```

At this point, `EventLoggingListener` is ready for event logging and you should see the following INFO message in the logs:

```
INFO EventLoggingListener: Logging events to [logPath]
```

=== [[logEvent]] Logging Event -- `logEvent` method

[source, scala]
----
logEvent(event: SparkListenerEvent, flushLogger: Boolean = false)
----

`logEvent` logs `event` as JSON using `org.apache.spark.util.JsonProtocol` object.

=== [[stop]] Stopping EventLoggingListener -- `stop` method

`stop` closes `PrintWriter` for the log file and renames the file to be without `.inprogress` extension.

If the target log file exists (one without `.inprogress` extension), it overwrites the file if <<spark_eventLog_overwrite, spark.eventLog.overwrite>> is enabled. You should see the following WARN message in the logs:

```
WARN EventLoggingListener: Event log [target] already exists. Overwriting...
```

If the target log file exists and overwrite is disabled, an `java.io.IOException` is thrown with the following message:

```
Target log file already exists ([logPath])
```

=== [[compressing-events]] Compressing Logged Events

If <<spark_eventLog_compress, event compression is enabled>>, `CompressionCodec.createCodec(sparkConf)` is called to create a compression codec using a short codec name or the fully-qualified class name of a codec.

TIP: Read link:spark-service-broadcastmanager.adoc#compression[Compression] to learn about the built-in compression codecs.

=== [[settings]] Settings

==== [[spark_eventLog_enabled]] spark.eventLog.enabled

`spark.eventLog.enabled` (default: `false`) - whether to log Spark events that encode the information displayed in the UI to persisted storage. It is useful for reconstructing the Web UI after a Spark application has finished.

==== [[spark_eventLog_dir]] spark.eventLog.dir

`spark.eventLog.dir` (default: `/tmp/spark-events`) - path to the directory in which events are logged, e.g. `hdfs://namenode:8021/directory`. The directory must exist before Spark starts up. See link:spark-sparkcontext.adoc#creating-instance[Creating a SparkContext].

==== [[spark_eventLog_buffer_kb]] spark.eventLog.buffer.kb

`spark.eventLog.buffer.kb` (default: `100`) - buffer size to use when writing to output streams.

==== [[spark_eventLog_overwrite]] spark.eventLog.overwrite

`spark.eventLog.overwrite` (default: `false`) - whether to delete or at least overwrite an existing `.inprogress` log file.

==== [[spark_eventLog_compress]] spark.eventLog.compress

`spark.eventLog.compress` (default: `false`) is the flag to enable compressing events.

See <<compressing-events, Compressing Events>>.

==== [[spark_eventLog_testing]] spark.eventLog.testing

`spark.eventLog.testing` (default: `false`) is an internal flag for testing purposes that enables adding JSON events to the internal `loggedEvents` array.
