== DAGScheduler

[NOTE]
====
The introduction that follows was highly influenced by the scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and only then read this and the related pages afterwards.

_"Reading the sources"_, I say?! Yes, I _am_ kidding!
====

=== Introduction

*DAGScheduler* is the scheduling layer of Apache Spark that implements *stage-oriented scheduling*, i.e. after an RDD action has been called it becomes a job that is then transformed into a set of stages that are submitted as TaskSets for execution (see link:spark-execution-model.adoc[Execution Model]).

.Executing action leads to new ResultStage and ActiveJob in DAGScheduler
image::images/dagscheduler-rdd-partitions-job-resultstage.png[align="center"]

The fundamental concepts of `DAGScheduler` are *jobs* and *stages* (refer to link:spark-dagscheduler-jobs.adoc[Jobs] and link:spark-dagscheduler-stages.adoc[Stages] respectively) that it tracks through <<internal-registries, internal registries and counters>>.

DAGScheduler works solely on the driver and is created as part of link:spark-sparkcontext.adoc#creating-instance[SparkContext's initialization] (right after link:spark-taskscheduler.adoc[TaskScheduler] and link:spark-scheduler-backends.adoc[SchedulerBackend] are ready).

.DAGScheduler as created by SparkContext with other services
image::images/dagscheduler-new-instance.png[align="center"]

DAGScheduler does three things in Spark (thorough explanations follow):

* Computes an *execution DAG*, i.e. DAG of stages, for a job.
* Determines the <<preferred-locations, preferred locations>> to run each task on.
* Handles failures due to *shuffle output files* being lost.

`DAGScheduler` computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to link:spark-taskscheduler.adoc[TaskScheduler].

In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to link:spark-taskscheduler.adoc[TaskScheduler].

Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage.

DAGScheduler uses an *event queue architecture* in which a thread can post `DAGSchedulerEvent` events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section <<event-loop, Internal Event Loop - dag-scheduler-event-loop>>.

DAGScheduler runs stages in topological order.

[TIP]
====
Enable `INFO`, `DEBUG` or `TRACE` logging levels for `org.apache.spark.scheduler.DAGSchedule` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.DAGScheduler=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

DAGScheduler needs link:spark-sparkcontext.adoc[SparkContext], link:spark-taskscheduler.adoc[Task Scheduler], link:spark-LiveListenerBus.adoc[LiveListenerBus], link:spark-service-mapoutputtracker.adoc[MapOutputTracker] and link:spark-blockmanager.adoc[Block Manager] to work. However, at the very minimum, DAGScheduler needs SparkContext only (and asks SparkContext for the other services).

DAGScheduler reports metrics about its execution (refer to the section <<metrics, Metrics>>).

When DAGScheduler schedules a job as a result of link:spark-rdd.adoc#actions[executing an action on a RDD] or link:spark-sparkcontext.adoc#runJob[calling SparkContext.runJob() method directly], it spawns parallel tasks to compute (partial) results per partition.

=== [[createResultStage]] `createResultStage` Internal Method

[source, scala]
----
createResultStage(
  rdd: RDD[_],
  func: (TaskContext, Iterator[_]) => _,
  partitions: Array[Int],
  jobId: Int,
  callSite: CallSite): ResultStage
----

CAUTION: FIXME

=== [[createShuffleMapStage]] `createShuffleMapStage` Method

CAUTION: FIXME

=== [[getOrCreateParentStages]] `getOrCreateParentStages` Internal Method

[source, scala]
----
getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage]
----

`getOrCreateParentStages` <<getShuffleDependencies, computes `ShuffleDependency` immediate parents>> and <<getOrCreateShuffleMapStage, gets or creates a shuffle map stage>> for each link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency].

NOTE: `getOrCreateParentStages` is used when `DAGScheduler` <<createShuffleMapStage, createShuffleMapStage>> and <<createResultStage, createResultStage>>.

=== [[getShuffleDependencies]] `getShuffleDependencies` Method

[source, scala]
----
getShuffleDependencies(
  rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]]
----

`getShuffleDependencies`...TK

CAUTION: FIXME

NOTE: `getShuffleDependencies` is used when `DAGScheduler` <<getOrCreateParentStages, getOrCreateParentStages>> and <<getMissingAncestorShuffleDependencies, getMissingAncestorShuffleDependencies>>.

=== [[getOrCreateShuffleMapStage]] `getOrCreateShuffleMapStage` Method

CAUTION: FIXME

=== [[getMissingAncestorShuffleDependencies]] `getMissingAncestorShuffleDependencies` Internal Method

CAUTION: FIXME

[source, scala]
----
getMissingAncestorShuffleDependencies(
  rdd: RDD[_]): Stack[ShuffleDependency[_, _, _]]
----

NOTE: `getMissingAncestorShuffleDependencies` is used when `DAGScheduler` <<getOrCreateShuffleMapStage, getOrCreateShuffleMapStage>>.

=== [[creating-instance]][[initialization]] Creating DAGScheduler Instance

[source, scala]
----
DAGScheduler(
  sc: SparkContext,
  taskScheduler: TaskScheduler,
  listenerBus: LiveListenerBus,
  mapOutputTracker: MapOutputTrackerMaster,
  blockManagerMaster: BlockManagerMaster,
  env: SparkEnv,
  clock: Clock = new SystemClock())
----

`DAGScheduler` requires a link:spark-sparkcontext.adoc[SparkContext], link:spark-taskscheduler.adoc[TaskScheduler], link:spark-LiveListenerBus.adoc[LiveListenerBus], link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster], link:spark-BlockManagerMaster.adoc[BlockManagerMaster], link:spark-sparkenv.adoc[SparkEnv], and a `Clock`.

NOTE: `DAGScheduler` can reference all the services through a single link:spark-sparkcontext.adoc[SparkContext].

When created, `DAGScheduler` does the following (in order):

1. Creates a `DAGSchedulerSource`
2. Creates <<messageScheduler, messageScheduler>>
3. Creates <<eventProcessLoop, eventProcessLoop>> and immediatelly link:spark-taskscheduler.adoc#setDAGScheduler[sets itself in the current `TaskScheduler`]
5. Initializes the <<internal-registries, internal registries and counters>>.

At the very end of the initialization, `DAGScheduler` starts <<eventProcessLoop, eventProcessLoop>>.

=== [[listenerBus]] `LiveListenerBus` Event Bus for ``SparkListenerEvent``s -- `listenerBus` Property

[source, scala]
----
listenerBus: LiveListenerBus
----

`listenerBus` is a link:spark-LiveListenerBus.adoc[LiveListenerBus] to post scheduling events and is passed in when <<creating-instance, `DAGScheduler` is created>>.

=== [[internal-registries]] Internal Registries and Counters

`DAGScheduler` uses internal registries and counters for managing active jobs and stages.

.`DAGScheduler` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Name | Description
| [[nextJobId]] `nextJobId` | The next job id counting from `0`.

Used when `DAGScheduler` <<submitJob, submits a job>> and <<submitMapStage, a map stage>>, and <<runApproximateJob, runs an approximate job>>.

| [[nextStageId]] `nextStageId` | The next stage id counting from `0`.

Used when `DAGScheduler` creates a <<createShuffleMapStage, shuffle map stage>> and a <<createResultStage, result stage>>. It is the key in <<stageIdToStage, stageIdToStage>>.

| [[stageIdToStage]] `stageIdToStage` | The lookup table for stages per their ids.

Used when `DAGScheduler` <<createShuffleMapStage, creates a shuffle map stage>>, <<createResultStage, creates a result stage>>, <<cleanupStateForJobAndIndependentStages, cleans up job state and independent stages>>, is informed that link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleBeginEvent[a task is started], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[a taskset has failed], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[a job is submitted (to compute a `ResultStage`)], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[a map stage was submitted], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[a task has completed] or link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleStageCancellation[a stage was cancelled], <<updateAccumulators, updates accumulators>>, <<abortStage, aborts a stage>> and <<failJobAndIndependentStages, fails a job and independent stages>>.

| [[jobIdToStageIds]] `jobIdToStageIds` | The lookup table of all stages per `ActiveJob` id

| [[shuffleIdToMapStage]] `shuffleIdToMapStage` | The lookup table of link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] per shuffle id
| [[jobIdToActiveJob]] `jobIdToActiveJob` | The lookup table of ``ActiveJob``s per job id.
| [[waitingStages]] `waitingStages` | The stages with parents to be computed
| [[runningStages]] `runningStages` | The stages currently running.

| [[failedStages]] `failedStages` | The collection of the stages that failed due to fetch failures (as reported by <<handleTaskCompletion-FetchFailed, CompletionEvents for FetchFailed end reasons>>).


| [[activeJobs]] `activeJobs` | A collection of `ActiveJob` instances
| [[cacheLocs]] `cacheLocs` | The internal cache of partition locations per RDD. Refer to <<cache-tracking, Cache Tracking>>.
| [[failedEpoch]] `failedEpoch` | The lookup table of lost executors and the epoch of the event.
|======================

=== [[runApproximateJob]] `runApproximateJob` Method

CAUTION: FIXME

=== [[executorHeartbeatReceived]] `executorHeartbeatReceived` Method

[source, scala]
----
executorHeartbeatReceived(
  execId: String,
  accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])],
  blockManagerId: BlockManagerId): Boolean
----

`executorHeartbeatReceived` posts a link:spark-SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate] (to <<listenerBus, listenerBus>>) and informs link:spark-BlockManagerMaster.adoc[BlockManagerMaster] that `blockManagerId` block manager is alive (by posting link:spark-BlockManagerMaster.adoc#BlockManagerHeartbeat[BlockManagerHeartbeat]).

NOTE: `executorHeartbeatReceived` is called when link:spark-taskschedulerimpl.adoc#executorHeartbeatReceived[`TaskSchedulerImpl` handles `executorHeartbeatReceived`].

=== [[cleanupStateForJobAndIndependentStages]] Cleaning Up Job State and Independent Stages -- `cleanupStateForJobAndIndependentStages` Method

[source, scala]
----
cleanupStateForJobAndIndependentStages(job: ActiveJob): Unit
----

`cleanupStateForJobAndIndependentStages` cleans up the state for `job` and any stages that are _not_ part of any other job.

`cleanupStateForJobAndIndependentStages` looks the `job` up in the internal <<jobIdToStageIds, jobIdToStageIds>> registry.

If no stages are found, the following ERROR is printed out to the logs:

```
ERROR No stages registered for job [jobId]
```

Oterwise, `cleanupStateForJobAndIndependentStages` uses <<stageIdToStage, stageIdToStage>> registry to find the stages (the real objects not ids!).

For each stage, `cleanupStateForJobAndIndependentStages` reads the jobs the stage belongs to.

If the `job` does not belong to the jobs of the stage, the following ERROR is printed out to the logs:

```
ERROR Job [jobId] not registered for stage [stageId] even though that stage was registered for the job
```

If the `job` was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. <<runningStages, runningStages>>, <<shuffleIdToMapStage, shuffleIdToMapStage>>, <<waitingStages, waitingStages>>, <<failedStages, failedStages>> and <<stageIdToStage, stageIdToStage>>.

While removing from <<runningStages, runningStages>>, you should see the following DEBUG message in the logs:

```
DEBUG Removing running stage [stageId]
```

While removing from <<waitingStages, waitingStages>>, you should see the following DEBUG message in the logs:

```
DEBUG Removing stage [stageId] from waiting set.
```

While removing from <<failedStages, failedStages>>, you should see the following DEBUG message in the logs:

```
DEBUG Removing stage [stageId] from failed set.
```

After all cleaning (using <<stageIdToStage, stageIdToStage>> as the source registry), if the stage belonged to the one and only `job`, you should see the following DEBUG message in the logs:

```
DEBUG After removal of stage [stageId], remaining stages = [stageIdToStage.size]
```

The `job` is removed from <<jobIdToStageIds, jobIdToStageIds>>, <<jobIdToActiveJob, jobIdToActiveJob>>, <<activeJobs, activeJobs>> registries.

The final stage of the `job` is removed, i.e. link:spark-dagscheduler-ResultStage.adoc#removeActiveJob[ResultStage] or link:spark-dagscheduler-ShuffleMapStage.adoc#removeActiveJob[ShuffleMapStage].

NOTE: `cleanupStateForJobAndIndependentStages` is used in link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ResultTask[`handleTaskCompletion` when a `ResultTask` has completed successfully], <<failJobAndIndependentStages, failJobAndIndependentStages>> and <<markMapStageJobAsFinished, markMapStageJobAsFinished>>.

=== [[markMapStageJobAsFinished]] Marking MapStage Job Finished -- `markMapStageJobAsFinished` Method

[source, scala]
----
markMapStageJobAsFinished(job: ActiveJob, stats: MapOutputStatistics): Unit
----

`markMapStageJobAsFinished` marks map stage jobs finished and notifies Spark listeners.

Internally, `markMapStageJobAsFinished` marks the zeroth partition finished and increases the number of tasks finished in `job`.

The link:spark-dagscheduler-JobListener.adoc#taskSucceeded[`job` listener is notified about the 0th task succeeded].

The <<cleanupStateForJobAndIndependentStages, state of the `job` and independent stages are cleaned up>>.

Ultimately, link:spark-SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to link:spark-LiveListenerBus.adoc[LiveListenerBus] (as <<listenerBus, listenerBus>>) for the `job`, the current time (in millis) and `JobSucceeded` job result.

NOTE: `markMapStageJobAsFinished` is used in link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[handleMapStageSubmitted] and link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc##handleTaskCompletion[handleTaskCompletion].

=== [[clearCacheLocs]] Clearing Cache of RDD Partition Locations -- `clearCacheLocs` Internal Method

[source, scala]
----
clearCacheLocs(): Unit
----

`clearCacheLocs` clears the <<cacheLocs, internal cache of the partition locations of all RDDs>>.

NOTE: `DAGScheduler` clears the cache while link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmitting failed stages], and as a result of link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#JobSubmitted[JobSubmitted], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#MapStageSubmitted[MapStageSubmitted], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#CompletionEvent[CompletionEvent], link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#ExecutorLost[ExecutorLost] events.

=== [[failJobAndIndependentStages]] Failing Job and Single-Job Stages -- `failJobAndIndependentStages` Internal Method

[source, scala]
----
failJobAndIndependentStages(
  job: ActiveJob,
  failureReason: String,
  exception: Option[Throwable] = None): Unit
----

The internal `failJobAndIndependentStages` method fails the input `job` and all the stages that are only used by the job.

Internally, `failJobAndIndependentStages` uses <<jobIdToStageIds, `jobIdToStageIds` internal registry>> to look up the stages registered for the job.

If no stages could be found, you should see the following ERROR message in the logs:

```
ERROR No stages registered for job [id]
```

Otherwise, for every stage, `failJobAndIndependentStages` finds the job ids the stage belongs to.

If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs:

```
ERROR Job [id] not registered for stage [id] even though that stage was registered for the job
```

Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in `runningStages` internal registry), link:spark-taskscheduler.adoc#contract[`TaskScheduler` is requested to cancel the stage's tasks] and <<markStageAsFinished, marks the stage finished>>.

NOTE: `failJobAndIndependentStages` is called from link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleJobCancellation[handleJobCancellation] and `abortStage`.

NOTE: `failJobAndIndependentStages` uses <<jobIdToStageIds, jobIdToStageIds>>, <<stageIdToStage, stageIdToStage>>, and <<runningStages, runningStages>> internal registries.

=== [[submitJob]] Posting `JobSubmitted` Event -- `submitJob` method

[source, scala]
----
submitJob[T, U](
  rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int],
  callSite: CallSite,
  resultHandler: (Int, U) => Unit,
  properties: Properties): JobWaiter[U]
----

`submitJob` creates a link:spark-dagscheduler-JobListener.adoc#JobWaiter[JobWaiter] and posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#JobSubmitted[`JobSubmitted` event].

.DAGScheduler.submitJob
image::images/dagscheduler-submitjob.png[align="center"]

Internally, `submitJob` does the following:

1. Checks whether `partitions` reference available partitions of the input `rdd`.
2. Increments <<nextJobId, nextJobId>> internal job counter.
3. Returns a 0-task link:spark-dagscheduler-JobListener.adoc#JobWaiter[JobWaiter] when the number of `partitions` is zero.
4. Posts a `JobSubmitted` event and returns a `JobWaiter`.

You may see a `IllegalArgumentException` thrown when the input `partitions` references partitions not in the input `rdd`:

```
Attempting to access a non-existent partition: [p]. Total number of partitions: [maxPartitions]
```

NOTE: `submitJob` is called when link:spark-sparkcontext.adoc#submitJob[`SparkContext` submits a job] and <<runJob, `DAGScheduler` runs a job>>.

NOTE: `submitJob` assumes that the partitions of a RDD are indexed from 0 onwards in sequential order.

=== [[cancelStage]] Posting `StageCancelled` Event -- `cancelStage` Method

[source, scala]
----
cancelStage(stageId: Int)
----

`cancelJobGroup` merely posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#StageCancelled[StageCancelled] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: `cancelStage` is executed when a link:spark-sparkcontext.adoc#cancelStage[`SparkContext` is requested to cancel a stage].

=== [[cancelJobGroup]] Posting `JobGroupCancelled` Event -- `cancelJobGroup` Method

[source, scala]
----
cancelJobGroup(groupId: String): Unit
----

`cancelJobGroup` prints the following INFO message to the logs followed by posting a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#JobGroupCancelled[JobGroupCancelled] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

```
INFO Asked to cancel job group [groupId]
```

NOTE: `cancelJobGroup` is executed when a link:spark-sparkcontext.adoc#cancelJobGroup[`SparkContext` is requested to cancel a specified group of jobs].

=== [[cancelAllJobs]] Posting `AllJobsCancelled` Event -- `cancelAllJobs` Method

[source, scala]
----
cancelAllJobs(): Unit
----

`cancelAllJobs` merely posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#AllJobsCancelled[AllJobsCancelled] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: `cancelAllJobs` is executed when a link:spark-sparkcontext.adoc#cancelAllJobs[`SparkContext` is requested to cancel all running and scheduled Spark jobs].

=== [[taskStarted]] Posting `BeginEvent` Event -- `taskStarted` Method

[source, scala]
----
taskStarted(task: Task[_], taskInfo: TaskInfo)
----

`taskStarted` merely posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#BeginEvent[BeginEvent] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: `taskStarted` is executed when a link:spark-tasksetmanager.adoc#resourceOffer[`TaskSetManager` starts a task].

=== [[taskGettingResult]] Posting `GettingResultEvent` Event -- `taskGettingResult` Method

[source, scala]
----
taskGettingResult(taskInfo: TaskInfo)
----

`taskGettingResult` merely posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#GettingResultEvent[GettingResultEvent] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: `taskGettingResult` is executed when a link:spark-tasksetmanager.adoc#handleTaskGettingResult[`TaskSetManager` gets notified about a task fetching result].

=== [[taskEnded]] Posting `CompletionEvent` Event -- `taskEnded` Method

[source, scala]
----
taskEnded(
  task: Task[_],
  reason: TaskEndReason,
  result: Any,
  accumUpdates: Map[Long, Any],
  taskInfo: TaskInfo,
  taskMetrics: TaskMetrics): Unit
----

`taskEnded` merely posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#CompletionEvent[CompletionEvent] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: `taskEnded` is called when a `TaskSetManager` reports task completions, i.e. successes or link:spark-tasksetmanager.adoc#handleFailedTask[failures].

TIP: Read about `TaskMetrics` in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

=== [[submitMapStage]] Posting `MapStageSubmitted` Event -- `submitMapStage` Method

[source, scala]
----
submitMapStage[K, V, C](
  dependency: ShuffleDependency[K, V, C],
  callback: MapOutputStatistics => Unit,
  callSite: CallSite,
  properties: Properties): JobWaiter[MapOutputStatistics]
----

`submitMapStage` posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#MapStageSubmitted[MapStageSubmitted] event to the <<eventProcessLoop, DAGScheduler's Internal Event Bus>> and returns the link:spark-dagscheduler-JobListener.adoc#JobWaiter[JobWaiter] with one task only and a result handler that will call the `callback` function.

`submitMapStage` increments <<nextJobId, nextJobId>> for the job id.

NOTE: `submitMapStage` is used when link:spark-sparkcontext.adoc#submitMapStage[`SparkContext` submits a map stage for execution].

=== [[taskSetFailed]] Posting `TaskSetFailed` Event -- `taskSetFailed` Method

[source, scala]
----
taskSetFailed(
  taskSet: TaskSet,
  reason: String,
  exception: Option[Throwable]): Unit
----

`taskSetFailed` simply posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#TaskSetFailed[TaskSetFailed] to <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

NOTE: The input arguments of `taskSetFailed` are exactly the arguments of link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#TaskSetFailed[TaskSetFailed].

NOTE: `taskSetFailed` is executed when a link:spark-tasksetmanager.adoc#abort[`TaskSetManager` is aborted].

=== [[executorLost]] Posting `ExecutorLost` Event -- `executorLost` Method

[source, scala]
----
executorLost(execId: String, reason: ExecutorLossReason): Unit
----

`executorLost` simply posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#ExecutorLost[ExecutorLost] event to <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

=== [[executorAdded]] Posting `ExecutorAdded` Event -- `executorAdded` Method

[source, scala]
----
executorAdded(execId: String, host: String): Unit
----

`executorAdded` simply posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#ExecutorAdded[ExecutorAdded] event to <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

=== [[cancelJob]] Posting `JobCancelled` Event -- `cancelJob` Method

[source, scala]
----
cancelJob(jobId: Int): Unit
----

`cancelJob` prints the following INFO message and posts a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#JobCancelled[JobCancelled] to <<eventProcessLoop, DAGScheduler's Internal Event Bus>>.

```
INFO DAGScheduler: Asked to cancel job [id]
```

NOTE: `cancelJob` is called when link:spark-sparkcontext.adoc#cancelJob[SparkContext] and link:spark-dagscheduler-JobListener.adoc#JobWaiter[JobWaiter] are requested to cancel a Spark job.

=== [[messageScheduler]] `messageScheduler` Single-Thread Executor

CAUTION: FIXME

=== [[runJob]] Submitting Action Job -- `runJob` Method

[source, scala]
----
runJob[T, U](
  rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int],
  callSite: CallSite,
  resultHandler: (Int, U) => Unit,
  properties: Properties): Unit
----

`runJob` submits an action job to the `DAGScheduler` and waits for a result.

Internally, `runJob` executes <<submitJob, submitJob>> and then waits until a result comes using link:spark-dagscheduler-JobListener.adoc#JobWaiter[JobWaiter].

When the job succeeds, you should see the following INFO message in the logs:

```
INFO Job [jobId] finished: [callSite], took [time] s
```

When the job fails, you should see the following INFO message in the logs and the exception (that led to the failure) is thrown.

```
INFO Job [jobId] failed: [callSite], took [time] s
```

NOTE: `runJob` is used when link:spark-sparkcontext.adoc#runJob[`SparkContext` runs a job].

=== [[abortStage]] Aborting Stage -- `abortStage` Internal Method

[source, scala]
----
abortStage(
  failedStage: Stage,
  reason: String,
  exception: Option[Throwable]): Unit
----

`abortStage` is an internal method that finds all the active jobs that depend on the `failedStage` stage and fails them.

Internally, `abortStage` looks the `failedStage` stage up in the internal <<stageIdToStage, stageIdToStage>> registry and exits if there the stage was not registered earlier.

If it was, `abortStage` finds all the active jobs (in the internal <<activeJobs, activeJobs>> registry) with the <<stageDependsOn, final stage depending on the `failedStage` stage>>.

At this time, the `completionTime` property (of the failed stage's `StageInfo`) is assigned to the current time (millis).

All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka _independent stages_) are <<failJobAndIndependentStages, failed>> (with the failure reason being "Job aborted due to stage failure: [reason]" and the input `exception`).

If there are no jobs depending on the failed stage, you should see the following INFO message in the logs:

```
INFO Ignoring failure of [failedStage] because all jobs depending on it are done
```

NOTE: `abortStage` is used to link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[handle `TaskSetFailed` event], when <<submitStage, submitting a stage with no active job>>

=== [[stageDependsOn]] Checking If Stage Depends on Another Stage -- `stageDependsOn` Method

[source, scala]
----
stageDependsOn(stage: Stage, target: Stage): Boolean
----

`stageDependsOn` compares two stages and returns whether the `stage` depends on `target` stage (i.e. `true`) or not (i.e. `false`).

NOTE: A stage `A` depends on stage `B` if `B` is among the ancestors of `A`.

Internally, `stageDependsOn` walks through the graph of RDDs of the input `stage`. For every RDD in the RDD's dependencies (using `RDD.dependencies`) `stageDependsOn` adds the RDD of a `NarrowDependency` to a stack of RDDs to visit while for a `ShuffleDependency` it <<getOrCreateShuffleMapStage, getOrCreateShuffleMapStage>> for the dependency and the ``stage``'s first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs.

After all the RDDs of the input `stage` are visited, `stageDependsOn` checks if the ``target``'s RDD is among the RDDs of the `stage`, i.e. whether the `stage` depends on `target` stage.

=== [[markStageAsFinished]] Marking Stage Finished -- `markStageAsFinished` Internal Method

[source, scala]
----
markStageAsFinished(stage: Stage, errorMessage: Option[String] = None): Unit
----

CAUTION: FIXME

=== [[event-loop]][[eventProcessLoop]] dag-scheduler-event-loop -- DAGScheduler's Internal Event Bus

`eventProcessLoop` is link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc[DAGScheduler's event bus] to which Spark (by <<submitJob, submitJob>>) posts jobs to schedule their execution. Later on, link:spark-tasksetmanager.adoc[TaskSetManager] talks back to `DAGScheduler` to inform about the status of the tasks using the same "communication channel".

It allows Spark to release the current thread when posting happens and let the event loop handle events on a separate thread - asynchronously.

...IMAGE...FIXME

CAUTION: FIXME statistics? `MapOutputStatistics`?

=== [[submitWaitingStages]] Submitting Waiting Stages for Execution -- `submitWaitingStages` Method

[source, scala]
----
submitWaitingChildStages(parent: Stage): Unit
----

`submitWaitingStages` method checks for waiting or failed stages that could now be eligible for submission.

When executed, you should see the following `TRACE` messages in the logs:

```
TRACE DAGScheduler: Checking for newly runnable parent stages
TRACE DAGScheduler: running: [runningStages]
TRACE DAGScheduler: waiting: [waitingStages]
TRACE DAGScheduler: failed: [failedStages]
```

The method clears the internal `waitingStages` set with stages that wait for their parent stages to finish.

It goes over the waiting stages sorted by job ids in increasing order and calls <<submitStage, submitStage>> method.

=== [[submitStage]] Submitting Stage for Execution -- `submitStage` Internal Method

[source, scala]
----
submitStage(stage: Stage)
----

`submitStage` is an internal method that `DAGScheduler` uses to submit the input `stage` or its missing parents (if there any).

NOTE: `submitStage` is also used to link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmit failed stages].

`submitStage` recursively submits any missing parents of the stage.

Internally, `submitStage` first finds the earliest-created `ActiveJob` that needs the `stage`.

You should see the following DEBUG message in the logs:

```
DEBUG DAGScheduler: submitStage([stage])
```

Only when the `stage` is not in waiting (`waitingStages`), running (`runningStages`) or <<failedStages, failed>> states `submitStage` proceeds.

The <<getMissingParentStages, list of missing parent stages of the `stage` is calculated>> (sorted by their ids) and the following DEBUG message shows up in the logs:

```
DEBUG DAGScheduler: missing: [missing]
```

When the `stage` has no parent stages missing, you should see the following INFO message in the logs:

```
INFO DAGScheduler: Submitting [stage] ([stage.rdd]), which has no missing parents
```

The stage is <<submitMissingTasks, submitted>>. That finishes the stage submission.

If however there are missing parent stages for the `stage`, all parent stages are <<submitStage, submitted>> (by id in increasing order), and the `stage` is added to `waitingStages` stages.

In case when `submitStage` could find no active job for the `stage`, it <<abortStage, aborts the stage>> with the reason:

```
No active job for stage [id]
```

=== [[getMissingParentStages]][[calculating-missing-parent-stages]] Calculating Missing Parent Map Stages -- `getMissingParentStages` Internal Method

[source, scala]
----
getMissingParentStages(stage: Stage): List[Stage]
----

`getMissingParentStages` calculates missing parent map stages for the input `stage`.

It starts with the stage's target RDD (as `stage.rdd`). If there are <<cache-tracking, uncached partitions>>, it traverses the dependencies of the RDD (as `RDD.dependencies`) that can be the instances of link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency] or link:spark-rdd-dependencies.adoc#NarrowDependency[NarrowDependency].

For each ShuffleDependency, the method searches for the corresponding link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] (using `getShuffleMapStage`) and if unavailable, the method adds it to a set of missing (map) stages.

CAUTION: FIXME Review `getShuffleMapStage`

CAUTION: FIXME...IMAGE with ShuffleDependencies queried

It continues traversing the chain for each NarrowDependency (using `Dependency.rdd`).

=== [[stage-attempts]] Fault recovery - stage attempts

A single stage can be re-executed in multiple *attempts* due to fault recovery. The number of attempts is configured (FIXME).

If `TaskScheduler` reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits that lost stage. This is detected through a `CompletionEvent` with `FetchFailed`, or an <<ExecutorLost, ExecutorLost>> event. `DAGScheduler` will wait a small amount of time to see whether other nodes or tasks fail, then resubmit `TaskSets` for any lost stage(s) that compute the missing tasks.

Please note that tasks from the old attempts of a stage could still be running.

A stage object tracks multiple `StageInfo` objects to pass to Spark listeners or the web UI.

The latest `StageInfo` for the most recent attempt for a stage is accessible through `latestInfo`.

=== [[cache-tracking]] Cache Tracking

DAGScheduler tracks which RDDs are cached to avoid recomputing them and likewise remembers which shuffle map stages have already produced output files to avoid redoing the map side of a shuffle.

DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of an RDD.

CAUTION: FIXME: A diagram, please

If link:spark-rdd-caching.adoc[the storage level of an RDD is NONE], there is no caching and hence no partition cache locations are available. In such cases, whenever asked, DAGScheduler returns a collection with empty-location elements for each partition. The empty-location elements are to mark *uncached partitions*.

Otherwise, a collection of `RDDBlockId` instances for each partition is created and spark-BlockManagerMaster.adoc[BlockManagerMaster] is asked for locations (using `BlockManagerMaster.getLocations`). The result is then mapped to a collection of `TaskLocation` for host and executor id.

=== [[preferred-locations]] Preferred Locations

DAGScheduler computes where to run each task in a stage based on link:spark-rdd.adoc#preferred-locations[the preferred locations of its underlying RDDs], or <<cache-tracking, the location of cached or shuffle data>>.

=== [[adaptive-query-planning]] Adaptive Query Planning

See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages.

=== ScheduledExecutorService daemon services

DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation):

* `dag-scheduler-message` - a daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`. It is used to post a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#ResubmitFailedStages[ResubmitFailedStages] event when `FetchFailed` is reported.

They are created using `ThreadUtils.newDaemonSingleThreadScheduledExecutor` method that uses Guava DSL to instantiate a ThreadFactory.

=== [[submitMissingTasks]] Submitting Missing Tasks for Stage and Job -- `submitMissingTasks` Internal Method

[source, scala]
----
submitMissingTasks(stage: Stage, jobId: Int): Unit
----

`submitMissingTasks` is a private method that...FIXME

When executed, it prints the following DEBUG message out to the logs:

```
DEBUG DAGScheduler: submitMissingTasks([stage])
```

The stage's `pendingPartitions` internal field is cleared (it is later filled out with the partitions to run tasks for).

The stage is asked for partitions to compute (see link:spark-dagscheduler-stages.adoc#findMissingPartitions[findMissingPartitions] in Stages).

The method adds the stage to <<runningStages, runningStages>> internal registry.

The stage is told to be started to link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] (using `outputCommitCoordinator.stageStart`)

CAUTION: FIXME Review `outputCommitCoordinator.stageStart`

The mapping between task ids and task preferred locations is computed (see <<computing-preferred-locations, getPreferredLocs - Computing Preferred Locations for Tasks and Partitions>>).

A new stage attempt is created (using `Stage.makeNewStageAttempt`).

link:spark-SparkListener.adoc#SparkListenerStageSubmitted[SparkListenerStageSubmitted] is posted.

The stage is serialized and broadcast to workers using link:spark-sparkcontext.adoc#creating-broadcast-variables[SparkContext.broadcast] method, i.e. it is `Serializer.serialize` to calculate `taskBinaryBytes` - an array of bytes of (rdd, func) for link:spark-dagscheduler-ResultStage.adoc[ResultStage] and (rdd, shuffleDep) for link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage].

CAUTION: FIXME Review `taskBinaryBytes`.

When serializing the stage fails, the stage is removed from <<runningStages, runningStages>> internal registry, the <<abortStage, stage is aborted>> and the method stops.

At this point in time, the stage is on workers.

For each partition to compute for the stage, a collection of <<spark-taskscheduler.adoc#shufflemaptask, ShuffleMapTask>> for link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] or
`ResultTask` for link:spark-dagscheduler-ResultStage.adoc[ResultStage] is created.

CAUTION: FIXME Image with creating tasks for partitions in the stage.

Any issue with creating a task leads to <<abortStage, aborting the stage>> and removing the `stage` from <<runningStages, runningStages>> internal registry.

If there are tasks to launch (there are missing partitions in the stage), the following INFO and DEBUG messages are in the logs:

```
INFO DAGScheduler: Submitting [tasks.size] missing tasks from [stage] ([stage.rdd])
DEBUG DAGScheduler: New pending partitions: [stage.pendingPartitions]
```

All tasks in the collection become a link:spark-taskscheduler-tasksets.adoc[TaskSet] for link:spark-taskscheduler.adoc#contract[TaskScheduler.submitTasks].

In case of no tasks to be submitted for a stage, a DEBUG message shows up in the logs.

For link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage]:

```
DEBUG DAGScheduler: Stage [stage] is actually done; (available: [stage.isAvailable],available outputs: [stage.numAvailableOutputs],partitions: [stage.numPartitions])
```

For link:spark-dagscheduler-ResultStage.adoc[ResultStage]:

```
DEBUG DAGScheduler: Stage [stage] is actually done; (partitions: [numPartitions])
```

NOTE: `submitMissingTasks` is called when...

=== [[getPreferredLocs]][[computing-preferred-locations]] Computing Preferred Locations for Tasks and Partitions -- `getPreferredLocs` Method

[source, scala]
----
getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation]
----

CAUTION: FIXME Review + why does the method return a sequence of TaskLocations?

NOTE: Task ids correspond to partition ids.

=== [[stop]][[stopping]] Stopping `DAGScheduler` -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the internal `dag-scheduler-message` thread pool, <<event-loop, dag-scheduler-event-loop>>, and link:spark-taskscheduler.adoc[TaskScheduler].

=== [[metrics]] Metrics

Spark's DAGScheduler uses link:spark-metrics.adoc[Spark Metrics System] (via `DAGSchedulerSource`) to report metrics about internal status.

CAUTION: FIXME What is `DAGSchedulerSource`?

The name of the source is *DAGScheduler*.

It emits the following numbers:

* *stage.failedStages* - the number of failed stages
* *stage.runningStages* - the number of running stages
* *stage.waitingStages* - the number of waiting stages
* *job.allJobs* - the number of all jobs
* *job.activeJobs* - the number of active jobs

=== [[updateAccumulators]] Updating Accumulators with Partial Values from Completed Tasks -- `updateAccumulators` Internal Method

[source, scala]
----
updateAccumulators(event: CompletionEvent): Unit
----

The private `updateAccumulators` method merges the partial values of accumulators from a completed task into their "source" accumulators on the driver.

NOTE: It is called by <<handleTaskCompletion, handleTaskCompletion>>.

For each link:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] in the `CompletionEvent`, a partial value from a task is obtained (from `AccumulableInfo.update`) and added to the driver's accumulator (using `Accumulable.++=` method).

For named accumulators with the update value being a non-zero value, i.e. not `Accumulable.zero`:

* `stage.latestInfo.accumulables` for the `AccumulableInfo.id` is set
* `CompletionEvent.taskInfo.accumulables` has a new link:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] added.

CAUTION: FIXME Where are `Stage.latestInfo.accumulables` and `CompletionEvent.taskInfo.accumulables` used?

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property | Default Value | Description
| [[spark_test_noStageRetry]] `spark.test.noStageRetry` | `false` | When enabled (i.e. `true`), FetchFailed will not cause stage retries, in order to surface the problem. Used for testing.
|======================
