== SessionState

`SessionState` is the <<sessionState, default separation layer>> for isolating state across sessions, including SQL configuration, tables, functions, UDFs, the link:spark-sql-sql-parsers.adoc#SparkSqlParser[SQL parser], and everything else that depends on a link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME Elaborate please.

It requires a link:spark-sql-sparksession.adoc[SparkSession] and manages its own link:spark-sql-SQLConf.adoc[SQLConf].

NOTE: Given the package `org.apache.spark.sql.internal` that `SessionState` belongs to, this one is truly _internal_. You've been warned.

NOTE: `SessionState` is a `private[sql]` class.

`SessionState` offers the following services:

* <<optimizer, optimizer>>
* <<analyzer, analyzer>>
* <<catalog, catalog>>
* <<streamingQueryManager, streamingQueryManager>>
* <<udf, udf>>
* <<newHadoopConf, newHadoopConf>> to create a new Hadoop's `Configuration`.
* link:spark-sql-sparksession.adoc#sessionState[sessionState]
* link:spark-sql-sql-parsers.adoc#SparkSqlParser[sqlParser]

=== [[catalog]] catalog Attribute

[source, scala]
----
catalog: SessionCatalog
----

`catalog` attribute points at shared internal <<SessionCatalog, SessionCatalog>> for managing tables and databases.

It is used to create the shared <<analyzer, analyzer>>, <<optimizer, optimizer>>

=== [[SessionCatalog]] SessionCatalog

`SessionCatalog` is a proxy between link:spark-sql-sparksession.adoc[SparkSession] and the underlying metastore, e.g. `HiveSessionCatalog`.

=== [[analyzer]] analyzer Attribute

[source, scala]
----
analyzer: Analyzer
----

`analyzer` is...

=== [[optimizer]] optimizer Attribute

[source, scala]
----
optimizer: Optimizer
----

`optimizer` is an optimizer for link:spark-sql-logical-plan.adoc[logical query plans].

It is (lazily) set to link:link:spark-sql-catalyst.adoc#SparkOptimizer[SparkOptimizer] that is a specialization of link:spark-sql-catalyst.adoc[Catalyst optimizer]. It is created for the session-owned <<catalog, SessionCatalog>>, link:spark-sql-SQLConf.adoc[SQLConf], and <<experimentalMethods, ExperimentalMethods>>.

=== [[experimentalMethods]] experimentalMethods

`experimentalMethods` is...

=== [[sqlParser]] sqlParser Attribute

`sqlParser` is...

=== [[planner]] planner method

`planner` is...

=== [[executePlan]] executePlan method

[source, scala]
----
executePlan(plan: LogicalPlan): QueryExecution
----

`executePlan` executes the input link:spark-sql-logical-plan.adoc[LogicalPlan] to produce a link:spark-sql-query-execution.adoc[QueryExecution] in the current link:spark-sql-sparksession.adoc[SparkSession].

=== [[refreshTable]] refreshTable method

`refreshTable` is...

=== [[addJar]] addJar method

`addJar` is...

=== [[analyze]] analyze method

`analyze` is...

=== [[streamingQueryManager]] streamingQueryManager Attribute

[source, scala]
----
streamingQueryManager: StreamingQueryManager
----

`streamingQueryManager` attribute points at shared link:spark-sql-StreamingQueryManager.adoc[StreamingQueryManager] (e.g. to link:spark-sql-streaming-DataStreamWriter.adoc#start[start streaming queries in `DataStreamWriter`]).

=== [[udf]] udf Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute points at shared `UDFRegistration` for a given Spark session.

=== [[newHadoopConf]] Creating New Hadoop Configuration (newHadoopConf method)

[source, scala]
----
newHadoopConf(): Configuration
----

`newHadoopConf` returns Hadoop's `Configuration` that it builds using link:spark-sparkcontext.adoc#hadoopConfiguration[SparkContext.hadoopConfiguration] (through link:spark-sql-sparksession.adoc[SparkSession]) with all configuration settings added.

NOTE: `newHadoopConf` is used by link:spark-sql-queryplanner.adoc#HiveSessionState[HiveSessionState] (for `HiveSessionCatalog`), `ScriptTransformation`, `ParquetRelation`, `StateStoreRDD`, and `SessionState` itself, and few other places.

CAUTION: FIXME What is `ScriptTransformation`? `StateStoreRDD`?
