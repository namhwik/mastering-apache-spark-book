== Settings

The following list are the settings used to configure Spark Streaming applications.

CAUTION: FIXME Describe how to set them in streaming applications.

* `spark.streaming.kafka.maxRetries` (default: `1`) sets up the number of connection attempts to Kafka brokers.

* `spark.streaming.receiver.writeAheadLog.enable` (default: `false`) controls what link:spark-streaming-receivedblockhandlers.adoc[ReceivedBlockHandler] to use: `WriteAheadLogBasedBlockHandler` or `BlockManagerBasedBlockHandler`.

* `spark.streaming.receiver.blockStoreTimeout` (default: `30`) time in seconds to wait until both writes to a write-ahead log and BlockManager complete successfully.

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to represent time. It is used in link:spark-streaming-jobgenerator.adoc[JobGenerator].

* `spark.streaming.ui.retainedBatches` (default: `1000`) controls the number of `BatchUIData` elements about completed batches in a first-in-first-out (FIFO) queue that are used to link:spark-streaming-webui.adoc[display statistics in Streaming page in web UI].
* `spark.streaming.receiverRestartDelay` (default: `2000`) - the time interval between a receiver is stopped and started again.

* `spark.streaming.concurrentJobs` (default: `1`) is the number of concurrent jobs, i.e. threads in link:spark-streaming-jobscheduler.adoc#streaming-job-executor[streaming-job-executor thread pool].

* `spark.streaming.stopSparkContextByDefault` (default: `true`) controls whether (`true`) or not (`false`) to stop the underlying SparkContext (regardless of whether this `StreamingContext` has been started).

[[spark_streaming_kafka_maxRatePerPartition]]
* `spark.streaming.kafka.maxRatePerPartition` (default: `0`) if non-`0` sets maximum number of messages per partition.

* `spark.streaming.manualClock.jump` (default: `0`) offsets (aka _jumps_) the system time, i.e. adds its value to checkpoint time, when used with the clock being a subclass of `org.apache.spark.util.ManualClock`. It is used when link:spark-streaming-jobgenerator.adoc[JobGenerator] is restarted from checkpoint.

* `spark.streaming.unpersist` (default: `true`) is a flag to control whether link:spark-streaming-dstreams.adoc#clearMetadata[output streams should unpersist old RDDs].

* `spark.streaming.gracefulStopTimeout` (default: 10 * link:spark-streaming-dstreamgraph.adoc#batch-interval[batch interval])

* `spark.streaming.stopGracefullyOnShutdown` (default: `false`) controls link:spark-streaming-streamingcontext.adoc#stop[whether to stop StreamingContext gracefully or not] and is used by link:spark-streaming-streamingcontext.adoc#stopOnShutdown[stopOnShutdown Shutdown Hook].

=== [[checkpointing]] Checkpointing

* `spark.streaming.checkpoint.directory` - when set and link:spark-streaming-streamingcontext.adoc#creating-instance[StreamingContext is created], the value of the setting gets passed on to link:spark-streaming-streamingcontext.adoc#checkpoint[StreamingContext.checkpoint] method.

=== [[back-pressure]] Back Pressure

* `spark.streaming.backpressure.enabled` (default: `false`) - enables (`true`) or disables (`false`) link:spark-streaming-backpressure.adoc[back pressure] in link:spark-streaming-receiverinputdstreams.adoc#back-pressure[input streams with receivers] or link:spark-streaming-kafka-DirectKafkaInputDStream.adoc#back-pressure[DirectKafkaInputDStream].

* `spark.streaming.backpressure.rateEstimator` (default: `pid`) is the link:spark-streaming-backpressure.adoc#RateEstimator[RateEstimator] to use.
