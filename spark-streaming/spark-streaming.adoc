== Spark Streaming

*Spark Streaming* is the incremental stream processing framework for Spark.

Spark Streaming offers the data abstraction called link:spark-streaming-dstreams.adoc[DStream] that hides the complexity of dealing with a continuous data stream and makes it as easy for programmers as using one single RDD at a time.

That is why Spark Streaming is also called a *micro-batching streaming framework* as a batch is one RDD at a time.

NOTE: I think Spark Streaming shines on performing the *T* stage well, i.e. the transformation stage, while leaving the *E* and *L* stages for more specialized tools like link:spark-streaming-kafka.adoc[Apache Kafka] or frameworks like Akka.

For a software developer, a `DStream` is similar to work with as a `RDD` with the DStream API to match RDD API. Interestingly, you can reuse your RDD-based code and apply it to `DStream` - a stream of RDDs - with no changes at all (through link:spark-streaming-operators.adoc#foreachRDD[foreachRDD]).

It runs <<Job, streaming jobs>> every <<batch-interval, batch duration>> to pull and process data (often called _records_) from one or many link:spark-streaming-inputdstreams.adoc[input streams].

Each batch link:spark-streaming-dstreams.adoc#contract[computes] (_generates_) a RDD for data in input streams for a given batch and link:spark-streaming-jobgenerator.adoc#generateJobs[submits a Spark job to compute the result]. It does this over and over again until link:spark-streaming-streamingcontext.adoc#stopping[the streaming context is stopped] (and the owning streaming application terminated).

To avoid losing records in case of failure, Spark Streaming supports link:spark-streaming-checkpointing.adoc[checkpointing that writes received records to a highly-available HDFS-compatible storage] and allows to recover from temporary downtimes.

Spark Streaming allows for integration with real-time data sources ranging from such basic ones like a HDFS-compatible file system or socket connection to more advanced ones like Apache Kafka or Apache Flume.

Checkpointing is also the foundation of link:spark-streaming-operators-stateful.adoc[stateful] and link:spark-streaming-windowedoperators.adoc[windowed] operations.

http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview[About Spark Streaming from the official documentation] (that pretty much nails what it offers):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* link:spark-streaming-operators.adoc[Stream Operators]
* <<batch, Batch>>, Batch time, and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Streaming Job>>
* link:spark-streaming-dstreams.adoc[Discretized Streams (DStreams)]
* link:spark-streaming-receivers.adoc[Receivers]

Other concepts often used in Spark Streaming:

* *ingestion* = the act of processing streaming data.

=== [[batch]][[micro-batch]] Micro Batch

*Micro Batch* is a collection of input records as collected by Spark Streaming that is later represented as an RDD.

A *batch* is internally represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[batchDuration]][[batch-interval]] Batch Interval (aka batchDuration)

*Batch Interval* is a property of a Streaming application that describes how often an RDD of input records is generated. It is the time to collect input records before they become a <<micro-batch, micro-batch>>.

=== [[Job]] Streaming Job

A streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When executed, it runs the computation (the input `func` function).

NOTE: A collection of streaming jobs is generated for a batch using link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph.generateJobs(time: Time)].

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id
