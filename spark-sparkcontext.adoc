== [[SparkContext]] SparkContext -- Entry Point to Spark (Core)

`SparkContext` (aka *Spark context*) is the entry point to Spark for a Spark application.

NOTE: You could also assume that a SparkContext instance _is_ a Spark application.

It link:spark-sparkcontext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment (deployment mode)].

Once a <<creating-instance, `SparkContext` instance is created>> you can use it to <<creating-rdds, create RDDs>>, <<creating-accumulators, accumulators>> and <<creating-broadcast-variables, broadcast variables>>, access Spark services and <<running-jobs, run jobs>>.

A Spark context is essentially a client of Spark's execution environment and acts as the _master of your Spark application_ (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though).

.Spark context acts as the master of your Spark application
image::diagrams/sparkcontext-services.png[align="center"]

`SparkContext` offers the following functions:

* Getting current configuration
** <<getConf, SparkConf>>
** <<master, deployment environment (as master URL)>>
** <<appName, application name>>
** <<deployMode, deploy mode>>
** <<defaultParallelism, default level of parallelism>>
** <<sparkUser, Spark user>>
** <<startTime, the time (in milliseconds) when `SparkContext` was created>>
** <<version, Spark version>>

* Setting configuration
** <<master-url, mandatory master URL>>
** <<setting-local-properties, local properties>>
** <<setting-default-log-level, default log level>>
* Creating objects
** <<creating-rdds, RDDs>>
** <<creating-accumulators, accumulators>>
** <<creating-broadcast-variables, broadcast variables>>
* Accessing services, e.g. link:spark-taskscheduler.adoc[TaskScheduler], link:spark-LiveListenerBus.adoc[LiveListenerBus], link:spark-blockmanager.adoc[BlockManager], link:spark-scheduler-backends.adoc[SchedulerBackends], link:spark-shuffle-manager.adoc[ShuffleManager].
* <<running-jobs, Running jobs>>
* <<custom-schedulers, Setting up custom Scheduler Backend, TaskScheduler and DAGScheduler>>
* <<closure-cleaning, Closure Cleaning>>
* <<submitJob, Submitting Jobs Asynchronously>>
* <<unpersist, Unpersisting RDDs, i.e. marking RDDs as non-persistent>>
* <<addSparkListener, Registering SparkListener>>
* <<dynamic-allocation, Programmable Dynamic Allocation>>

TIP: Read the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.SparkContext` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.SparkContext=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[persistentRdds]] Persisted RDDs

CAUTION: FIXME

=== [[persistRDD]] persistRDD

[source, scala]
----
persistRDD(rdd: RDD[_])
----

`persistRDD` is a `private[spark]` method to register `rdd` in <<persistentRdds, persistentRdds>> registry.

=== [[dynamic-allocation]] Programmable Dynamic Allocation

`SparkContext` offers the following methods as the developer API for link:spark-dynamic-allocation.adoc[dynamic allocation of executors]:

* <<requestExecutors, requestExecutors>>
* <<killExecutors, killExecutors>>
* <<requestTotalExecutors, requestTotalExecutors>>
* (private!) <<getExecutorIds, getExecutorIds>>

==== [[requestExecutors]] Requesting New Executors -- `requestExecutors` method

[source, scala]
----
requestExecutors(numAdditionalExecutors: Int): Boolean
----

`requestExecutors` requests `numAdditionalExecutors` executors from link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend].

==== [[killExecutors]] Requesting to Kill Executors -- `killExecutors` method

[source, scala]
----
killExecutors(executorIds: Seq[String]): Boolean
----

CAUTION: FIXME

==== [[requestTotalExecutors]] Requesting Total Executors -- `requestTotalExecutors` method

[source, scala]
----
requestTotalExecutors(
  numExecutors: Int,
  localityAwareTasks: Int,
  hostToLocalTaskCount: Map[String, Int]): Boolean
----

`requestTotalExecutors` is a `private[spark]` method that link:spark-scheduler-backends-coarse-grained.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend].

NOTE: It works for link:spark-scheduler-backends-coarse-grained.adoc[coarse-grained scheduler backends] only.

When called for other scheduler backends you should see the following WARN message in the logs:

```
WARN Requesting executors is only supported in coarse-grained mode
```

==== [[getExecutorIds]] Getting Executor Ids -- `getExecutorIds` method

`getExecutorIds` is a `private[spark]` method that is a part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply link:spark-scheduler-backends-coarse-grained.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls `getExecutorIds`].

NOTE: It works for link:spark-scheduler-backends-coarse-grained.adoc[coarse-grained scheduler backends] only.

When called for other scheduler backends you should see the following WARN message in the logs:

```
WARN Requesting executors is only supported in coarse-grained mode
```

CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!)

=== [[creating-instance]] Creating SparkContext

You can create a `SparkContext` instance with or without creating a link:spark-configuration.adoc[SparkConf] object first.

NOTE: You may want to read link:spark-sparkcontext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when `SparkContext` is created.

==== [[getOrCreate]] Getting Existing or Creating New SparkContext (getOrCreate methods)

[source, scala]
----
getOrCreate(): SparkContext
getOrCreate(conf: SparkConf): SparkContext
----

`SparkContext.getOrCreate` methods allow you to get the existing `SparkContext` or create a new one.

[source, scala]
----
import org.apache.spark.SparkContext
val sc = SparkContext.getOrCreate()

// Using an explicit SparkConf object
import org.apache.spark.SparkConf
val conf = new SparkConf()
  .setMaster("local[*]")
  .setAppName("SparkMe App")
val sc = SparkContext.getOrCreate(conf)
----

The no-param `getOrCreate` method requires that the two mandatory Spark settings - <<master, master>> and <<appName, application name>> - are specified using link:spark-submit.adoc[spark-submit].

==== [[constructors]] Constructors

[source, scala]
----
SparkContext()
SparkContext(conf: SparkConf)
SparkContext(master: String, appName: String, conf: SparkConf)
SparkContext(
  master: String,
  appName: String,
  sparkHome: String = null,
  jars: Seq[String] = Nil,
  environment: Map[String, String] = Map())
----

You can create a `SparkContext` instance using the four constructors.

[source, scala]
----
import org.apache.spark.SparkConf
val conf = new SparkConf()
  .setMaster("local[*]")
  .setAppName("SparkMe App")

import org.apache.spark.SparkContext
val sc = new SparkContext(conf)
----

When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services):

```
INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT
```

NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).

=== [[getConf]] Getting Current SparkConf (getConf method)

[source, scala]
----
getConf: SparkConf
----

`getConf` returns the current link:spark-configuration.adoc[SparkConf].

NOTE: Changing the `SparkConf` object does not change the current configuration (as the method returns a copy).

=== [[master]][[master-url]] Getting Deployment Environment (master method)

[source, scala]
----
master: String
----

`master` method returns the current value of link:spark-configuration.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use.

=== [[appName]] Getting Application Name (appName method)

[source, scala]
----
appName: String
----

`appName` returns the value of the mandatory link:spark-configuration.adoc#spark.app.name[spark.app.name] setting.

NOTE: It is used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend (to create a ApplicationDescription when it starts)], for link:spark-webui.adoc#SparkUI-createLiveUI[SparkUI.createLiveUI] (when link:spark-webui.adoc#spark_ui_enabled[spark.ui.enabled] is enabled), when `postApplicationStart` is executed, and for Mesos and checkpointing in Spark Streaming.

=== [[deployMode]] Getting Deploy Mode (deployMode method)

[source,scala]
----
deployMode: String
----

`deployMode` returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or `client` if not set.

=== [[getSchedulingMode]] Getting Scheduling Mode (getSchedulingMode method)

[source, scala]
----
getSchedulingMode: SchedulingMode.SchedulingMode
----

`getSchedulingMode` returns the current link:spark-taskscheduler-schedulingmode.adoc[Scheduling Mode].

=== [[getPoolForName]] Getting Schedulable (Pool) by Name (getPoolForName method)

[source, scala]
----
getPoolForName(pool: String): Option[Schedulable]
----

`getPoolForName` returns a link:spark-taskscheduler-schedulable.adoc[Schedulable] by the `pool` name, if one exists.

NOTE: `getPoolForName` is part of the Developer's API and may change in the future.

Internally, it requests the link:spark-taskscheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-taskscheduler-pool.adoc#schedulableNameToSchedulable[looks up the `Schedulable` by the `pool` name].

It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)].

=== [[getAllPools]] Getting All Pools (getAllPools method)

[source, scala]
----
getAllPools: Seq[Schedulable]
----

`getAllPools` collects the link:spark-taskscheduler-pool.adoc[Pools] in link:spark-taskscheduler.adoc#contract[TaskScheduler.rootPool].

NOTE: `TaskScheduler.rootPool` is part of the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

NOTE: `getAllPools` is part of the Developer's API.

CAUTION: FIXME Where is the method used?

NOTE: `getAllPools` is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used.

=== [[defaultParallelism]] Computing Default Level of Parallelism

*Default level of parallelism* is the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user.

It is used for the methods like `SparkContext.parallelize`, `SparkContext.range` and `SparkContext.makeRDD` (as well as link:spark-streaming/spark-streaming.adoc[Spark Streaming]'s `DStream.countByValue` and `DStream.countByValueAndWindow` and few other places). It is also used to instantiate link:spark-rdd-partitions.adoc#HashPartitioner[HashPartitioner] or for the minimum number of partitions in link:spark-rdd-hadooprdd.adoc[HadoopRDDs].

Internally, `defaultParallelism` relays requests for the default level of parallelism to link:spark-taskscheduler.adoc#defaultParallelism[TaskScheduler] (it is a part of its contract).

=== [[version]] Getting Spark Version

[source, scala]
----
version: String
----

`version` returns the Spark version this `SparkContext` uses.

=== [[setting-local-properties]][[setLocalProperty]] Setting Local Properties

[source, scala]
----
setLocalProperty(key: String, value: String): Unit
----

`setLocalProperty` sets a local thread-scoped `key` property to `value`.

[source, scala]
----
sc.setLocalProperty("spark.scheduler.pool", "myPool")
----

The goal of the local property concept is to differentiate between or group jobs submitted from different threads by local properties.

NOTE: It is used to link:spark-taskscheduler-FairSchedulableBuilder.adoc#spark.scheduler.pool[group jobs into pools in FAIR job scheduler by spark.scheduler.pool per-thread property] and in link:spark-sql-SQLExecution.adoc#withNewExecutionId[SQLExecution.withNewExecutionId Helper Methods]

If `value` is `null` the `key` property is removed the `key` from the local properties

[source, scala]
----
sc.setLocalProperty("spark.scheduler.pool", null)
----

A common use case for the local property concept is to set a local property in a thread, say link:spark-taskscheduler-FairSchedulableBuilder.adoc[spark.scheduler.pool], after which all jobs submitted within the thread will be grouped, say into a pool by FAIR job scheduler.

[source, scala]
----
val rdd = sc.parallelize(0 to 9)

sc.setLocalProperty("spark.scheduler.pool", "myPool")

// these two jobs (one per action) will run in the myPool pool
rdd.count
rdd.collect

sc.setLocalProperty("spark.scheduler.pool", null)

// this job will run in the default pool
rdd.count
----

=== [[makeRDD]] SparkContext.makeRDD

CAUTION: FIXME

=== [[submitJob]] Submitting Jobs Asynchronously -- `submitJob` method

[source, scala]
----
submitJob[T, U, R](
  rdd: RDD[T],
  processPartition: Iterator[T] => U,
  partitions: Seq[Int],
  resultHandler: (Int, U) => Unit,
  resultFunc: => R): SimpleFutureAction[R]
----

`submitJob` submits a job in an asynchronous, non-blocking way to link:spark-dagscheduler.adoc#submitJob[DAGScheduler].

It cleans the `processPartition` input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the link:link:spark-dagscheduler.adoc#JobWaiter[JobWaiter] instance (it has received from `DAGScheduler.submitJob`).

CAUTION: FIXME What are `resultFunc`?

It is used in:

* link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods
* link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver]

=== [[spark-configuration]] Spark Configuration

CAUTION: FIXME

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

You use a Spark context to create RDDs (see <<creating-rdds, Creating RDD>>).

When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.

.A Spark context creates a living space for RDDs.
image::diagrams/sparkcontext-rdds.png[align="center"]

=== [[creating-rdds]][[parallelize]] Creating RDD

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource` using `sc.newAPIHadoopFile`

Read link:spark-rdd.adoc#creating-rdds[Creating RDDs] in link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[unpersist]] Unpersisting RDDs (Marking RDDs as non-persistent)

It removes an RDD from the master's link:spark-blockmanager.adoc[Block Manager] (calls `removeRdd(rddId: Int, blocking: Boolean)`) and the internal <<persistentRdds, persistentRdds>> mapping.

It finally posts link:spark-SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to `listenerBus`.

=== [[setCheckpointDir]] Setting Checkpoint Directory (setCheckpointDir method)

[source, scala]
----
setCheckpointDir(directory: String)
----

`setCheckpointDir` method is used to set up the checkpoint directory...FIXME

CAUTION: FIXME

=== [[register]] Registering Custom Accumulators (register methods)

[source, scala]
----
register(acc: AccumulatorV2[_, _]): Unit
register(acc: AccumulatorV2[_, _], name: String): Unit
----

`register` registers the `acc` link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a `name`.

TIP: You can create built-in accumulators for longs, doubles, and collection types using <<creating-accumulators, specialized methods>>.

Internally, link:spark-accumulators.adoc#register[`register` registers the `SparkContext` to the accumulator].

=== [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators

[source, scala]
----
longAccumulator: LongAccumulator
longAccumulator(name: String): LongAccumulator
doubleAccumulator: DoubleAccumulator
doubleAccumulator(name: String): DoubleAccumulator
collectionAccumulator[T]: CollectionAccumulator[T]
collectionAccumulator[T](name: String): CollectionAccumulator[T]
----

You can use `longAccumulator`, `doubleAccumulator` or `collectionAccumulator` to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values.

`longAccumulator` returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value `0`.

`doubleAccumulator` returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value `0.0`.

`collectionAccumulator` returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value `java.util.List[T]`.

[source, scala]
----
scala> val acc = sc.longAccumulator
acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0)

scala> val counter = sc.longAccumulator("counter")
counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0)

scala> counter.value
res0: Long = 0

scala> sc.parallelize(0 to 9).foreach(n => counter.add(n))

scala> counter.value
res3: Long = 45
----

The `name` input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage).

.Accumulators in the Spark UI
image::images/spark-webui-accumulators.png[align="center"]

TIP: You can register custom accumulators using <<register, register>> methods.

=== [[broadcast]][[creating-broadcast-variables]] Creating Broadcast Variables

[source, scala]
----
broadcast[T](value: T): Broadcast[T]
----

`broadcast` method creates a link:spark-broadcast.adoc[broadcast variable] that is a shared memory with `value` on all Spark executors.

```
scala> val hello = sc.broadcast("hello")
hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0)
```

Spark transfers the value to Spark executors _once_, and tasks can share it without incurring repetitive network transmissions when requested multiple times.

.Broadcasting a value to executors
image::images/sparkcontext-broadcast-executors.png[align="center"]

When a broadcast value is created the following INFO message appears in the logs:

```
INFO SparkContext: Created broadcast [id] from broadcast at <console>:25
```

[NOTE]
====
Spark does not support broadcasting RDDs.

```
scala> sc.broadcast(sc.range(0, 10))
java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)
  ... 48 elided
```
====

Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]).

.Broadcast Variables In web UI's Executors Tab
image::images/spark-broadcast-webui-executors-rdd-blocks.png[align="center"]

=== [[jars]] Distribute JARs to workers

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

The configuration setting `spark.jars` is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or `local:/path` for a file on every worker node.

```
scala> sc.addJar("build.sbt")
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
```

CAUTION: FIXME Why is HttpFileServer used for addJar?

=== SparkContext as the global configuration for services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-dagscheduler-ShuffleMapStage.adoc[registering shuffle dependencies] to link:spark-shuffle-manager.adoc[Shuffle Service].

=== [[runJob]][[running-jobs]] Running Jobs (runJob methods)

[source, scala]
----
runJob[T, U](
  rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int],
  resultHandler: (Int, U) => Unit): Unit
runJob[T, U](
  rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int]): Array[U]
runJob[T, U](
  rdd: RDD[T],
  func: Iterator[T] => U,
  partitions: Seq[Int]): Array[U]
runJob[T, U](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U]
runJob[T, U](rdd: RDD[T], func: Iterator[T] => U): Array[U]
runJob[T, U](
  rdd: RDD[T],
  processPartition: (TaskContext, Iterator[T]) => U,
  resultHandler: (Int, U) => Unit)
runJob[T, U: ClassTag](
  rdd: RDD[T],
  processPartition: Iterator[T] => U,
  resultHandler: (Int, U) => Unit)
----

link:spark-rdd.adoc#actions[RDD actions] in Spark run link:spark-dagscheduler-jobs.adoc[jobs] using one of `runJob` methods. It executes a function on one or many partitions of a RDD to produce a collection of values per partition.

TIP: For some actions, e.g. `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

TIP: Read about `TaskContext` in link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Running a job is essentially executing a `func` function on all or a subset of partitions in an `rdd` RDD and returning the result as an array (with elements being the results per partition).

When executed, `runJob` prints out the following INFO message:

```
INFO Starting job: ...
```

And it follows up on link:spark-rdd-lineage.adoc#spark.logLineage[spark.logLineage] and then hands over the execution to link:spark-dagscheduler.adoc#runJob[DAGScheduler.runJob].

.Executing action
image::images/spark-runjob.png[align="center"]

Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing] and posts `JobSubmitted` event (see <<event-loop, Event loop>>).

[CAUTION]
====
Spark can only run jobs when a Spark context is available and active, i.e. started. See <<stopping, Stopping Spark context>>.

Since SparkContext runs inside a Spark driver, i.e. a Spark application, it must be alive to run jobs.
====

=== [[stop]][[stopping]] Stopping SparkContext (stop method)

[source, scala]
----
stop(): Unit
----

You can stop a `SparkContext` using `stop` method. Stopping a Spark context stops the link:spark-sparkenv.adoc[Spark Runtime Environment] and shuts down the entire Spark application (see link:spark-anatomy-spark-application.adoc[Anatomy of Spark Application]).

Calling `stop` many times leads to the following INFO message in the logs:

```
INFO SparkContext: SparkContext already stopped.
```

An attempt to use a stopped SparkContext's services will result in `java.lang.IllegalStateException: SparkContext has been shutdown`.

[source, scala]
----
scala> sc.stop

scala> sc.parallelize(0 to 5)
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
----

When a SparkContext is being stopped, it does the following:

* Posts a application end event link:spark-SparkListener.adoc#SparkListenerApplicationEnd[SparkListenerApplicationEnd] to link:spark-LiveListenerBus.adoc[LiveListenerBus]
* Stops link:spark-webui.adoc[web UI]
* Requests link:spark-metrics.adoc[MetricSystem] to report metrics from all registered sinks (using `MetricsSystem.report()`)
* `metadataCleaner.cancel()`
* Stops link:spark-service-contextcleaner.adoc[ContextCleaner]
* Stops link:spark-service-executor-allocation-manager.adoc[ExecutorAllocationManager]
* Stops link:spark-dagscheduler.adoc[DAGScheduler]
* Stops link:spark-LiveListenerBus.adoc[LiveListenerBus]
* Stops link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener]
* Stops link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver]
* Stops optional <<ConsoleProgressBar, ConsoleProgressBar>>
* It clears the reference to TaskScheduler (i.e. `_taskScheduler` is `null`)
* Stops link:spark-sparkenv.adoc[SparkEnv] and calls `SparkEnv.set(null)`

CAUTION: FIXME `SparkEnv.set(null)` what is this doing?

* It clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[SPARK_YARN_MODE flag].

* It calls `SparkContext.clearActiveContext()`.

CAUTION: FIXME What is `SparkContext.clearActiveContext()` doing?

If all went fine till now you should see the following INFO message in the logs:

```
INFO SparkContext: Successfully stopped SparkContext
```

=== [[addSparkListener]] Registering SparkListener -- `addSparkListener` method

[source, scala]
----
addSparkListener(listener: SparkListenerInterface): Unit
----

You can register a custom link:spark-SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using `addSparkListener` method

NOTE: You can also register custom listeners using link:spark-LiveListenerBus.adoc#spark_extraListeners[spark.extraListeners] setting.

=== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

It is also applicable to `SchedulerBackend` and `TaskScheduler` using `schedulerBackend_=(sb: SchedulerBackend)` and `taskScheduler_=(ts: TaskScheduler)` methods, respectively.

CAUTION: FIXME Make it an advanced exercise.

=== [[events]] Events

When a Spark context starts, it triggers link:spark-SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and link:spark-SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages.

Refer to the section <<creating-instance, SparkContext's initialization>>.

=== [[setLogLevel]][[setting-default-log-level]] Setting Default Log Level (setLogLevel method)

[source, scala]
----
setLogLevel(logLevel: String)
----

`setLogLevel` allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell].

Internally, `setLogLevel` calls `org.apache.log4j.Level.toLevel(logLevel)` and `org.apache.log4j.Logger.getRootLogger().setLevel(l)`.

=== [[SparkStatusTracker]] SparkStatusTracker

`SparkStatusTracker` requires a Spark context to work. It is created as part of <<creating-instance, SparkContext's initialization>>.

SparkStatusTracker is only used by <<ConsoleProgressBar, ConsoleProgressBar>>.

=== [[ConsoleProgressBar]] ConsoleProgressBar

`ConsoleProgressBar` shows the progress of active stages in console (to `stderr`). It polls the status of stages from <<SparkStatusTracker, SparkStatusTracker>> periodically and prints out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time.

```
[Stage 0:====>          (316 + 4) / 1000][Stage 1:>                (0 + 0) / 1000][Stage 2:>                (0 + 0) / 1000]]]
```

The progress includes the stage's id, the number of completed, active, and total tasks.

It is useful when you `ssh` to workers and want to see the progress of active stages.

It is only instantiated if the value of the boolean property `spark.ui.showConsoleProgress` (default: `true`) is `true` and the log level of `org.apache.spark.SparkContext` logger is `WARN` or higher (refer to link:spark-logging.adoc[Logging]).

[source, scala]
----
import org.apache.log4j._
Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)
----

To print the progress nicely ConsoleProgressBar uses `COLUMNS` environment variable to know the width of the terminal. It assumes `80` columns.

The progress bar prints out the status after a stage has ran at least `500ms`, every `200ms` (the values are not configurable).

See the progress bar in Spark shell with the following:

[source]
----
$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true  # <1>

scala> sc.setLogLevel("OFF")  // <2>

scala> Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)  // <3>

scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count  // <4>
[Stage 2:>                                                          (0 + 4) / 4]
[Stage 2:==============>                                            (1 + 3) / 4]
[Stage 2:=============================>                             (2 + 2) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
----
<1> Make sure `spark.ui.showConsoleProgress` is `true`. It is by default.
<2> Disable (`OFF`) the root logger (that includes Spark's logger)
<3> Make sure `org.apache.spark.SparkContext` logger is at least `WARN`.
<4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar.

https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action.

You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017\] show progress bar in console #3029]):

```
> ./bin/spark-shell
scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> a.union(b).count()
```

=== [[closure-cleaning]] Closure Cleaning (clean method)

Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors.

SparkContext comes with `clean(f: F, checkSerializable: Boolean = true)` method that does this. It in turn calls `ClosureCleaner.clean` method.

Not only does `ClosureCleaner.clean` method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively.

A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.ClosureCleaner` logger to see what happens inside the class.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

With `DEBUG` logging level you should see the following messages in the logs:

```
+++ Cleaning closure [func] ([func.getClass.getName]) +++
 + declared fields: [declaredFields.size]
     [field]
 ...
+++ closure [func] ([func.getClass.getName]) is now cleaned +++
```

Serialization is verified using a new instance of `Serializer` (as link:spark-sparkenv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization].

CAUTION: FIXME an example, please.

=== [[hadoopConfiguration]] Hadoop Configuration

While a <<creating-instance, `SparkContext` is being created>>, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration] that is available as `_hadoopConfiguration`).

NOTE: link:varia/spark-hadoop.adoc#SparkHadoopUtil[SparkHadoopUtil.get.newConfiguration] is used.

If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default `Configuration` object is returned.

If `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are both available, the following settings are set for the Hadoop configuration:

* `fs.s3.awsAccessKeyId`, `fs.s3n.awsAccessKeyId`, `fs.s3a.access.key` are set to the value of `AWS_ACCESS_KEY_ID`
* `fs.s3.awsSecretAccessKey`, `fs.s3n.awsSecretAccessKey`, and `fs.s3a.secret.key` are set to the value of `AWS_SECRET_ACCESS_KEY`

Every `spark.hadoop.` setting becomes a setting of the configuration with the prefix `spark.hadoop.` removed for the key.

The value of `spark.buffer.size` (default: `65536`) is used as the value of `io.file.buffer.size`.

=== [[listenerBus]] listenerBus

`listenerBus` is a link:spark-LiveListenerBus.adoc[LiveListenerBus] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver].

NOTE: It is created and started when link:spark-sparkcontext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver.

NOTE: `listenerBus` is a `private[spark]` value in `SparkContext`.

=== [[startTime]] Time when SparkContext was Created (startTime value)

[source, scala]
----
startTime: Long
----

`startTime` is the time in milliseconds when <<creating-instance, SparkContext was created>>.

[source, scala]
----
scala> sc.startTime
res0: Long = 1464425605653
----

=== [[sparkUser]] Spark User (sparkUser value)

[source, scala]
----
sparkUser: String
----

`sparkUser` is the user who started the `SparkContext` instance.

NOTE: It is computed when link:spark-sparkcontext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-sparkcontext-creating-instance-internals.adoc#[Utils.getCurrentUserName].

=== [[settings]] Settings

==== [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts

Quoting the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

You can however control the behaviour using `spark.driver.allowMultipleContexts` flag.

It is disabled, i.e. `false`, by default.

If enabled (i.e. `true`), Spark prints the following WARN message to the logs:

```
WARN Multiple running SparkContexts detected in the same JVM!
```

If disabled (default), it will throw an `SparkException` exception:

```
Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
[ctx.creationSite.longForm]
```

When creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.

=== [[environment-variables]] Environment Variables

==== [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY

`SPARK_EXECUTOR_MEMORY` sets the amount of memory to allocate to each executor. See link:spark-executor.adoc#memory[Executor Memory].

==== [[SPARK_USER]] SPARK_USER

`SPARK_USER` is the user who is running `SparkContext`. It is available later as <<sparkUser, sparkUser>>.
