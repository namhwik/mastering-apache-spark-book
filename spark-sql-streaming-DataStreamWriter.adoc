== DataStreamWriter

CAUTION: FIXME

`DataFrameWriter` is a part of  link:spark-sql-structured-streaming.adoc[Structured Streaming API].

[source, scala]
----
val df: DataFrame = ...

import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
df.writeStream
  .queryName("textStream")
  .trigger(ProcessingTime(10.seconds))
  .format("console")
  .start
----

* <<trigger, trigger>> to set the link:spark-sql-trigger.adoc[Trigger] for a stream query.
* <<queryName, queryName>>
* <<startStream, startStream>> to start a continuous write.

=== [[streams]][[startStream]] Data Streams (startStream methods)

`DataFrameWriter` comes with two `startStream` methods to return a link:spark-sql-StreamingQuery.adoc[StreamingQuery] object to continually write data.

[source, scala]
----
startStream(): StreamingQuery
startStream(path: String): StreamingQuery  // <1>
----
<1> Sets `path` option to `path` and calls `startStream()`

NOTE: `startStream` uses link:spark-sql-StreamingQueryManager.adoc#startQuery[StreamingQueryManager.startQuery] to create link:spark-sql-StreamingQuery.adoc[StreamingQuery].

NOTE: Whether or not you have to specify `path` option depends on the link:spark-sql-datasource.adoc[DataSource] in use.

Recognized options:

* `queryName` is the name of active streaming query.
* `checkpointLocation` is the directory for checkpointing.

NOTE: Define options using <<option, option>> or <<options, options>> methods.

NOTE: It is a new feature of Spark *2.0.0*.

=== [[outputMode]] Specifying Output Mode (outputMode method)

[source, scala]
----
outputMode(outputMode: OutputMode): DataStreamWriter[T]
----

`outputMode` specifies *output mode* of a streaming link:spark-sql-dataset.adoc[Dataset] which is what gets written to a link:spark-sql-sink.adoc[streaming sink] when there is a new data available.

Currently, the following output modes are supported:

* `OutputMode.Append` -- only the new rows in the streaming dataset will be written to a sink.

* `OutputMode.Complete` -- entire streaming dataset (with all the rows) will be written to a sink every time there are updates. It is supported only for streaming queries with aggregations.

=== [[queryName]] queryName

[source, scala]
----
queryName(queryName: String): DataStreamWriter[T]
----

`queryName` sets the name of a link:spark-sql-StreamingQuery.adoc[streaming query].

Internally, it is just an additional <<option, option>> with the key `queryName`.

=== [[trigger]] trigger

[source, scala]
----
trigger(trigger: Trigger): DataStreamWriter[T]
----

`trigger` method sets the time interval of the *trigger* (batch) for a streaming query.

NOTE: `Trigger` specifies how often results should be produced by a link:spark-sql-StreamingQuery.adoc[StreamingQuery]. See link:spark-sql-trigger.adoc[Trigger].

The default trigger is link:spark-sql-trigger.adoc#ProcessingTime[ProcessingTime(0L)] that runs a streaming query as often as possible.

TIP: Consult link:spark-sql-trigger.adoc[Trigger] to learn about `Trigger` and `ProcessingTime` types.

=== [[start]] start methods

[source, scala]
----
start(path: String): StreamingQuery
start(): StreamingQuery
----

=== [[foreach]] foreach
