== DataStreamWriter

`DataFrameWriter` is a part of  link:spark-sql-structured-streaming.adoc[Structured Streaming API] as of Spark 2.0 that is responsible for writing the output of streaming queries to sinks and hence starting their execution.

[source, scala]
----
val people: Dataset[Person] = ...

import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.OutputMode.Complete
df.writeStream
  .queryName("textStream")
  .outputMode(Complete)
  .trigger(ProcessingTime(10.seconds))
  .format("console")
  .start
----

1. <<queryName, queryName>> to set the name of a query
2. <<outputMode, outputMode>> to specify output mode.
3. <<trigger, trigger>> to set the link:spark-sql-streaming-trigger.adoc[Trigger] for a stream query.
4. <<start, start>> to start continuous writing to a sink.

=== [[outputMode]] Specifying Output Mode -- `outputMode` method

[source, scala]
----
outputMode(outputMode: OutputMode): DataStreamWriter[T]
----

`outputMode` specifies *output mode* of a streaming link:spark-sql-dataset.adoc[Dataset] which is what gets written to a link:spark-sql-streaming-sink.adoc[streaming sink] when there is a new data available.

Currently, the following output modes are supported:

* `OutputMode.Append` -- only the new rows in the streaming dataset will be written to a sink.

* `OutputMode.Complete` -- entire streaming dataset (with all the rows) will be written to a sink every time there are updates. It is supported only for streaming queries with aggregations.

=== [[queryName]] Setting Query Name -- `queryName` method

[source, scala]
----
queryName(queryName: String): DataStreamWriter[T]
----

`queryName` sets the name of a link:spark-sql-streaming-StreamingQuery.adoc[streaming query].

Internally, it is just an additional <<option, option>> with the key `queryName`.

=== [[trigger]] Setting How Often to Execute Streaming Query -- `trigger` method

[source, scala]
----
trigger(trigger: Trigger): DataStreamWriter[T]
----

`trigger` method sets the time interval of the *trigger* (batch) for a streaming query.

NOTE: `Trigger` specifies how often results should be produced by a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery]. See link:spark-sql-streaming-trigger.adoc[Trigger].

The default trigger is link:spark-sql-streaming-trigger.adoc#ProcessingTime[ProcessingTime(0L)] that runs a streaming query as often as possible.

TIP: Consult link:spark-sql-streaming-trigger.adoc[Trigger] to learn about `Trigger` and `ProcessingTime` types.

=== [[start]] Starting Continuous Writing to Sink -- `start` methods

[source, scala]
----
start(): StreamingQuery
start(path: String): StreamingQuery  // <1>
----
<1> Sets `path` option to `path` and calls `start()`

`start` methods start a streaming query and return a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] object to continually write data.

NOTE: Whether or not you have to specify `path` option depends on the link:spark-sql-datasource.adoc[DataSource] in use.

Recognized options:

* `queryName` is the name of active streaming query.
* `checkpointLocation` is the directory for checkpointing.

NOTE: Define options using <<option, option>> or <<options, options>> methods.

=== [[foreach]] `foreach` method
