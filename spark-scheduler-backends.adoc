== [[SchedulerBackend]] Scheduler Backends

Spark comes with a pluggable backend mechanism called *scheduler backend* (aka _backend scheduler_) to support various cluster managers, e.g. link:spark-mesos/spark-mesos.adoc[Apache Mesos], link:yarn/README.adoc[Hadoop YARN] or Spark's own link:spark-standalone.adoc[Spark Standalone] and link:spark-local.adoc#LocalBackend[Spark local].

These cluster managers differ by their custom task scheduling modes and resource offers mechanisms, and Spark's approach is to abstract the differences in <<contract, SchedulerBackend Contract>>.

A scheduler backend is created and started as part of SparkContext's initialization (when link:spark-taskscheduler.adoc[TaskSchedulerImpl] is started - see link:spark-sparkcontext-creating-instance-internals.adoc#createTaskScheduler[Creating Scheduler Backend and Task Scheduler]).

CAUTION: FIXME Image how it gets created with SparkContext in play here or in SparkContext doc.

Scheduler backends are started and stopped as part of TaskSchedulerImpl's initialization and stopping.

Being a scheduler backend in Spark assumes a http://mesos.apache.org/[Apache Mesos]-like model in which "an application" gets *resource offers* as machines become available and can launch tasks on them. Once a scheduler backend obtains the resource allocation, it can start executors.

TIP: Understanding how http://mesos.apache.org/[Apache Mesos] works can greatly improve understanding Spark.

=== [[contract]] SchedulerBackend Contract

NOTE: `org.apache.spark.scheduler.SchedulerBackend` is a `private[spark]` Scala trait in Spark.

Every `SchedulerBackend` has to follow the following contract:

* Can be started (using `start()`) and stopped (using `stop()`)
* <<reviveOffers, reviveOffers>>
* Calculate <<defaultParallelism, default level of parallelism>>
* <<killTask, killTask>>
* Answers `isReady()` to inform whether it is currently started or stopped. It returns `true` by default.
* Knows the application id for a job (using `applicationId()`).

CAUTION: FIXME `applicationId()` doesn't accept an input parameter. How is Scheduler Backend related to a job and an application?

* Knows an application attempt id (see <<applicationAttemptId, applicationAttemptId>>)
* Knows the URLs for the driver's logs (see <<getDriverLogUrls, getDriverLogUrls>>).

CAUTION: FIXME Screenshot the tab and the links

==== [[reviveOffers]] `reviveOffers` Method

NOTE: It is used in `TaskSchedulerImpl` using `backend` internal reference when link:spark-taskschedulerimpl.adoc#submitTasks[submitting tasks].

There are currently three custom implementations of `reviveOffers` available in Spark for different clustering options:

* For local mode read  link:spark-local.adoc#task-submission[Task Submission a.k.a. reviveOffers].

* link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#reviveOffers[CoarseGrainedSchedulerBackend]

* link:spark-mesos/spark-mesos.adoc#reviveOffers[MesosFineGrainedSchedulerBackend]

==== [[defaultParallelism]] Default Level of Parallelism (defaultParallelism method)

[source, scala]
----
defaultParallelism(): Int
----

*Default level of parallelism* is used by link:spark-taskscheduler.adoc[TaskScheduler] to use as a hint for sizing jobs.

NOTE: It is used in `TaskSchedulerImpl.defaultParallelism`.

Refer to link:spark-local.adoc#LocalBackend[LocalBackend] for local mode.

Refer to link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#defaultParallelism[Default Level of Parallelism] for CoarseGrainedSchedulerBackend.

Refer to link:spark-mesos/spark-mesos.adoc#defaultParallelism[Default Level of Parallelism] for CoarseMesosSchedulerBackend.

No other custom implementations of `defaultParallelism()` exists.

==== [[killTask]] Killing Task -- `killTask` Method

[source, scala]
----
killTask(taskId: Long, executorId: String, interruptThread: Boolean)
----

`killTask` throws a `UnsupportedOperationException` by default.

==== [[applicationAttemptId]] applicationAttemptId

[source, scala]
----
applicationAttemptId(): Option[String] = None
----

`applicationAttemptId` returns the application attempt id of a Spark application.

It is currently only supported by link:spark-yarn-yarnschedulerbackend.adoc#applicationAttemptId[YARN cluster scheduler backend] as the YARN cluster manager supports multiple application attempts.

NOTE: `applicationAttemptId` is also a part of link:spark-taskscheduler.adoc#contract[TaskScheduler contract] and link:spark-taskschedulerimpl.adoc#applicationAttemptId[`TaskSchedulerImpl` directly calls the SchedulerBackend's `applicationAttemptId`].

==== [[getDriverLogUrls]] getDriverLogUrls

`getDriverLogUrls: Option[Map[String, String]]` returns no URLs by default.

It is currently only supported by link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc#YarnClusterSchedulerBackend[YarnClusterSchedulerBackend]

=== Available Implementations

Spark comes with the following scheduler backends:

* link:spark-local.adoc#LocalBackend[LocalBackend] (local mode)
* link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]
** *SparkDeploySchedulerBackend* used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone] (and local-cluster - FIXME)
** link:yarn/spark-yarn-yarnschedulerbackend.adoc[YarnSchedulerBackend]
*** link:yarn/spark-yarn-client-yarnclientschedulerbackend.adoc#YarnClientSchedulerBackend[YarnClientSchedulerBackend] (for *client* deploy mode)
*** link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc#YarnClusterSchedulerBackend[YarnClusterSchedulerBackend] (for *cluster* deploy mode).
** link:spark-mesos/spark-mesos.adoc#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend]
* link:spark-mesos/spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend]
