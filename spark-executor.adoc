== Executors

*Executors* are distributed agents that execute link:spark-taskscheduler-tasks.adoc[tasks].

They _typically_ run for the entire lifetime of a Spark application and is called *static allocation of executors* (but you could also opt in for link:spark-dynamic-allocation.adoc[dynamic allocation]).

Executors send <<heartbeats-and-active-task-metrics, active task metrics>> to the link:spark-driver.adoc[driver] and inform link:spark-executor-backends.adoc[executor backends] about task status updates (including task results).

NOTE: Executors are managed exclusively by link:spark-executor-backends.adoc[executor backends].

Executors provide in-memory storage for RDDs that are cached in Spark applications (via link:spark-blockmanager.adoc[Block Manager]).

When executors are started they register themselves with the driver and communicate directly to execute tasks.

*Executor offers* are described by executor id and the host on which an executor runs (see <<resource-offers, Resource Offers>> in this document).

Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track link:spark-executor-taskrunner.adoc[running tasks] (by their task ids in <<runningTasks, runningTasks>> internal registry). Consult <<launching-tasks, Launching Tasks>> section.

Executors use a <<thread-pool, thread pool>> for <<launching-tasks, launching tasks>> and <<metrics, sending metrics>>.

It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster.

Executors are described by their *id*, *hostname*, *environment* (as `SparkEnv`), and *classpath* (and, less importantly, and more for internal optimization, whether they run in link:spark-local.adoc[local] or link:spark-cluster.adoc[cluster mode]).

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.executor.Executor` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[stop]] Stopping Executor -- `stop` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `Executor` Instance

`Executor` requires `executorId`, `executorHostname`, a link:spark-sparkenv.adoc[SparkEnv], `userClassPath` and whether it runs in local or cluster mode (with cluster as the default).

NOTE: `isLocal` is enabled exclusively for link:spark-local.adoc#LocalEndpoint[LocalEndpoint] (for link:spark-local.adoc[Spark in local mode]).

When created, you should see the following INFO messages in the logs:

```
INFO Executor: Starting executor ID [executorId] on host [executorHostname]
```

It <<startDriverHeartbeater, creates an RPC endpoint for sending hearbeats to the driver>>.

When in non-local/cluster mode, a link:spark-blockmanager.adoc#initialize[`BlockManager` is initialized].

NOTE: The `BlockManager` for an executor is available in `SparkEnv` passed to the constructor.

A worker requires the additional services (beside the common ones like ...):

* executorActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Akka only)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker]
* link:spark-metrics.adoc[MetricsSystem] with the name `executor`

link:spark-executor-ExecutorSource.adoc[ExecutorSource] is created (with `executorId`). And, only for cluster mode, link:spark-metrics.adoc#registerSource[`MetricsSystem` is requested to register it].

(only for cluster mode) link:spark-blockmanager.adoc#initialize[`BlockManager` is initialized].

NOTE: A `Executor` is created when link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[`CoarseGrainedExecutorBackend` receives `RegisteredExecutor` message], in link:spark-executor-backends.adoc#MesosExecutorBackend[MesosExecutorBackend.registered] and when link:spark-local.adoc#LocalEndpoint-creating-instance[LocalEndpoint is created].

CAUTION: FIXME How many cores are assigned per executor?

=== [[launchTask]][[launching-tasks]] Launching Tasks -- `launchTask` Method

[source, scala]
----
launchTask(
  context: ExecutorBackend,
  taskId: Long,
  attemptNumber: Int,
  taskName: String,
  serializedTask: ByteBuffer): Unit
----

`launchTask` executes the input `serializedTask` task concurrently.

Internally, `launchTask` creates a link:spark-executor-taskrunner.adoc[TaskRunner], registers it in <<runningTasks, `runningTasks` internal registry>> (by `taskId`), and finally executes it on <<threadPool, "Executor task launch worker" thread pool>>.

.Launching tasks on executor using TaskRunners
image::images/executor-taskrunner-executorbackend.png[align="center"]

NOTE: `launchTask` is called by link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] (when it handles link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#LaunchTask[LaunchTask] message), link:spark-executor-backends.adoc#MesosExecutorBackend[MesosExecutorBackend], and link:spark-local.adoc#LocalEndpoint[LocalEndpoint].

=== [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics -- `startDriverHeartbeater` Method

Executors keep sending <<metrics, metrics for active tasks>> to the driver every <<spark_executor_heartbeatInterval, spark.executor.heartbeatInterval>> (defaults to `10s` with some random initial delay so the heartbeats from different executors do not pile up on the driver).

.Executors use HeartbeatReceiver endpoint to report task metrics
image::images/executor-heartbeatReceiver-endpoint.png[align="center"]

An executor sends heartbeats using the <<heartbeater, internal heartbeater - Heartbeat Sender Thread>>.

.HeartbeatReceiver's Heartbeat Message Handler
image::images/spark-HeartbeatReceiver-Heartbeat.png[align="center"]

For each link:spark-taskscheduler-tasks.adoc[task] in link:spark-executor-taskrunner.adoc[TaskRunner] (in <<runningTasks, runningTasks>> internal registry), the task's metrics are computed (i.e. `mergeShuffleReadMetrics` and `setJvmGCTime`) that become part of the heartbeat (with accumulators).

CAUTION: FIXME How do `mergeShuffleReadMetrics` and `setJvmGCTime` influence `accumulators`?

NOTE: Executors track the link:spark-executor-taskrunner.adoc[TaskRunner] that run link:spark-taskscheduler-tasks.adoc[tasks]. A link:spark-executor-taskrunner.adoc#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat.

A blocking link:spark-sparkcontext-HeartbeatReceiver.adoc#Heartbeat[Heartbeat] message that holds the executor id, all accumulator updates (per task id), and link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId] is sent to link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint] (with <<spark_executor_heartbeatInterval, spark.executor.heartbeatInterval>> timeout).

CAUTION: FIXME When is `heartbeatReceiverRef` created?

If the response link:spark-sparkcontext-HeartbeatReceiver.adoc#Heartbeat[requests to reregister BlockManager], you should see the following INFO message in the logs:

```
INFO Executor: Told to re-register on heartbeat
```

The link:spark-blockmanager.adoc#reregister[BlockManager is reregistered].

The internal <<heartbeatFailures, heartbeatFailures>> counter is reset (i.e. becomes `0`).

If there are any issues with communicating with the driver, you should see the following WARN message in the logs:

```
WARN Executor: Issue communicating with driver in heartbeater
```

The internal <<heartbeatFailures, heartbeatFailures>> is incremented and checked to be less than the <<spark_executor_heartbeat_maxFailures, acceptable number of failures>>. If the number is greater, the following ERROR is printed out to the logs:

```
ERROR Executor: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times
```

The executor exits (using `System.exit` and exit code 56).

TIP: Read about `TaskMetrics` in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

=== [[heartbeater]] heartbeater - Heartbeat Sender Thread

`heartbeater` is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread.

The name of the thread pool is *driver-heartbeater*.

=== [[coarse-grained-executor]] Coarse-Grained Executors

*Coarse-grained executors* are executors that use link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] for task scheduling.

=== [[FetchFailedException]] FetchFailedException

CAUTION: FIXME

`FetchFailedException` exception is thrown when an executor (more specifically link:spark-executor-taskrunner.adoc[TaskRunner]) has failed to fetch a shuffle block.

It contains the following:

* the unique identifier for a BlockManager (as `BlockManagerId`)
* `shuffleId`
* `mapId`
* `reduceId`
* `message` - a short exception message
* `cause` - a `Throwable` object

link:spark-executor-taskrunner.adoc[TaskRunner] catches it and informs link:spark-executor-backends.adoc[ExecutorBackend] about the case (using `statusUpdate` with `TaskState.FAILED` task state).

CAUTION: FIXME Image with the call to ExecutorBackend.

=== [[resource-offers]] Resource Offers

Read link:spark-taskschedulerimpl.adoc#resourceOffers[resourceOffers] in TaskSchedulerImpl and link:spark-tasksetmanager.adoc##resourceOffers[resourceOffer] in TaskSetManager.

=== [[threadPool]][[thread-pool]] "Executor task launch worker" Thread Pool

Executors use the daemon cached thread pools with the name *Executor task launch worker-ID* (with `ID` being the task id) for <<launching-tasks, launching tasks>>.

=== [[memory]] Executor Memory -- `spark.executor.memory` or `SPARK_EXECUTOR_MEMORY` settings

You can control the amount of memory per executor using <<spark_executor_memory, spark.executor.memory>> setting. It sets the available memory equally for all executors per application.

NOTE: The amount of memory per executor is looked up when link:spark-sparkcontext.adoc#creating-instance[SparkContext is created].

You can change the assigned memory per executor per node in link:spark-standalone.adoc[standalone cluster] using link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable.

You can find the value displayed as *Memory per Node* in link:spark-standalone-master.adoc[web UI for standalone Master] (as depicted in the figure below).

.Memory per Node in Spark Standalone's web UI
image::images/spark-standalone-webui-memory-per-node.png[align="center"]

The above figure shows the result of running link:spark-shell.adoc[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e.

```
./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g
```

=== [[metrics]] Metrics

Every executor registers its own link:spark-executor-ExecutorSource.adoc[ExecutorSource] to link:spark-metrics.adoc#report[report metrics].

=== [[internal-registries]] Internal Registries and Counters

.Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Name | Description
| [[runningTasks]] `runningTasks` |
| [[heartbeatFailures]] `heartbeatFailures` |
|======================

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property | Default Value | Description
| [[spark_executor_cores]] `spark.executor.cores` | | Number of cores for an executor.
| [[spark_executor_extraClassPath]] `spark.executor.extraClassPath` | | List of URLs representing a user's CLASSPATH.

Each entry is separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
| [[spark_executor_extraJavaOptions]] `spark.executor.extraJavaOptions` | | Extra Java options for executors.

Used to link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[prepare the command to launch `CoarseGrainedExecutorBackend` in a YARN container].

| [[spark_executor_extraLibraryPath]] `spark.executor.extraLibraryPath` | | List of additional library paths separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.

Used to link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[prepare the command to launch `CoarseGrainedExecutorBackend` in a YARN container].

| [[spark_executor_userClassPathFirst]] `spark.executor.userClassPathFirst` | `false` | Flag to control whether to load classes in user jars before those in Spark jars.

| [[spark_executor_heartbeatInterval]] `spark.executor.heartbeatInterval` | `10s` | Interval after which an executor reports heartbeat and metrics for active tasks to the driver.

Refer to <<heartbeats-and-active-task-metrics, Sending heartbeats and partial metrics for active tasks>> in this document.

| [[spark_executor_heartbeat_maxFailures]] `spark.executor.heartbeat.maxFailures` | `60` | Number of times an executor will try to send heartbeats to the driver before it gives up and exits (with exit code `56`).

NOTE: It was introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times].

| [[spark_executor_id]] `spark.executor.id` | |

| [[spark_executor_instances]] `spark.executor.instances` | `0` | Number of executors to use.

NOTE: When greater than `0`, it disables link:spark-dynamic-allocation.adoc[dynamic allocation].

| [[spark_executor_memory]] `spark.executor.memory` | `1g` | Amount of memory to use per executor process (equivalent to link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable).

Refer to <<memory, Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings>> in this document.

| [[spark_executor_port]] `spark.executor.port` | |

| `spark.executor.logs.rolling.maxSize` | |
| `spark.executor.logs.rolling.maxRetainedFiles` | |
| `spark.executor.logs.rolling.strategy` | |
| `spark.executor.logs.rolling.time.interval` | |
| `spark.executor.port` | |
| `spark.executor.uri` | | Equivalent to `SPARK_EXECUTOR_URI`

| [[spark_task_maxDirectResultSize]] `spark.task.maxDirectResultSize` | `1048576B` |
|======================
