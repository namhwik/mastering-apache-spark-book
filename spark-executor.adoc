== [[Executor]] Executors

*Executors* are distributed agents that execute link:spark-taskscheduler-tasks.adoc[tasks].

They _typically_ run for the entire lifetime of a Spark application which is called *static allocation of executors* (but you could also opt in for link:spark-dynamic-allocation.adoc[dynamic allocation]).

Executors send <<heartbeats-and-active-task-metrics, active task metrics>> to the link:spark-driver.adoc[driver] and inform link:spark-ExecutorBackend.adoc[executor backends] about task status updates (including task results).

NOTE: Executors are managed exclusively by link:spark-ExecutorBackend.adoc[executor backends].

Executors provide in-memory storage for RDDs that are cached in Spark applications (via link:spark-blockmanager.adoc[Block Manager]).

When executors are started they register themselves with the driver and communicate directly to execute tasks.

*Executor offers* are described by executor id and the host on which an executor runs (see <<resource-offers, Resource Offers>> in this document).

Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track link:spark-executor-TaskRunner.adoc[running tasks] (by their task ids in <<runningTasks, runningTasks>> internal registry). Consult <<launchTask, Launching Tasks>> section.

Executors use a <<threadPool, Executor task launch worker thread pool>> for <<launchTask, launching tasks>>.

Executors send <<metrics, metrics>> (and heartbeats) using the <<heartbeater, internal heartbeater - Heartbeat Sender Thread>>.

It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster.

Executors are described by their *id*, *hostname*, *environment* (as `SparkEnv`), and *classpath* (and, less importantly, and more for internal optimization, whether they run in link:spark-local.adoc[local] or link:spark-cluster.adoc[cluster mode]).

CAUTION: FIXME How many cores are assigned per executor?

.Executor's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[maxDirectResultSize]] `maxDirectResultSize`
|

| [[maxResultSize]] `maxResultSize`
|

| [[runningTasks]] `runningTasks`
|

| [[heartbeatFailures]] `heartbeatFailures`
|

|===

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.executor.Executor` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[createClassLoader]] `createClassLoader` Method

CAUTION: FIXME

=== [[addReplClassLoaderIfNeeded]] `addReplClassLoaderIfNeeded` Method

CAUTION: FIXME

=== [[creating-instance]] Creating Executor Instance

`Executor` takes the following when created:

* [[executorId]] Executor ID
* [[executorHostname]] Executor's host name
* [[env]] link:spark-sparkenv.adoc[SparkEnv]
* [[userClassPath]] Collection of user-defined JARs (<<createClassLoader, to add to tasks' class path>>). Empty by default
* [[isLocal]] Flag whether it runs in local or cluster mode (disabled by default, i.e. cluster is preferred)

NOTE: User-defined JARs are defined using link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#main[`--user-class-path` command-line option of `CoarseGrainedExecutorBackend`] that can be set using <<spark.executor.extraClassPath, spark.executor.extraClassPath>> property.

NOTE: `isLocal` is enabled exclusively for link:spark-LocalEndpoint.adoc[LocalEndpoint] (for link:spark-local.adoc[Spark in local mode]).

When created, you should see the following INFO messages in the logs:

```
INFO Executor: Starting executor ID [executorId] on host [executorHostname]
```

(only for non-local mode) `Executor` sets `SparkUncaughtExceptionHandler` as the default handler invoked when a thread abruptly terminates due to an uncaught exception.

(only for non-local mode) `Executor` link:spark-metrics.adoc#registerSource[registers `ExecutorSource`] and link:spark-blockmanager.adoc#initialize[initializes the local `BlockManager`].

NOTE: `Executor` uses `SparkEnv` to access the local link:spark-sparkenv.adoc#metricsSystem[MetricsSystem] and link:spark-sparkenv.adoc#blockManager[BlockManager].

`Executor` <<createClassLoader, creates a task class loader>> (possibly with <<addReplClassLoaderIfNeeded, REPL support>>) and link:spark-Serializer.adoc#setDefaultClassLoader[requests the current `Serializer` to use it].

NOTE: `Executor` uses link:spark-sparkenv.adoc#serializer[`SparkEnv` to access the local `Serializer`].

`Executor` <<startDriverHeartbeater, starts sending heartbeats and active tasks metrics>>.

`Executor` initializes the <<internal-registries, internal registries and counters>> in the meantime (not necessarily at the very end).

[NOTE]
====
`Executor` is created when:

1. link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[`CoarseGrainedExecutorBackend` receives `RegisteredExecutor` message]

2. link:spark-mesos/spark-executor-backends-MesosExecutorBackend.adoc#registered[`MesosExecutorBackend` does `registered`]

3. link:spark-LocalEndpoint.adoc#creating-instance[`LocalEndpoint` is created]
====

=== [[launchTask]] Launching Task -- `launchTask` Method

[source, scala]
----
launchTask(
  context: ExecutorBackend,
  taskId: Long,
  attemptNumber: Int,
  taskName: String,
  serializedTask: ByteBuffer): Unit
----

`launchTask` executes the input `serializedTask` task concurrently.

Internally, `launchTask` creates a link:spark-executor-TaskRunner.adoc[TaskRunner], registers it in <<runningTasks, `runningTasks` internal registry>> (by `taskId`), and finally executes it on <<threadPool, "Executor task launch worker" thread pool>>.

.Launching tasks on executor using TaskRunners
image::images/executor-taskrunner-executorbackend.png[align="center"]

NOTE: `launchTask` is called by link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] (when it handles link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#LaunchTask[LaunchTask] message), link:spark-mesos/spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend], and link:spark-LocalEndpoint.adoc[LocalEndpoint].

=== [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics -- `startDriverHeartbeater` Method

Executors keep sending <<metrics, metrics for active tasks>> to the driver every <<spark_executor_heartbeatInterval, spark.executor.heartbeatInterval>> (defaults to `10s` with some random initial delay so the heartbeats from different executors do not pile up on the driver).

.Executors use HeartbeatReceiver endpoint to report task metrics
image::images/executor-heartbeatReceiver-endpoint.png[align="center"]

An executor sends heartbeats using the <<heartbeater, internal heartbeater - Heartbeat Sender Thread>>.

.HeartbeatReceiver's Heartbeat Message Handler
image::images/spark-HeartbeatReceiver-Heartbeat.png[align="center"]

For each link:spark-taskscheduler-tasks.adoc[task] in link:spark-executor-TaskRunner.adoc[TaskRunner] (in <<runningTasks, runningTasks>> internal registry), the task's metrics are computed (i.e. `mergeShuffleReadMetrics` and `setJvmGCTime`) that become part of the heartbeat (with accumulators).

CAUTION: FIXME How do `mergeShuffleReadMetrics` and `setJvmGCTime` influence `accumulators`?

NOTE: Executors track the link:spark-executor-TaskRunner.adoc[TaskRunner] that run link:spark-taskscheduler-tasks.adoc[tasks]. A link:spark-executor-TaskRunner.adoc#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat.

A blocking link:spark-sparkcontext-HeartbeatReceiver.adoc#Heartbeat[Heartbeat] message that holds the executor id, all accumulator updates (per task id), and link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId] is sent to link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint] (with <<spark_executor_heartbeatInterval, spark.executor.heartbeatInterval>> timeout).

CAUTION: FIXME When is `heartbeatReceiverRef` created?

If the response link:spark-sparkcontext-HeartbeatReceiver.adoc#Heartbeat[requests to reregister BlockManager], you should see the following INFO message in the logs:

```
INFO Executor: Told to re-register on heartbeat
```

The link:spark-blockmanager.adoc#reregister[BlockManager is reregistered].

The internal <<heartbeatFailures, heartbeatFailures>> counter is reset (i.e. becomes `0`).

If there are any issues with communicating with the driver, you should see the following WARN message in the logs:

```
WARN Executor: Issue communicating with driver in heartbeater
```

The internal <<heartbeatFailures, heartbeatFailures>> is incremented and checked to be less than the <<spark_executor_heartbeat_maxFailures, acceptable number of failures>>. If the number is greater, the following ERROR is printed out to the logs:

```
ERROR Executor: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times
```

The executor exits (using `System.exit` and exit code 56).

TIP: Read about `TaskMetrics` in link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics].

=== [[heartbeater]] heartbeater - Heartbeat Sender Thread

`heartbeater` is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread.

The name of the thread pool is *driver-heartbeater*.

=== [[coarse-grained-executor]] Coarse-Grained Executors

*Coarse-grained executors* are executors that use link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] for task scheduling.

=== [[resource-offers]] Resource Offers

Read link:spark-taskschedulerimpl.adoc#resourceOffers[resourceOffers] in TaskSchedulerImpl and link:spark-TaskSetManager.adoc##resourceOffers[resourceOffer] in TaskSetManager.

=== [[threadPool]] "Executor task launch worker" Thread Pool -- `threadPool` Property

`Executor` uses `threadPool` daemon cached thread pool with the name *Executor task launch worker-[ID]* (with `ID` being the task id) for <<launchTask, launching tasks>>.

`threadPool` is created when <<creating-instance, `Executor` is created>> and shut down when <<stop, it stops>>.

=== [[memory]] Executor Memory -- `spark.executor.memory` or `SPARK_EXECUTOR_MEMORY` settings

You can control the amount of memory per executor using <<spark_executor_memory, spark.executor.memory>> setting. It sets the available memory equally for all executors per application.

NOTE: The amount of memory per executor is looked up when link:spark-sparkcontext.adoc#creating-instance[SparkContext is created].

You can change the assigned memory per executor per node in link:spark-standalone.adoc[standalone cluster] using link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable.

You can find the value displayed as *Memory per Node* in link:spark-standalone-master.adoc[web UI for standalone Master] (as depicted in the figure below).

.Memory per Node in Spark Standalone's web UI
image::images/spark-standalone-webui-memory-per-node.png[align="center"]

The above figure shows the result of running link:spark-shell.adoc[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e.

```
./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g
```

=== [[metrics]] Metrics

Every executor registers its own link:spark-executor-ExecutorSource.adoc[ExecutorSource] to link:spark-metrics.adoc#report[report metrics].

=== [[stop]] Stopping Executor -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` link:spark-metrics.adoc#report[requests `MetricsSystem` for a report].

NOTE: `stop` uses link:spark-sparkenv.adoc#metricsSystem[`SparkEnv` to access the current `MetricsSystem`].

`stop` shuts <<heartbeater, driver-heartbeater thread>> down (and waits at most 10 seconds).

`stop` shuts <<threadPool, Executor task launch worker thread pool>> down.

(only when <<isLocal, not local>>) `stop` link:spark-sparkenv.adoc#stop[requests `SparkEnv` to stop].

NOTE: `stop` is used when link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#Shutdown[CoarseGrainedExecutorBackend] and link:spark-LocalEndpoint.adoc#StopExecutor[LocalEndpoint] are requested to stop their managed executors.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark.executor.cores]] `spark.executor.cores`
|
| Number of cores for an executor.

| [[spark.executor.extraClassPath]] `spark.executor.extraClassPath`
| (empty)
| List of URLs representing user-defined class path entries that are added to an executor's class path.

Each entry is separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.

| [[spark.executor.extraJavaOptions]] `spark.executor.extraJavaOptions`
|
| Extra Java options for executors.

Used to link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[prepare the command to launch `CoarseGrainedExecutorBackend` in a YARN container].

| [[spark.executor.extraLibraryPath]] `spark.executor.extraLibraryPath`
|
| Extra library paths separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.

Used to link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[prepare the command to launch `CoarseGrainedExecutorBackend` in a YARN container].

| [[spark_executor_userClassPathFirst]] `spark.executor.userClassPathFirst` | `false` | Flag to control whether to load classes in user jars before those in Spark jars.

| [[spark_executor_heartbeatInterval]] `spark.executor.heartbeatInterval` | `10s` | Interval after which an executor reports heartbeat and metrics for active tasks to the driver.

Refer to <<heartbeats-and-active-task-metrics, Sending heartbeats and partial metrics for active tasks>> in this document.

| [[spark_executor_heartbeat_maxFailures]] `spark.executor.heartbeat.maxFailures` | `60` | Number of times an executor will try to send heartbeats to the driver before it gives up and exits (with exit code `56`).

NOTE: It was introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times].

| [[spark_executor_id]] `spark.executor.id` | |

| [[spark.executor.instances]] `spark.executor.instances`
| `0`
| Number of executors to use.

| [[spark.executor.memory]][[spark_executor_memory]] `spark.executor.memory` | `1g` | Amount of memory to use per executor process (equivalent to link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable).

Refer to <<memory, Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings>> in this document.

| [[spark_executor_port]] `spark.executor.port` | |

| `spark.executor.logs.rolling.maxSize` | |
| `spark.executor.logs.rolling.maxRetainedFiles` | |
| `spark.executor.logs.rolling.strategy` | |
| `spark.executor.logs.rolling.time.interval` | |
| `spark.executor.port` | |
| `spark.executor.uri` | | Equivalent to `SPARK_EXECUTOR_URI`

| [[spark_task_maxDirectResultSize]] `spark.task.maxDirectResultSize` | `1048576B` |
|===
