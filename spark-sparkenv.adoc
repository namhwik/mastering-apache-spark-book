== SparkEnv - Spark Runtime Environment

*Spark Runtime Environment* (`SparkEnv`) is the runtime environment with Spark services that interact with each other to build the entire Spark computing platform.

Spark Runtime Environment is represented by a <<SparkEnv, SparkEnv>> object that holds all the required services for a running Spark instance, i.e. a master or an executor.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.SparkEnv` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.SparkEnv=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[SparkEnv]] SparkEnv

*SparkEnv* holds all runtime objects for a running Spark instance, using <<createDriverEnv, SparkEnv.createDriverEnv()>> for a driver and <<createExecutorEnv, SparkEnv.createExecutorEnv()>> for an executor.

You can access the Spark environment using `SparkEnv.get`.

```
scala> import org.apache.spark._
import org.apache.spark._

scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@2220c5f7
```

=== [[create]] Creating "Base" SparkEnv (create method)

[source, scala]
----
create(
  conf: SparkConf,
  executorId: String,
  hostname: String,
  port: Int,
  isDriver: Boolean,
  isLocal: Boolean,
  numUsableCores: Int,
  listenerBus: LiveListenerBus = null,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`create` is a internal helper method to create a "base" `SparkEnv` regardless of the target environment -- be it a driver or an executor.

When executed, `create` creates a `Serializer` (based on <<spark.serializer, spark.serializer>> setting). You should see the following `DEBUG` message in the logs:

```
DEBUG SparkEnv: Using serializer: [serializer]
```

It creates another `Serializer` (based on <<spark.closure.serializer, spark.closure.serializer>>).

It creates a link:spark-shuffle-manager.adoc[ShuffleManager] based on <<settings, spark.shuffle.manager>> setting.

It creates a link:spark-MemoryManager.adoc[MemoryManager] based on <<spark.memory.useLegacyMode, spark.memory.useLegacyMode>> setting (with link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] being the default).

It creates a link:spark-blocktransferservice.adoc#NettyBlockTransferService[NettyBlockTransferService].

[[BlockManagerMaster]]
It creates a link:spark-BlockManagerMaster.adoc[BlockManagerMaster] object with the `BlockManagerMaster` RPC endpoint reference (by <<registerOrLookupEndpoint, registering or looking it up by name>> and link:spark-BlockManagerMaster.adoc#BlockManagerMasterEndpoint[BlockManagerMasterEndpoint]), the input link:spark-configuration.adoc[SparkConf], and the input `isDriver` flag.

.Creating BlockManager for the Driver
image::images/sparkenv-driver-blockmanager.png[align="center"]

NOTE: `create` registers the *BlockManagerMaster* RPC endpoint for the driver and looks it up for executors.

.Creating BlockManager for Executor
image::images/sparkenv-executor-blockmanager.png[align="center"]

It creates a link:spark-blockmanager.adoc#creating-instance[BlockManager] (using the above `BlockManagerMaster` object and other services).

It creates a link:spark-service-broadcastmanager.adoc[BroadcastManager].

It creates a CacheManager.

It creates a MetricsSystem for a driver and a worker separately.

It initializes `userFiles` temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor.

An OutputCommitCoordinator is created.

NOTE: `create` is called by <<createDriverEnv, createDriverEnv>> and <<createExecutorEnv, createExecutorEnv>>.

=== [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name (registerOrLookupEndpoint method)

[source, scala]
----
registerOrLookupEndpoint(name: String, endpointCreator: => RpcEndpoint)
----

`registerOrLookupEndpoint` registers or looks up a RPC endpoint by `name`.

If called from the driver, you should see the following INFO message in the logs:

```
INFO SparkEnv: Registering [name]
```

And the RPC endpoint is registered in the RPC environment.

Otherwise, it obtains a RPC endpoint reference by `name`.

=== [[createDriverEnv]] Creating SparkEnv for Driver (createDriverEnv method)

[source, scala]
----
createDriverEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus,
  numCores: Int,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`createDriverEnv` creates a `SparkEnv` execution environment for the driver.

.Spark Environment for driver
image::images/sparkenv-driver.png[align="center"]

The method accepts an instance of link:spark-configuration.adoc[SparkConf], link:spark-deployment-environments.adoc[whether it runs in local mode or not], link:spark-LiveListenerBus.adoc[LiveListenerBus], the number of driver's cores to use for execution in local mode or `0` otherwise, and a link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] (default: none).

`createDriverEnv` ensures that <<spark.driver.host, spark.driver.host>> and <<spark.driver.port, spark.driver.port>> settings are set in `conf` link:spark-configuration.adoc[SparkConf].

It then passes the call straight on to the <<create, create helper method>> (with `driver` executor id, `isDriver` enabled, and the input parameters).

NOTE: `createDriverEnv` is exclusively used by link:spark-sparkcontext-creating-instance-internals.adoc#createSparkEnv[SparkContext to create a `SparkEnv`] (while a link:spark-sparkcontext.adoc#creating-instance[SparkContext is being created for the driver]).

=== [[createExecutorEnv]] Creating SparkEnv for Executor (createExecutorEnv method)

`SparkEnv.createExecutorEnv` creates an *executor's (execution) environment* that is the Spark execution environment for an executor.

.Spark Environment for executor
image::images/sparkenv-executor.png[align="center"]

It uses SparkConf, the executor's identifier, hostname, port, the number of cores, and whether or not it runs in local mode.

For Akka-based RPC Environment (obsolete since Spark 1.6.0-SNAPSHOT), the name of the actor system for an executor is *sparkExecutor*.

It creates an link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker] object and looks up `MapOutputTracker` RPC endpoint. See link:spark-service-mapoutputtracker.adoc[MapOutputTracker].

It creates a MetricsSystem for *executor* and starts it.

An OutputCommitCoordinator is created and *OutputCommitCoordinator* RPC endpoint looked up.

=== [[serializer]] serializer

CAUTION: FIXME

=== [[closureSerializer]] closureSerializer

CAUTION: FIXME

=== [[get]] Getting Current SparkEnv (SparkEnv.get method)

=== [[settings]] Settings

==== [[spark.driver.host]] spark.driver.host

`spark.driver.host` is the name of the machine where the driver runs. It is set when link:spark-sparkcontext.adoc#creating-instance[SparkContext is created].

==== [[spark.driver.port]] spark.driver.port

`spark.driver.port` is the port the driver listens to. It is first set to `0` in the driver when link:spark-sparkcontext.adoc#creating-instance[SparkContext is initialized]. It is later set to the port of link:spark-rpc.adoc[RpcEnv] of the driver (in <<create, SparkEnv.create>>).

==== [[spark.serializer]] spark.serializer

`spark.serializer` (default: `org.apache.spark.serializer.JavaSerializer`) - the Serializer.

[TIP]
====
Enable DEBUG logging level for `org.apache.spark.SparkEnv` logger to see the current value.

```
DEBUG SparkEnv: Using serializer: [serializer]
```
====

==== [[spark.closure.serializer]] spark.closure.serializer

`spark.closure.serializer` (default: `org.apache.spark.serializer.JavaSerializer`) is the Serializer.

==== [[spark.shuffle.manager]] spark.shuffle.manager

`spark.shuffle.manager` (default: `sort`) - one of the three available implementations of link:spark-shuffle-manager.adoc[ShuffleManager] or a fully-qualified class name of a custom implementation of `ShuffleManager`:

* `hash` or `org.apache.spark.shuffle.hash.HashShuffleManager`
* `sort` or `org.apache.spark.shuffle.sort.SortShuffleManager`
* `tungsten-sort` or `org.apache.spark.shuffle.sort.SortShuffleManager`

==== [[spark.memory.useLegacyMode]] spark.memory.useLegacyMode

`spark.memory.useLegacyMode` (default: `false`) controls the link:spark-MemoryManager.adoc[MemoryManager] in use. It is `StaticMemoryManager` when enabled (`true`) or link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] when disabled (`false`).
