== [[Builder]] `Builder` -- Building `SparkSession` with Fluent API

`Builder` is the fluent API to build a fully-configured link:spark-sql-sparksession.adoc[SparkSession].

.Builder Methods
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Method | Description
| <<enableHiveSupport, enableHiveSupport>> | Enables Hive support.
|======================

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .getOrCreate
----

You can use the fluent design pattern to set the various properties of a `SparkSession` that opens a session to Spark SQL.

NOTE: You can have multiple ``SparkSession``s in a single Spark application.

=== [[config]] `config` method

CAUTION: FIXME

=== [[enableHiveSupport]] Enabling Hive Support -- `enableHiveSupport` Method

When link:spark-sql-sparksession.adoc#creating-instance[creating a `SparkSession`], you can enable Hive support using `enableHiveSupport` method.

[source, scala]
----
enableHiveSupport(): Builder
----

`enableHiveSupport` enables Hive support.

Internally, `enableHiveSupport` makes sure that the Hive classes are on CLASSPATH, i.e. Spark SQL's link:spark-sql-queryplanner.adoc#HiveSessionState[org.apache.spark.sql.hive.HiveSessionState] and `org.apache.hadoop.hive.conf.HiveConf`, and sets link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] property to `hive`.
